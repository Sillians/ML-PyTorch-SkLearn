{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f4fab0",
   "metadata": {},
   "source": [
    "**complete, modern, end-to-end PyTorch pipeline** for IMDB using the HuggingFace `datasets` library and a classic **LSTM classifier**. \n",
    "It includes:\n",
    "\n",
    "* dataset loading (`datasets`)\n",
    "* simple tokenizer (the one used earlier)\n",
    "* building `stoi`/`itos` with `min_freq` + `max_vocab`\n",
    "* `text_pipeline` / `label_pipeline`\n",
    "* `collate_fn` (padded batches + lengths)\n",
    "* `LSTMClassifier` (embedding → LSTM (packed) → classifier)\n",
    "* training & eval loop that works on **MPS / CUDA / CPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else\n",
    "                      (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load dataset\n",
    "# -------------------------\n",
    "imdb = load_dataset(\"imdb\")\n",
    "train_hf = imdb[\"train\"]   # dataset object (25k)\n",
    "test_hf  = imdb[\"test\"]    # dataset object (25k)\n",
    "\n",
    "# create train/valid split like original example: 20k / 5k\n",
    "torch.manual_seed(1)\n",
    "train_list = list(train_hf)\n",
    "train_list, valid_list = random_split(train_list, [20000, 5000])\n",
    "\n",
    "# -------------------------\n",
    "# 2) Tokenizer (same as earlier)\n",
    "# -------------------------\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    return text.split()\n",
    "\n",
    "# -------------------------\n",
    "# 3) Build vocab (stoi/itos)\n",
    "#    - min_freq and max_vocab to limit size\n",
    "# -------------------------\n",
    "def build_vocab(dataset_iterable, tokenizer, max_vocab=30000, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for sample in dataset_iterable:\n",
    "        tokens = tokenizer(sample[\"text\"])\n",
    "        counter.update(tokens)\n",
    "    # keep tokens that appear at least min_freq, and top-k by frequency\n",
    "    most_common = [t for t, c in counter.most_common(max_vocab) if counter[t] >= min_freq]\n",
    "    # reserve special tokens\n",
    "    specials = [\"<pad>\", \"<unk>\"]\n",
    "    itos = specials + most_common\n",
    "    stoi = {tok: idx for idx, tok in enumerate(itos)}\n",
    "    return stoi, itos, counter\n",
    "\n",
    "stoi, itos, counter = build_vocab(train_list, tokenizer, max_vocab=30000, min_freq=2)\n",
    "print(\"vocab size:\", len(stoi))\n",
    "\n",
    "# -------------------------\n",
    "# 4) pipelines\n",
    "# -------------------------\n",
    "def text_pipeline(text):\n",
    "    return [stoi.get(tok, stoi[\"<unk>\"]) for tok in tokenizer(text)]\n",
    "\n",
    "def label_pipeline(label):\n",
    "    # label in HF is int 0/1 already, ensure float\n",
    "    return float(label)\n",
    "\n",
    "# -------------------------\n",
    "# 5) collate_fn\n",
    "# -------------------------\n",
    "def collate_batch(batch):\n",
    "    # batch: list of samples (each sample is dict {'text':..., 'label':...})\n",
    "    texts = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for sample in batch:\n",
    "        text = sample[\"text\"]\n",
    "        label = sample[\"label\"]\n",
    "        ids = torch.tensor(text_pipeline(text), dtype=torch.long)\n",
    "        texts.append(ids)\n",
    "        labels.append(label_pipeline(label))\n",
    "        lengths.append(len(ids))\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    padded = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=stoi[\"<pad>\"])\n",
    "    return padded.to(device), labels.to(device), lengths.to(device)\n",
    "\n",
    "# quick sanity dataloader\n",
    "small_loader = DataLoader(train_list[:16], batch_size=4, collate_fn=collate_batch)\n",
    "x, y, l = next(iter(small_loader))\n",
    "print(\"sanity shapes:\", x.shape, y.shape, l.shape)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Model: LSTMClassifier\n",
    "#    - supports bidirectional, dropout, packed sequences\n",
    "# -------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_size=256, num_layers=1,\n",
    "                 bidirectional=True, dropout=0.3, fc_hidden=128, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim, padding_idx=stoi[\"<pad>\"])\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size * self.num_directions, fc_hidden)\n",
    "        self.fc_out = nn.Linear(fc_hidden, num_classes)  # num_classes=1 -> single logit\n",
    "        # initialize\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (B, T) token ids\n",
    "        # lengths: (B,) actual lengths\n",
    "        emb = self.embedding(x)                 # (B, T, emb_dim)\n",
    "        # pack padded\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h_n, c_n) = self.lstm(packed)\n",
    "        # h_n shape: (num_layers * num_directions, B, hidden_size)\n",
    "        # get final hidden state for each direction, concat\n",
    "        if self.bidirectional:\n",
    "            # take last layer's forward and backward\n",
    "            h_forward = h_n[-2, :, :]   # (B, hidden_size)\n",
    "            h_backward = h_n[-1, :, :]  # (B, hidden_size)\n",
    "            h_final = torch.cat([h_forward, h_backward], dim=1)  # (B, 2*hidden_size)\n",
    "        else:\n",
    "            h_final = h_n[-1, :, :]  # (B, hidden_size)\n",
    "        x = self.dropout(h_final)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        logits = self.fc_out(x).squeeze(1)  # (B,) for num_classes=1\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# 7) Training utilities\n",
    "# -------------------------\n",
    "def binary_accuracy_from_logits(logits, targets):\n",
    "    preds = torch.sigmoid(logits) >= 0.5\n",
    "    return (preds.float() == targets).float().mean()\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, lb in dataloader:\n",
    "            logits = model(xb, lb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            acc = binary_accuracy_from_logits(logits, yb)\n",
    "            batch_size = xb.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_acc += acc.item() * batch_size\n",
    "            n += batch_size\n",
    "    return running_loss / n, running_acc / n\n",
    "\n",
    "# -------------------------\n",
    "# 8) Instantiate model, dataloaders, optimizer\n",
    "# -------------------------\n",
    "vocab_size = len(stoi)\n",
    "model = LSTMClassifier(vocab_size=vocab_size, emb_dim=128, hidden_size=256,\n",
    "                       num_layers=2, bidirectional=True, dropout=0.3, fc_hidden=128).to(device)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_list, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid_list, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_hf, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "# -------------------------\n",
    "# 9) Training loop (simple, with early stopping)\n",
    "# -------------------------\n",
    "epochs = 6\n",
    "best_val_loss = math.inf\n",
    "patience = 2\n",
    "stale = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    for xb, yb, lb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb, lb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        n += bs\n",
    "    train_loss = running_loss / n\n",
    "    val_loss, val_acc = evaluate(model, valid_loader, loss_fn)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f} time={elapsed:.1f}s\")\n",
    "\n",
    "    # early stopping + checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        stale = 0\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"stoi\": stoi,\n",
    "        }, \"best_lstm_imdb.pth\")\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale > patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# -------------------------\n",
    "# 10) Test evaluation (load best)\n",
    "# -------------------------\n",
    "ckpt = torch.load(\"best_lstm_imdb.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "test_loss, test_acc = evaluate(model, test_loader, loss_fn)\n",
    "print(f\"Test | loss={test_loss:.4f} acc={test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83abf2",
   "metadata": {},
   "source": [
    "### Quick notes & rationale (short)\n",
    "\n",
    "* **Vocabulary size**: limited via `max_vocab` and `min_freq`. Adjust depending on memory.\n",
    "* **Embedding dim (128)**: good default for sentiment tasks; increase if dataset large or using pretrained vectors.\n",
    "* **Hidden size (256)** and **num_layers=2**: moderate capacity; increase for more expressivity.\n",
    "* **Bidirectional LSTM**: captures context from both directions (common for sentence classification).\n",
    "* **Packed sequences**: `pack_padded_sequence` ensures LSTM ignores padded tokens (efficient & correct).\n",
    "* **BCEWithLogitsLoss**: single-logit binary classification (more stable than sigmoid + BCELoss).\n",
    "* **Gradient clipping** & **LR scheduler** added for stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76f552",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
