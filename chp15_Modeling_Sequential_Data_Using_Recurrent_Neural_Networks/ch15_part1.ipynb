{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e07973e",
   "metadata": {},
   "source": [
    "# **Modeling Sequential Data Using Recurrent Neural Networks (Part 1/3)**\n",
    "\n",
    "\n",
    "## **Introducing sequential data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed6524",
   "metadata": {},
   "source": [
    "### **Modeling sequential data⁠—order matters**\n",
    "\n",
    "What makes sequences unique, compared to other types of data, is that elements in a sequence appear in a certain order and are not independent of each other. Typical machine learning algorithms for supervised learning assume that the input is `independent and identically distributed (IID)` data, which means that the training examples are `mutually independent` and have the same underlying distribution. In this regard, based on the mutual independence assumption, the order in which the training examples are given to the model is irrelevant. For example, if we have a sample consisting of n training examples, `x(1), x(2), ..., x(n)`, the order in which we use the data for training our machine learning algorithm does not matter. An example of this scenario would be the Iris dataset that we worked with previously. In the Iris dataset, each flower has been measured independently, and the measurements of one flower do not influence the measurements of another flower.\n",
    "\n",
    "However, this assumption is not valid when we deal with `sequences—by definition`, order matters. Predicting the market value of a particular stock would be an example of this scenario. For instance, assume we have a sample of `n training ` examples, where each training example represents the market value of a certain stock on a particular day. If our task is to predict the stock market value for the next three days, it would make sense to consider the previous stock prices in a date-sorted order to derive trends rather than utilize these training examples in a randomized order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccf646",
   "metadata": {},
   "source": [
    "### **Sequential data versus time series data**\n",
    "\n",
    "Time series data is a special type of sequential data where each example is associated with a dimension for time. In time series data, samples are taken at successive timestamps, and therefore, the time dimension determines the order among the data points. For example, stock prices and voice or speech records are time series data.\n",
    "\n",
    "On the other hand, not all sequential data has the time dimension. For example, in text data or DNA sequences, the examples are ordered, but text or DNA does not qualify as time series data. \n",
    "\n",
    "The fundamental characteristics of sequential data is that the order of its elements is significant and matters greatly for its meaning and context. \n",
    "- Order Matters\n",
    "- Contextual Understanding\n",
    "- Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd08727",
   "metadata": {},
   "source": [
    "### **Representing sequences**\n",
    "\n",
    "![Example of time series data](./figures/15_01.png)\n",
    "\n",
    "As we have already mentioned, the standard `NN` models that we have covered so far, such as `multilayer perceptrons (MLPs)` and `CNNs` for image data, assume that the training examples are independent of each other and thus do not incorporate ordering information. We can say that such models do not have a memory of previously seen training examples. \n",
    "\n",
    "For instance, the samples are passed through the `feedforward` and `backpropagation` steps, and the weights are updated independently of the order in which the training examples are processed.\n",
    "\n",
    "`RNNs`, by contrast, are designed for modeling sequences and are capable of remembering past information and processing new events accordingly, which is a clear advantage when working with sequence data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1af5e4",
   "metadata": {},
   "source": [
    "### **The different categories of sequence modeling**\n",
    "\n",
    "- Language translation\n",
    "- Image Captioning\n",
    "- text generation\n",
    "\n",
    "\n",
    "![Effectiveness of RNNs](./figures/diags.jpeg)\n",
    "\n",
    "\n",
    "If neither the input nor output data represent sequences, then we are dealing with standard data, and we could simply use a multilayer perceptron to model such data. However, if either the input or output is a sequence, the modeling task likely falls into one of these categories:\n",
    "\n",
    "- `Many-to-one:` The input data is a sequence, but the output is a fixed-size vector or scalar, not a sequence. For example, in sentiment analysis, the input is text-based (for example, a movie review) and the output is a class label (for example, a label denoting whether a reviewer liked\n",
    "the movie).\n",
    "\n",
    "\n",
    "- `One-to-many:` The input data is in standard format and not a sequence, but the output is a sequence. An example of this category is image captioning—the input is an image and the output is an English phrase summarizing the content of that image.\n",
    "\n",
    "\n",
    "- `Many-to-many:` Both the input and output arrays are sequences. This category can be further divided based on whether the input and output are synchronized. An example of a synchronized many-to-many modeling task is video classification, where each frame in a video is labeled.\n",
    "An example of a delayed many-to-many modeling task would be translating one language into another. For instance, an entire English sentence must be read and processed by a machine before its translation into German is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c156186",
   "metadata": {},
   "source": [
    "## **RNNs for modeling sequences**\n",
    "\n",
    "### **Understanding the dataflow in RNNs**\n",
    "\n",
    "![The dataflow of a standard feedforward NN and an RNN](./figures/15_03.png)\n",
    "\n",
    "\n",
    "In a standard feedforward network, information flows from the input to the hidden layer, and then from the hidden layer to the output layer. On the other hand, in an RNN, the hidden layer receives its input from both the input layer of the current time step and the hidden layer from the previous time step.\n",
    "\n",
    "The flow of information in adjacent time steps in the hidden layer allows the network to have a memory of past events. This flow of information is usually displayed as a loop, also known as a `recurrent edge` in graph notation, which is how this general RNN architecture got its name.\n",
    "\n",
    "Similar to multilayer perceptrons, RNNs can consist of multiple hidden layers. Note that it’s a common convention to refer to RNNs with one hidden layer as a single-layer RNN, which is not to be confused with single-layer NNs without a hidden layer. \n",
    "\n",
    "\n",
    "![An RNN with one and two hidden layers](./figures/15_04.png)\n",
    "\n",
    "\n",
    "As we know, each hidden unit in a standard NN receives only one input—the net preactivation associated with the input layer. In contrast, each hidden unit in an RNN receives two distinct sets of input—the preactivation from the input layer and the activation of the same hidden layer from the previous time step, `t – 1`.\n",
    "\n",
    "At the first time step, `t = 0`, the hidden units are initialized to zeros or small random values. Then, at a time step where `t > 0`, the hidden units receive their input from the data point at the current time, `x(t)`, and the previous values of hidden units at `t – 1`, indicated as `h(t–1)`.\n",
    "\n",
    "Similarly, in the case of a multilayer RNN, we can summarize the information flow as follows:\n",
    "\n",
    "- `layer = 1`: Here, the hidden layer is represented as $h_{1}^{(t)}$ and it receives its input from the data point, $x^{(t)}$, and the hidden values in the same layer, but at the previous time step, $h_{1}^{(t - 1)}$.\n",
    "\n",
    "- `layer = 2`: The second hidden layer, $h_{2}^{(t)}$, receives its inputs from the outputs of the layer below at the current time step ($o_{1}^{(t)}$) and its own hidden values from the previous time step $h_{2}^{(t - 1)}$.\n",
    "\n",
    "\n",
    "Since, in this case, each recurrent layer must receive a sequence as input, all the recurrent layers except the last one must return a sequence as output (that is, we will later have to set `return_sequences=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a7892",
   "metadata": {},
   "source": [
    "### **Computing activations in an RNN**\n",
    "\n",
    "Now that you understand the structure and general flow of information in an `RNN`, let’s get more specific and compute the actual activations of the hidden layers, as well as the output layer. For simplicity, we will consider just a single hidden layer; however, the same concept applies to multilayer RNNs.\n",
    "\n",
    "Each directed edge (the connections between boxes) in the representation of an `RNN` that we just looked at is associated with a weight matrix. Those weights do not depend on time, `t`; therefore, they are shared across the time axis. The different weight matrices in a single-layer `RNN` are as follows:\n",
    "\n",
    "- $W_{xh}$: The weight matrix between the input, $x^{(t)}$, and the hidden layer, $h$.\n",
    "- $W_{hh}$: The weight matrix associated with the recurrent edge.\n",
    "- $W_{ho}$: The weight matrix between the hidden layer and the output layer.\n",
    "\n",
    "These weight matrices are depicted in the figure below;\n",
    "\n",
    "![Applying weights to a single-layer RNN](./figures/15_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8553d",
   "metadata": {},
   "source": [
    "\n",
    "### **Deep Dive into the Mathematics of RNNs**\n",
    "\n",
    "\n",
    "\n",
    "**1. Core Idea**\n",
    "\n",
    "RNNs process sequential data by **reusing** a hidden state across time. The hidden state acts as a **memory** that encodes information about all previous steps.\n",
    "\n",
    "Given an input sequence $`x = (x_1, x_2, \\dots, x_T)`$,\n",
    "\n",
    "the RNN produces hidden states $`h = (h_1, h_2, \\dots, h_T)`$\n",
    "\n",
    "and (optionally) outputs $`y = (y_1, y_2, \\dots, y_T)`$.\n",
    "\n",
    "\n",
    "![Computing the activations](./figures/15_06.png)\n",
    "\n",
    "\n",
    "**2. Hidden State Update Equation**\n",
    "\n",
    "The hidden state at time $`t`$ is computed as:\n",
    "\n",
    "$$h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "| Symbol          | Meaning                                  |\n",
    "| --------------- | ---------------------------------------- |\n",
    "| $`x_t`$         | Input vector at time step $`t`$          |\n",
    "| $`h_t`$         | Hidden state at time step $`t`$          |\n",
    "| $`W_{xh}`$      | Input-to-hidden weight matrix            |\n",
    "| $`W_{hh}`$      | Hidden-to-hidden recurrent weight matrix |\n",
    "| $`b_h`$         | Bias for hidden state                    |\n",
    "| $`\\phi(\\cdot)`$ | Nonlinearity (tanh or ReLU)              |\n",
    "\n",
    "If the network outputs at each time step:\n",
    "\n",
    "$$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "\n",
    "\n",
    "**3. Forward Pass Unrolled in Time**\n",
    "\n",
    "Unrolling the RNN shows how the state flows:\n",
    "\n",
    "$`h_1 = \\phi(W_{xh} x_1 + W_{hh} h_0 + b_h)`$\n",
    "\n",
    "$`h_2 = \\phi(W_{xh} x_2 + W_{hh} h_1 + b_h)`$\n",
    "\n",
    "$`\\cdots`$\n",
    "\n",
    "$`h_T = \\phi(W_{xh} x_T + W_{hh} h_{T-1} + b_h)`$\n",
    "\n",
    "This makes it clear that:\n",
    "\n",
    "**Information flows recursively**, but **gradients must pass through many multiplications of $`W_{hh}`$**.\n",
    "\n",
    "\n",
    "\n",
    "**4. Loss Over the Sequence**\n",
    "\n",
    "If the task requires predictions for each timestep:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{t=1}^{T} \\mathrm{Loss}(y_t, \\hat{y}_t)$$\n",
    "\n",
    "\n",
    "\n",
    "**5. Backpropagation Through Time (BPTT)**\n",
    "\n",
    "To learn parameters, gradients must propagate backward across **time**, not just layers.\n",
    "\n",
    "Derivative of loss w.r.t. hidden state:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_t}\n",
    "=========================================\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}*t}{\\partial h_t}\n",
    "+\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h*{t+1}} \\cdot \\frac{\\partial h_{t+1}}{\\partial h_t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_{t+1}}{\\partial h_t}\n",
    "=====================================\n",
    "\n",
    "W_{hh}^\\top \\cdot \\mathrm{diag}\\left(\\phi'(a_{t+1})\\right)\n",
    "$$\n",
    "\n",
    "So gradients accumulate multiplicatively:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_t}\n",
    "=========================================\n",
    "\n",
    "\\sum_{k=t}^{T}\n",
    "\\left(\n",
    "\\prod_{j=t+1}^{k} W_{hh}^\\top \\mathrm{diag}(\\phi'(a_j))\n",
    "\\right)\n",
    "\\frac{\\partial \\mathcal{L}_k}{\\partial h_k}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**6. Vanishing and Exploding Gradients**\n",
    "\n",
    "The critical term is:\n",
    "\n",
    "$$\\prod_{j=t+1}^{k} W_{hh}^\\top \\mathrm{diag}(\\phi'(a_j))$$\n",
    "\n",
    "If the largest singular value of $`W_{hh}`$ is:\n",
    "\n",
    "* **Less than 1** → gradients **shrink** → **vanishing gradient**.\n",
    "* **Greater than 1** → gradients **blow up** → **exploding gradient**.\n",
    "\n",
    "This explains why **long sequences** break standard RNN training.\n",
    "\n",
    "\n",
    "\n",
    "**7. Why LSTM/GRU Fix the Problem**\n",
    "\n",
    "They introduce **gates** to **control** information flow:\n",
    "\n",
    "* Additive memory updates reduce multiplicative gradient chains.\n",
    "* Gradients propagate more stably.\n",
    "\n",
    "Key structural difference:\n",
    "\n",
    "Standard RNN memory update (multiplicative):\n",
    "\n",
    "$$h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1})$$\n",
    "\n",
    "LSTM memory update (additive):\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "\n",
    "This **additive path** preserves gradient magnitude.\n",
    "\n",
    "\n",
    "\n",
    "**8. Summary Table**\n",
    "\n",
    "| Model        | Memory Mechanism                | Gradient Behavior          | Suitable For     |\n",
    "| ------------ | ------------------------------- | -------------------------- | ---------------- |\n",
    "| Standard RNN | Multiplicative recurrence       | Vanishing/Exploding common | Short sequences  |\n",
    "| GRU          | Gated recurrence (reset/update) | More stable                | Medium sequences |\n",
    "| LSTM         | Explicit memory cell + gates    | Most stable                | Long sequences   |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f664a35",
   "metadata": {},
   "source": [
    "### **Hidden-recurrence vs. output-recurrence**\n",
    "\n",
    "So far, you have seen recurrent networks in which the hidden layer has the recurrent property. However, note that there is an alternative model in which the recurrent connection comes from the output layer. In this case, the net activations from the output layer at the previous time step, $o^{t–1}$, can be added in one of two ways:\n",
    "\n",
    "- To the hidden layer at the current time step, $h^t$ (output-to-hidden).\n",
    "- To the output layer at the current time step, $o^t$ (output-to-output).\n",
    "\n",
    "\n",
    "![Different recurrent connection models](./figures/15_07.png)\n",
    "\n",
    "\n",
    "As shown in the figure above, the difference between these architectures can be clearly seen in the recurring connections. Following our notation, the weights associated with the recurrent connection will be denoted for the `hidden-to-hidden` recurrence by $W_{hh}$, for the `output-to-hidden` recurrence by $W_{oh}$, and for the `output-to-output` recurrence by $W_{oo}$. In some articles in literature, the weights associated with the recurrent connections are also denoted by $W_{rec}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4495cff",
   "metadata": {},
   "source": [
    "- Manually compute the forward pass for one of these recurrent types. Using the `torch.nn` module, a recurrent layer can be defined via `RNN`, which is similar to the `hidden-to-hidden` recurrence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8256a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da0bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_xh shape: torch.Size([2, 5])\n",
      "W_hh shape: torch.Size([2, 2])\n",
      "b_xh shape: torch.Size([2])\n",
      "b_hh shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "torch.manual_seed(1)\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2,\n",
    "                   num_layers=1, batch_first=True)\n",
    "\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape:', w_xh.shape)\n",
    "print('W_hh shape:', w_hh.shape)\n",
    "print('b_xh shape:', b_xh.shape)\n",
    "print('b_hh shape:', b_hh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795aa03",
   "metadata": {},
   "source": [
    "- The input shape for this layer is `(batch_size, sequence_length, input_size)`,\n",
    "where;\n",
    "    - the first dimension is the batch dimension\n",
    "    - the second dimension correponds to the sequence, and \n",
    "    - the last dimension corresponds to the features.\n",
    "\n",
    "- We will output a sequence, which, for an input sequence of length `3`, will result in the output sequence $({o^{(0)}, o^{(1)}, o^{(2)}})$.\n",
    "- You can set `num_layers` to stack multiple RNN layers together to form a stacked RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abaee02",
   "metadata": {},
   "source": [
    "- Call forward pass on the `rnn_layer` and manually compute the outputs at each time step and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fa6749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0 =>\n",
      "   Input           : [[1. 1. 1. 1. 1.]]\n",
      "   Hidden          : [[-0.47019297  0.58639044]]\n",
      "   Output (manual) : [[-0.35198015  0.52525216]]\n",
      "   RNN output      : [[-0.3519801   0.52525216]]\n",
      "\n",
      "Time step 1 =>\n",
      "   Input           : [[2. 2. 2. 2. 2.]]\n",
      "   Hidden          : [[-0.8888316  1.2364398]]\n",
      "   Output (manual) : [[-0.68424344  0.76074266]]\n",
      "   RNN output      : [[-0.68424344  0.76074266]]\n",
      "\n",
      "Time step 2 =>\n",
      "   Input           : [[3. 3. 3. 3. 3.]]\n",
      "   Hidden          : [[-1.3074702  1.8864892]]\n",
      "   Output (manual) : [[-0.8649416  0.9046636]]\n",
      "   RNN output      : [[-0.8649416  0.9046636]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "\n",
    "## output of the simple RNN:\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "\n",
    "## manually computing the output:\n",
    "out_man = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('   Input           :', xt.numpy())\n",
    "    \n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh    \n",
    "    print('   Hidden          :', ht.detach().numpy())\n",
    "    \n",
    "    if t>0:\n",
    "        prev_h = out_man[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out_man.append(ot)\n",
    "    print('   Output (manual) :', ot.detach().numpy())\n",
    "    print('   RNN output      :', output[:, t].detach().numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6177ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7c120",
   "metadata": {},
   "source": [
    "- We used the `hyperbolic tangent (tanh)` activation function since it is also used in `RNN (default activation)`. \n",
    "- Outputs from the manual forward computations exactly match the output of the `RNN` layer at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013a368",
   "metadata": {},
   "source": [
    "### **The challenges of learning long-range interactions**\n",
    "\n",
    "`BPTT`, introduces some new challenges. Because of the multiplicative factor, in computing the gradients of a loss function, the so-called `vanishing` and `exploding` gradient problems arise.\n",
    "\n",
    "\n",
    "![Problems in computing the gradients of the loss function](./figures/15_08.png)\n",
    "\n",
    "\n",
    "In practice, there are at least three solutions to this problem:\n",
    "\n",
    "- `Gradient clipping`\n",
    "- `Truncated backpropagation through time (TBPTT)`\n",
    "- `LSTM`\n",
    "\n",
    "\n",
    "* Using `gradient clipping`, we specify a cut-off or threshold value for the gradients, and we assign this cut-off value to gradient values that exceed this value. \n",
    "\n",
    "* In contrast, `TBPTT` simply limits the number of time steps that the signal can `backpropagate` after each forward pass. For example, even if the sequence has `100` elements or steps, we may only backpropagate the most recent `20` time steps.\n",
    "\n",
    "* `LSTM` has been more successful in vanishing and exploding gradient problems while modeling long-range dependencies through the use of memory cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4b4ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324aa7a6",
   "metadata": {},
   "source": [
    "### **Long short-term memory cells**\n",
    "\n",
    "`LSTMs` were introduced to overcome the vanishing gradient problem. The building block of an LSTM is a `memory cell`, which essentially represents or replaces the hidden layer of standard `RNNs`.\n",
    "\n",
    "In each memory cell, there is a recurrent edge that has the desirable weight, $w = 1$, to overcome the `vanishing` and `exploding` gradient problems. The values associated with this recurrent edge are collectively called the `cell state`. \n",
    "\n",
    "Below is the unfolded structure of a modern `LSTM` cell;\n",
    "\n",
    "![The structure of an LSTM cell](./figures/15_09.png)\n",
    "\n",
    "\n",
    "Notice that the cell state from the previous time step, $C^{(t–1)}$, is modified to get the cell state at the current time step, $C^{(t)}$, without being multiplied directly by any weight factor. The flow of information in this memory cell is controlled by several computation units (often called gates) that will be described here. In the figure, `⨀` refers to the `element-wise product` (element-wise multiplication) and `⨁` means `element-wise summation` (element-wise addition). Furthermore, $x^{(t)}$ refers to the input data at time $t$, and $h^{(t–1)}$ indicates the hidden units at time $t – 1$. Four boxes are indicated with an activation function, either the sigmoid function ($\\sigma$) or $tanh$, and a set of weights; these boxes apply a linear combination by performing matrix-vector multiplications on their inputs (which are $h^{(t–1)}$ and $x^{(t)}$). These units of computation with sigmoid activation functions, whose output units are passed through `⨀`, are called *gates*.\n",
    "\n",
    "\n",
    "In an LSTM cell, there are three different types of *gates*, which are known as the \n",
    "- forget gate, \n",
    "- the input gate, and \n",
    "- the output gate:\n",
    "\n",
    "- The `forget gate` $(f_{t})$ allows the memory cell to reset the cell state without growing indefinitely. In fact, the forget gate decides which information is allowed to go through and which information to suppress. \n",
    "\n",
    "Now, $f_t$ is computed as follows:\n",
    "\n",
    "$$f_{t} = \\sigma(W_{xf}x^{(t)} + W_{hf}h^{(t - 1)} + b_{f})$$\n",
    "\n",
    "\n",
    "- The `input gate` $(i_{t})$ and `candidate value` $(\\tilde{C})$ are responsible for updating the cell state. They are computed as follows:\n",
    "\n",
    "\n",
    "$$i_{t} = \\sigma(W_{xi}x^{(t)} + W_{hi}h^{(t - 1)} + b_{i})$$\n",
    "\n",
    "$$\\tilde{C} = \\tanh(W_{xc}x^{(t)} + W_{hc}h^{(t - 1)} + b_{c})$$\n",
    "\n",
    "\n",
    "The cell state at time t is computed as follows:\n",
    "\n",
    "\n",
    "$$C^{(t)} = (C^{(t - 1)} ⨀ f_{t}) ⨁ (i_{t} ⨀ \\tilde{C}_{t})$$\n",
    "\n",
    "\n",
    "The `output gate` $(o_{t})$ decides how to update the values of hidden units:\n",
    "\n",
    "$$o_{t} = \\sigma(W_{xo}x^{(t)} + W_{ho}h^{(t - 1)} + b_{o})$$\n",
    "\n",
    "\n",
    "Given this, the hidden units at the current time step are computed as follows:\n",
    "\n",
    "$$h^{(t)} = o_{t}⨀\\tanh(C^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e6432",
   "metadata": {},
   "source": [
    "The structure of an `LSTM` cell and its underlying computations might seem very complex and hard to implement. However, the good news is that `PyTorch` has already implemented everything in optimized wrapper functions, which allows us to define our LSTM cells easily and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf44ea",
   "metadata": {},
   "source": [
    "### **Long Short-Term Memory (LSTM) Networks: Deep Dive into Equations and Gradient Flow**\n",
    "\n",
    "\n",
    "\n",
    "**1. Motivation**\n",
    "\n",
    "Standard RNNs suffer from **vanishing/exploding gradients** because their memory update is **multiplicative**:\n",
    "\n",
    "$$h_t = \\phi(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$\n",
    "\n",
    "Gradients propagate through repeated multiplication by $`W_{hh}`$, which becomes unstable over long time spans.\n",
    "\n",
    "LSTMs solve this by introducing an **explicit memory cell** with **additive** state updates, enabling stable gradient flow across long sequences.\n",
    "\n",
    "\n",
    "\n",
    "**2. LSTM Architecture Overview**\n",
    "\n",
    "At each timestep $`t`$, the LSTM maintains two states:\n",
    "\n",
    "* **Hidden state** $`h_t`$\n",
    "\n",
    "* **Cell state** $`c_t`$ (long-term memory)\n",
    "\n",
    "The cell state allows **additive accumulation** of information, preventing vanishing gradients.\n",
    "\n",
    "\n",
    "\n",
    "**3. LSTM Forward Equations**\n",
    "\n",
    "Define the input at timestep $`t`$ as $`x_t`$, previous hidden state $`h_{t-1}`$, and previous cell state $`c_{t-1}`$.\n",
    "\n",
    "Concatenate:\n",
    "\n",
    "$$z_t = \\begin{bmatrix} x_t \\\\ h_{t-1} \\end{bmatrix}$$\n",
    "\n",
    "#### Gates (sigmoid activations):\n",
    "\n",
    "1. **Forget Gate** (controls what to erase)\n",
    "   $`f_t = \\sigma(W_f z_t + b_f)`$\n",
    "\n",
    "2. **Input Gate** (controls what to write)\n",
    "   $`i_t = \\sigma(W_i z_t + b_i)`$\n",
    "\n",
    "3. **Output Gate** (controls what to expose)\n",
    "   $`o_t = \\sigma(W_o z_t + b_o)`$\n",
    "\n",
    "#### Candidate Cell Content (tanh activation):\n",
    "\n",
    "$$\\tilde{c}_t = \\tanh(W_c z_t + b_c)$$\n",
    "\n",
    "#### Cell State Update (additive):\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "\n",
    "#### Hidden State Update:\n",
    "\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "| Term              | Meaning                          |\n",
    "| ----------------- | -------------------------------- |\n",
    "| $`\\sigma(\\cdot)`$ | Logistic sigmoid gate activation |\n",
    "| $`\\tanh(\\cdot)`$  | Candidate state nonlinearity     |\n",
    "| $`\\odot`$         | Element-wise multiplication      |\n",
    "\n",
    "\n",
    "\n",
    "**4. Gradient Flow Through the Cell State**\n",
    "\n",
    "The key innovation is visible in:\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "\n",
    "Differentiate w.r.t. the previous cell state:\n",
    "\n",
    "$$\\frac{\\partial c_t}{\\partial c_{t-1}} = f_t$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\frac{\\partial c_T}{\\partial c_t} = \\prod_{k=t+1}^{T} f_k$$\n",
    "\n",
    "Since gate activations $`f_k \\in (0, 1)`$, gradients **scale smoothly**, without uncontrolled decay or explosion.\n",
    "\n",
    "This **additive path** is why LSTMs retain information over long sequences.\n",
    "\n",
    "\n",
    "\n",
    "**5. Gradient Through the Hidden State**\n",
    "\n",
    "Hidden state:\n",
    "\n",
    "$`h_t = o_t \\odot \\tanh(c_t)`$\n",
    "\n",
    "Gradient contributes through both $`o_t`$ and $`c_t`$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial c_t}\n",
    "=================================\n",
    "\n",
    "o_t \\odot \\left(1 - \\tanh^2(c_t)\\right)\n",
    "$$\n",
    "\n",
    "Combined with the previous gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial c_t}\n",
    "=========================================\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h_t} \\odot o_t \\odot \\left(1 - \\tanh^2(c_t)\\right)\n",
    "+\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial c_{t+1}} \\odot f_{t+1}\n",
    "$$\n",
    "\n",
    "**Important Insight**\n",
    "\n",
    "* The recurrence involves **addition**, not pure multiplication.\n",
    "* This prevents gradients from collapsing through deep time.\n",
    "\n",
    "\n",
    "\n",
    "**6. Why LSTMs Avoid Vanishing Gradients**\n",
    "\n",
    "| Model        | Memory Update          | Gradient Path             | Stability                   |\n",
    "| ------------ | ---------------------- | ------------------------- | --------------------------- |\n",
    "| Standard RNN | Multiplicative         | $`\\prod W_{hh}`$          | Unstable for long sequences |\n",
    "| LSTM         | Additive (via $`c_t`$) | $`\\prod f_t`$ with gating | Stable long-term behavior   |\n",
    "\n",
    "The forget gate $`f_t`$ acts as a **learned decay coefficient**, enabling **controlled memory retention**.\n",
    "\n",
    "\n",
    "\n",
    "**7. Interpretation of Gates**\n",
    "\n",
    "| Gate            | Interpretation    | Effect                                    |\n",
    "| --------------- | ----------------- | ----------------------------------------- |\n",
    "| $`f_t`$         | What to forget    | Controls decay of past memory             |\n",
    "| $`i_t`$         | What to write now | Controls strength of new information      |\n",
    "| $`o_t`$         | What to expose    | Controls visible influence on output      |\n",
    "| $`\\tilde{c}_t`$ | Candidate content | The new information proposed to be stored |\n",
    "\n",
    "This separation allows the network to **store**, **retain**, and **expose** information selectively.\n",
    "\n",
    "\n",
    "\n",
    "**8. Summary**\n",
    "\n",
    "* LSTMs introduce a **cell state $`c_t`$** that evolves **additively**, ensuring stable gradient flow.\n",
    "* Gates regulate information storage and retrieval.\n",
    "* The key mathematical stability comes from:\n",
    "\n",
    "$`\\frac{\\partial c_t}{\\partial c_{t-1}} = f_t`$\n",
    "\n",
    "instead of repeated multiplication by weight matrices.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
