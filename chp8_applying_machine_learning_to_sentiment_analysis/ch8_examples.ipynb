{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7003700b",
   "metadata": {},
   "source": [
    "# **Applying Machine Learning to Sentiment Analysis**\n",
    "\n",
    "\n",
    "- Preparing the IMDb movie review data for text processing\n",
    "    - Obtaining the IMDb movie review dataset\n",
    "    - Preprocessing the movie dataset into more convenient format\n",
    "\n",
    "- Introducing the bag-of-words model\n",
    "    - Transforming words into feature vectors\n",
    "    - Assessing word relevancy via term frequency-inverse document frequency\n",
    "    - Cleaning text data\n",
    "    - Processing documents into tokens\n",
    "\n",
    "- Training a logistic regression model for document classification\n",
    "\n",
    "- Working with bigger data – online algorithms and out-of-core learning\n",
    "\n",
    "- Topic modeling\n",
    "    - Decomposing text documents with Latent Dirichlet Allocation\n",
    "    - Latent Dirichlet Allocation with scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca268a23",
   "metadata": {},
   "source": [
    "**Sentiment Analysis**\n",
    "\n",
    "\n",
    "### **1. Overview**\n",
    "\n",
    "**Sentiment Analysis** is a Natural Language Processing (NLP) technique that determines the **emotional tone**, **opinion**, or **attitude** expressed in text.\n",
    "It classifies text as **positive**, **negative**, or **neutral**, and in more advanced setups, as **multi-class emotions** (e.g., joy, anger, sadness).\n",
    "\n",
    "\n",
    "\n",
    "### **2. Core Workflow**\n",
    "\n",
    "| Step                      | Description                                                                              |\n",
    "| ------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| **1. Data Collection**    | Gather labeled datasets (e.g., tweets, product reviews).                                 |\n",
    "| **2. Text Preprocessing** | Clean and normalize text: remove stop words, punctuation, apply lemmatization/stemming.  |\n",
    "| **3. Feature Extraction** | Convert text into numerical form — Bag-of-Words, TF-IDF, or embeddings (Word2Vec, BERT). |\n",
    "| **4. Model Training**     | Train classifiers such as Logistic Regression, Naïve Bayes, LSTM, or Transformer models. |\n",
    "| **5. Evaluation**         | Use metrics like accuracy, precision, recall, F1-score, and confusion matrix.            |\n",
    "| **6. Prediction**         | Infer sentiment of unseen data.                                                          |\n",
    "\n",
    "\n",
    "\n",
    "### **3. Mathematical Foundation**\n",
    "\n",
    "#### **A. Bag-of-Words (BoW) / TF-IDF**\n",
    "\n",
    "Text is vectorized into counts or weighted frequencies:\n",
    "\n",
    "$$TFIDF(w, d) = TF(w, d) \\times \\log\\left(\\frac{N}{DF(w)}\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $`TF(w, d)`$ = term frequency of word *w* in document *d*\n",
    "* $`DF(w)`$ = number of documents containing *w*\n",
    "* $`N`$ = total number of documents\n",
    "\n",
    "\n",
    "\n",
    "#### **B. Logistic Regression for Sentiment**\n",
    "\n",
    "For binary sentiment classification:\n",
    "\n",
    "$$P(y = 1 | x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$\n",
    "\n",
    "The model learns parameters $`w`$ and $`b`$ by minimizing the **binary cross-entropy loss**:\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)]$$\n",
    "\n",
    "\n",
    "\n",
    "#### **C. Neural Network (e.g., LSTM / Transformer)**\n",
    "\n",
    "Text sequences are embedded (using pre-trained vectors or embeddings) and fed through recurrent or transformer layers to capture context and dependencies:\n",
    "\n",
    "$$h_t = f(W_h x_t + U_h h_{t-1} + b_h)$$\n",
    "\n",
    "Final hidden states are passed to a softmax layer for sentiment classification.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Advanced Models**\n",
    "\n",
    "| Model Type                                   | Description                                                           |\n",
    "| -------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **Naïve Bayes**                              | Assumes feature independence; performs well on simple text data.      |\n",
    "| **Logistic Regression / SVM**                | Strong baselines using TF-IDF features.                               |\n",
    "| **LSTM / GRU**                               | Captures sequential dependencies and contextual meaning.              |\n",
    "| **Transformer-based Models (BERT, RoBERTa)** | Leverage bidirectional context; state-of-the-art for sentiment tasks. |\n",
    "\n",
    "\n",
    "\n",
    "### **5. Evaluation Metrics**\n",
    "\n",
    "* **Accuracy:** Overall correctness\n",
    "* **Precision:** Correct positive predictions\n",
    "* **Recall:** Coverage of actual positives\n",
    "* **F1-score:** Balance between precision and recall\n",
    "* **ROC-AUC:** Quality of classification threshold\n",
    "\n",
    "\n",
    "\n",
    "### **6. Use Cases**\n",
    "\n",
    "| Domain                   | Application                                             |\n",
    "| ------------------------ | ------------------------------------------------------- |\n",
    "| **Business / Marketing** | Customer feedback analysis, brand reputation monitoring |\n",
    "| **Finance**              | Sentiment-based stock prediction, market trend analysis |\n",
    "| **Healthcare**           | Patient feedback or stress analysis                     |\n",
    "| **Social Media**         | Public opinion mining, political sentiment tracking     |\n",
    "\n",
    "\n",
    "\n",
    "### **7. Key Insights**\n",
    "\n",
    "* **Imbalanced data** is common — use oversampling, undersampling, or class weights.\n",
    "* **Pretrained embeddings (e.g., BERT, GloVe)** significantly boost accuracy.\n",
    "* **Explainability** (e.g., SHAP, LIME) helps interpret model predictions.\n",
    "* **Context matters** — the same word may differ by domain (\"killer app\" vs. \"killer bacteria\").\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa6110",
   "metadata": {},
   "source": [
    "## **Preparing the IMDb movie review data for text processing**\n",
    "\n",
    "Sentiment analysis, sometimes also called `opinion mining`, is a popular subdiscipline of the broader field of `NLP`; it is concerned with analyzing the sentiment of documents. A popular task in sentiment analysis is the classification of documents based on the expressed opinions or emotions of the authors with regard to a particular topic.\n",
    "\n",
    "- We will be working with a large dataset of movie reviews from `IMDb`.\n",
    "- The movie review dataset consists of `50,000` polar movie reviews that are labeled as either positive or negative; \n",
    "  - positive means that a movie was rated with more than six stars on IMDb, and\n",
    "  - negative means that a movie was rated with fewer than five stars on `IMDb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cb11f",
   "metadata": {},
   "source": [
    "### **Obtaining the movie review dataset**\n",
    "\n",
    "- A compressed archive of the movie review dataset (84.1 MB) can be downloaded from http://ai.stanford.edu/~amaas/data/sentiment/ as a gzip-compressed tarball archive.\n",
    "\n",
    "\n",
    "### Preprocessing the movie dataset into a more convenient format\n",
    "\n",
    "- Read the movie reviews into a pandas DataFrame object.\n",
    "- Use the Python Progress Indicator (PyPrind) package, which was developed several years ago for such purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dee7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:18\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from packaging import version\n",
    "\n",
    "basepath = '../data/aclImdb'\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "\n",
    "# if the progress bar does not show, change stream=sys.stdout to stream=2\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "                \n",
    "            if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n",
    "                x = pd.DataFrame([[txt, labels[l]]], columns=['review', 'sentiment'])\n",
    "                df = pd.concat([df, x], ignore_index=False)\n",
    "\n",
    "            else:\n",
    "                df = df.append([[txt, labels[l]]], \n",
    "                               ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8412161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Towards the end of the movie, I felt it was to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the kind of movie that my enemies cont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "0   I went and saw this movie last night after bei...          1\n",
       "0   Actor turned director Bill Paxton follows up h...          1\n",
       "0   As a recreational golfer with some knowledge o...          1\n",
       "0   I saw this film in a sneak preview, and it is ...          1\n",
       "0   Bill Paxton has taken the true story of the 19...          1\n",
       "..                                                ...        ...\n",
       "0   Towards the end of the movie, I felt it was to...          0\n",
       "0   This is the kind of movie that my enemies cont...          0\n",
       "0   I saw 'Descent' last night at the Stockholm Fi...          0\n",
       "0   Some films that you pick up for a pound turn o...          0\n",
       "0   This is one of the dumbest films, I've ever se...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6f5923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932fd752",
   "metadata": {},
   "source": [
    "- In the preceding code, we first initialized a new progress bar object, `pbar`, with `50,000 iterations`, which was the number of documents we were going to read in. Using the nested for loops, we iterated over the `train` and `test` subdirectories in the main `aclImdb` directory and read the individual text files from the `pos` and `neg` subdirectories that we eventually appended to the `df pandas DataFrame`, together with an integer class label `(1 = positive and 0 = negative)`.\n",
    "\n",
    "\n",
    "- Since the `class labels` in the assembled dataset are sorted, we will now shuffle the DataFrame using the permutation function from the `np.random` submodule—this will be useful for splitting the dataset into `training` and `test` datasets in later sections, when we will stream the data from our local drive directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa99430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as a CSV file\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b1688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "0  Actor turned director Bill Paxton follows up h...          1\n",
       "0  As a recreational golfer with some knowledge o...          1\n",
       "0  I saw this film in a sneak preview, and it is ...          1\n",
       "0  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23badfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm data is saved as CSV\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "# column renaming is necessary on some computers:\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519af1cf",
   "metadata": {},
   "source": [
    "## **Introducing the bag-of-words model**\n",
    "\n",
    "In this section, we will introduce the `bag-of-words` model, which allows us to represent text as numerical feature vectors. The idea behind `bag-of-words` is quite simple and can be summarized as follows:\n",
    "\n",
    "- We create a vocabulary of unique tokens—for example, words—from the entire set of documents.\n",
    "- We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.\n",
    "\n",
    "Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros, which is why we call them `sparse`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553627f5",
   "metadata": {},
   "source": [
    "### **Transforming words into feature vectors**\n",
    "\n",
    "To construct a `bag-of-words` model based on the word counts in the respective documents, we can use the `CountVectorizer` class implemented in scikit-learn. As you will see in the following code section, `CountVectorizer` takes an array of text data, which can be documents or sentences, and constructs\n",
    "the bag-of-words model for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63188be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0193811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The sun is shining', 'The weather is sweet',\n",
       "       'The sun is shining, the weather is sweet, and one and one is two'],\n",
       "      dtype='<U64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39f7441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b340e5",
   "metadata": {},
   "source": [
    "By calling the `fit_transform` method on `CountVectorizer`, we constructed the vocabulary of the bag-of-words model and transformed the following three sentences into sparse feature vectors:\n",
    "\n",
    "- 'The sun is shining'\n",
    "- 'The weather is sweet'\n",
    "- 'The sun is shining, the weather is sweet, and one and one is two'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "# print contents of the vocabulary\n",
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f24eb",
   "metadata": {},
   "source": [
    "- As you can see from executing the preceding command, the vocabulary is stored in a Python dictionary that maps the unique words to integer indices. Next, let’s print the `feature vectors` that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc4b2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38be1b5",
   "metadata": {},
   "source": [
    "- Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the `CountVectorizer` vocabulary. For example, the first feature at index position 0 resembles the count of the word \"and\", which only occurs in the last document, and the word \"is\" at index position 1 (the 2nd feature in the document vectors) occurs in all three sentences. Those values in the feature vectors are also called the raw term frequencies: `tf (t,d)`—the number of times a term `t` occurs in a document `d`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b08d9b",
   "metadata": {},
   "source": [
    "### **Assessing word relevancy via term frequency-inverse document frequency**\n",
    "\n",
    "When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweigh those frequently occurring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency:\n",
    "\n",
    "\n",
    "$$tf-idf(t, d) = tf(t, d) x idf(t, d)$$\n",
    "\n",
    "Here the tf(t, d) is the term frequency that we introduced in the previous section, and the inverse document frequency idf(t, d) can be calculated as:\n",
    "\n",
    "$$idf(t, d) = \\log\\frac{n_{d}}{1 + df(d, t)}$$\n",
    "\n",
    "\n",
    "where \n",
    "- $n_{d}$ is the total number of documents, and \n",
    "- $df(d, t)$ is the number of documents $d$ that contain the term $t$. \n",
    "- Note that adding the constant `1` to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training examples; \n",
    "- the `log` is used to ensure that low document frequencies are not given too much weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75615283",
   "metadata": {},
   "source": [
    "- Scikit-learn implements yet another transformer, the `TfidfTransformer` class, that takes the raw term frequencies from `CountVectorizer` as input and transforms them into `tf-idfs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300d7efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43370786 0.         0.55847784 0.55847784 0.\n",
      "  0.43370786 0.         0.        ]\n",
      " [0.         0.43370786 0.         0.         0.         0.55847784\n",
      "  0.43370786 0.         0.55847784]\n",
      " [0.50238645 0.44507629 0.50238645 0.19103892 0.19103892 0.19103892\n",
      "  0.29671753 0.25119322 0.19103892]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         norm='l2', \n",
    "                         smooth_idf=True)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs))\n",
    "      .toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3c83543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43370786 0.         0.55847784 0.55847784 0.\n",
      "  0.43370786 0.         0.        ]\n",
      " [0.         0.43370786 0.         0.         0.         0.55847784\n",
      "  0.43370786 0.         0.55847784]\n",
      " [0.50238645 0.44507629 0.50238645 0.19103892 0.19103892 0.19103892\n",
      "  0.29671753 0.25119322 0.19103892]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.fit_transform(bag).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e41ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit_transform(bag).toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38045fb5",
   "metadata": {},
   "source": [
    "As we saw in the previous subsection, the word \"is\" had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into `tf-idfs`, we see that the word \"is\" is now associated with a relatively small `tf-idf` (0.45) in document 3 since it is also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information.\n",
    "\n",
    "\n",
    "However, if we'd manually calculated the `tf-idfs` of the individual terms in our feature vectors, we'd have noticed that the `TfidfTransformer` calculates the tf-idfs slightly differently compared to the standard textbook equations that we defined earlier. The equations for the idf and `tf-idf` that were implemented in scikit-learn are:\n",
    "\n",
    "\n",
    "$$idf(t, d) = \\log\\frac{1 + n_{d}}{1 + df(d, t)}$$\n",
    "\n",
    "\n",
    "The `tf-idf` equation that was implemented in scikit-learn is as follows:\n",
    "\n",
    "$$tf-idf(t, d) = tf(t, d) x (idf(t, d) + 1)$$\n",
    "\n",
    "\n",
    "Note that the `\"+1\"` in the previous `idf` equation is due to setting `smooth_idf=True` in the previous code example, which is helpful for assigning zero weight (that is, `idf(t, d) = log(1) = 0`) to terms that occur in all documents.\n",
    "\n",
    "\n",
    "While it is also more typical to normalize the raw term frequencies before calculating the `tf-idfs`, the `TfidfTransformer` normalizes the `tf-idfs` directly.\n",
    "\n",
    "By default `(norm='l2')`, scikit-learn's `TfidfTransformer` applies the `L2-normalization`, which returns a vector of length 1 by dividing an un-normalized feature vector `v` by its `L2-norm`:\n",
    "\n",
    "$$v_{norm} = \\frac{v}{||v||_{2}} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + ... + v_{n}^{2}}} = \\frac{v}{(\\sum _{i=1}^{n} v_{i}^{2})^{1/2}}$$\n",
    "\n",
    "To make sure that we understand how `TfidfTransformer` works, let us walk through an example and calculate the `tf-idf` of the word \"is\" in the 3rd document.\n",
    "\n",
    "The word \"is\" has a term frequency of 3 (tf = 3) in document 3 (), and the document frequency of this term is 3 since the term \"is\" occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n",
    "\n",
    "$$idf(\"is\", d_{3}) = \\log\\frac{1 + 3}{1 + 3} = 0$$\n",
    "\n",
    "\n",
    "Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n",
    "\n",
    "$$tf-idf(\"is\", d_{3}) = 3 \\times (0 + 1) = 3$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0506c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf of term \"is\" = 3.00\n"
     ]
    }
   ],
   "source": [
    "tf_is = 3\n",
    "n_docs = 3\n",
    "idf_is = np.log((n_docs+1) / (3+1))\n",
    "tfidf_is = tf_is * (idf_is + 1)\n",
    "print(f'tf-idf of term \"is\" = {tfidf_is:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140c713",
   "metadata": {},
   "source": [
    "If we repeated these calculations for all terms in the 3rd document, we'd obtain the following `tf-idf` vectors: `[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]`. However, we notice that the values in this feature vector are different from the values that we obtained from the `TfidfTransformer` that we used previously. The final step that we are missing in this `tf-idf` calculation is the `L2-normalization`, which can be applied as follows:\n",
    "\n",
    "\n",
    "\n",
    "$$tf-idf(d_{3})_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]}{\\sqrt{3.39^{2} + 3.0^{2} + 3.39^{2} + 1.29^{2} + 1.29^{2} + 1.29^{2} + 2.0^{2} + 1.69^{2} + 1.29{2}}} = [0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
    "\n",
    "\n",
    "$tf-idf(\"is\", d_{3}) = 0.45$\n",
    "\n",
    "\n",
    "As we can see, the results match the results returned by scikit-learn's `TfidfTransformer` (below). Since we now understand how `tf-idfs` are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22a71daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.38629436, 3.        , 3.38629436, 1.28768207, 1.28768207,\n",
       "       1.28768207, 2.        , 1.69314718, 1.28768207])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
    "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
    "raw_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e193a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50238645, 0.44507629, 0.50238645, 0.19103892, 0.19103892,\n",
       "       0.19103892, 0.29671753, 0.25119322, 0.19103892])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
    "l2_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c864712e",
   "metadata": {},
   "source": [
    "### **Cleaning text data**\n",
    "\n",
    "- Before we build our bag-of-words model, its ideal to clean the text data by stripping it of all unwanted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a708c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". <br /><br />This is one I'd recommend to anyone.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[3, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958c61f",
   "metadata": {},
   "source": [
    "- As you can see here, the text contains `HTML` markup as well as punctuation and other non-letter characters. While `HTML` markup does not contain many useful semantics, punctuation marks can represent useful, additional information in certain `NLP` contexts. However, for simplicity, we will now remove all punctuation marks except for emoticon characters, since those are certainly useful for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721ca85",
   "metadata": {},
   "source": [
    "- Using Python's `regular expression (regex)` library, `re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b889b",
   "metadata": {},
   "source": [
    "- Via the first regex, `<[^>]*>`, in the preceding code section, we tried to remove all of the HTML markup from the movie reviews. Although many programmers generally advise against the use of regex to parse HTML, this regex should be sufficient to clean this particular dataset. Since we are only interested in removing HTML markup and do not plan to use the HTML markup further, using regex to do the job should be acceptable.\n",
    "\n",
    "- After we removed the HTML markup, we used a slightly more complex regex to find emoticons, which we temporarily stored as emoticons. Next, we removed all non-word characters from the text via the regex `[\\W]+` and converted the text into lowercase characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34956ab",
   "metadata": {},
   "source": [
    "- Although the addition of the `emoticon` characters to the end of the cleaned document strings may not look like the most elegant approach, we must note that the order of the words doesn’t matter in our `bag-of-words` model if our vocabulary consists of only one-word tokens. But before we talk more about the splitting of documents into individual terms, words, or tokens, let’s confirm that our preprocessor function works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this is one i d recommend to anyone '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[3, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83dae0",
   "metadata": {},
   "source": [
    "- Let's now apply our `preprocessor` function to all the movie reviews in our `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actor turned director bill paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i saw this film in a sneak preview and it is d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bill paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  i went and saw this movie last night after bei...          1\n",
       "1  actor turned director bill paxton follows up h...          1\n",
       "2  as a recreational golfer with some knowledge o...          1\n",
       "3  i saw this film in a sneak preview and it is d...          1\n",
       "4  bill paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13af898",
   "metadata": {},
   "source": [
    "### **Processing documents into tokens**\n",
    "\n",
    "- Split the text corpora into individual elements.\n",
    "- One way to `tokenize` documents is to split them into individual words by splitting the cleaned documents at their whitespace characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Porter stemming algorithm\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f3c40d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04cf7ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde0334",
   "metadata": {},
   "source": [
    "- Using the `PorterStemmer` from the `nltk` package, we modified our tokenizer function to reduce words to their root form, which was illustrated by the simple preceding example where the word 'running' was stemmed to its root form 'run'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e5a470",
   "metadata": {},
   "source": [
    "**Stemming algorithms**\n",
    "\n",
    "While stemming can create non-real words, such as `'thu' (from 'thus')`, as shown in the previous example, a technique called `lemmatization` aims to obtain the canonical (grammatically correct) forms of individual words—the so-called `lemmas`. However, `lemmatization` is computationally more difficult and expensive compared to `stemming` and, in practice, it has been observed that stemming and `lemmatization` have little impact on the performance of text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af928a1",
   "metadata": {},
   "source": [
    "**Stop word removal**\n",
    "\n",
    "Stop words are common words like `\"the,\"` `\"a,\"` and `\"is\"` that are often removed during Natural Language Processing (NLP) to reduce noise and focus on more meaningful content. By removing these high-frequency, low-information words, NLP tasks become more efficient and accurate, improving performance in areas like search engines and topic modeling. However, their removal is not always necessary, and in some contexts, such as sentiment analysis where words like `\"not\"` are crucial, they should be retained.\n",
    "\n",
    "\n",
    "**Why stop words are removed**\n",
    "\n",
    "- `Increase efficiency:` Removing stop words reduces the amount of data that needs to be processed, which can speed up analysis and reduce computational costs. \n",
    "\n",
    "- `Improve accuracy:` It helps algorithms focus on the most informative words, leading to more accurate results in tasks like search and topic modeling. \n",
    "\n",
    "- `Reduce noise:` By filtering out common grammatical words, NLP models can better identify the core meaning of a text. \n",
    "\n",
    "- `Example:` In the sentence, \"The quick brown fox jumps over the lazy dog,\" removing \"the\" and \"over\" leaves \"quick brown fox jumps lazy dog,\" which retains the core meaning for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b4f2f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ade16",
   "metadata": {},
   "source": [
    "- After we download the stop words set, we can load and apply the English stop word set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43c9d784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21eaca",
   "metadata": {},
   "source": [
    "## **Training a logistic regression model for document classification**\n",
    "\n",
    "- Train logistic regression model to classify the movie reviews into `positive` and `negative` reviews based on the bag-of-words model.\n",
    "- Divide data into `25k` training and `25k` testing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5065979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                              review  sentiment\n",
       " 0  i went and saw this movie last night after bei...          1\n",
       " 1  actor turned director bill paxton follows up h...          1\n",
       " 2  as a recreational golfer with some knowledge o...          1\n",
       " 3  i saw this film in a sneak preview and it is d...          1\n",
       " 4  bill paxton has taken the true story of the 19...          1,\n",
       " (50000, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1334b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6049028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25001,), (25000,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804a833",
   "metadata": {},
   "source": [
    "- `GridSearchCV` object to find the optimal set of parameters for our logistic regression model using `5-fold` stratified cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f9339d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None,\n",
    "                        token_pattern=None)\n",
    "\n",
    "small_param_grid = [{'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "                     'vect__stop_words': [None],\n",
    "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                     'clf__C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "                     'vect__use_idf': [True],\n",
    "                     'vect__min_df': [2, 5],\n",
    "                     'vect__max_df': [0.8, 1.0],\n",
    "                     'vect__sublinear_tf': [True]},\n",
    "                    {'vect__ngram_range': [(1, 1), (1, 3)],\n",
    "                     'vect__stop_words': [stop, None],\n",
    "                     'vect__tokenizer': [tokenizer],\n",
    "                     'vect__use_idf':[False],\n",
    "                     'vect__norm':[None],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                  'clf__C': [0.01, 0.1, 1.0, 10.0, 100]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, \n",
    "                           small_param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f622c",
   "metadata": {},
   "source": [
    "- Note that for the logistic regression classifier, we are using the `LIBLINEAR` solver as it can perform better than the default choice `('lbfgs')` for relatively large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187037c",
   "metadata": {},
   "source": [
    "**Multiprocessing via the n_jobs parameter**\n",
    "\n",
    "- Please note that it is highly recommended to use `n_jobs=-1` (instead of `n_jobs=1`) in the previous code example to utilize all available cores on your machine and speed up the grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad163a9",
   "metadata": {},
   "source": [
    "- In the previous code example, we replaced `CountVectorizer` and `TfidfTransformer` from the previous subsection with `TfidfVectorizer`, which combines `CountVectorizer` with the `TfidfTransformer`.\n",
    "\n",
    "- Our `param_grid` consisted of two parameter dictionaries. \n",
    "  - In the first dictionary, we used `TfidfVectorizer` with its default settings `(use_idf=True, smooth_idf=True, and norm='l2')` to calculate the `tf-idfs`; in the second dictionary, we set those parameters to `use_idf=False`, `smooth_idf=False`, and `norm=None` in order to train a model based on raw term frequencies.\n",
    "  - Furthermore, for the logistic regression classifier itself, we trained models using `L2` regularization via the penalty parameter and compared different regularization strengths by defining a range of values for the inverse-regularization parameter `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53859e5",
   "metadata": {},
   "source": [
    "- Print the best parameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3324f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x1620294c0>}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125aa80",
   "metadata": {},
   "source": [
    "- Get the average `5-fold` cross-validation accuracy scores on the training dataset and the classification accuracy on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "885bea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.873\n"
     ]
    }
   ],
   "source": [
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = gs_lr_tfidf.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4efd806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                                 tokenizer=&lt;function tokenizer at 0x1620294c0&gt;)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                 TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                                 tokenizer=&lt;function tokenizer at 0x1620294c0&gt;)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                tokenizer=&lt;function tokenizer at 0x1620294c0&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(lowercase=False, token_pattern=None,\n",
       "                                 tokenizer=<function tokenizer at 0x1620294c0>)),\n",
       "                ('clf', LogisticRegression(solver='liblinear'))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21f951ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.881\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0c3de",
   "metadata": {},
   "source": [
    "- The results reveal that our machine learning model can predict whether a movie review is `positive` or `negative` with `88` percent accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd346779",
   "metadata": {},
   "source": [
    "**Make Prediction for a new text input (review) using our trained model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ed1d6588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "95b86ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Example input review\n",
    "new_review = [\"The movie was absolutely good and entertaining!, I had a pretty good time\"]\n",
    "\n",
    "# Predict sentiment (1 = positive, 0 = negative)\n",
    "prediction = gs_lr_tfidf.best_estimator_.predict(new_review)\n",
    "\n",
    "# Display result\n",
    "print(\"Predicted Sentiment:\", \"Positive\" if prediction[0] == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9707e6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability (Negative, Positive): [0.332789 0.667211]\n"
     ]
    }
   ],
   "source": [
    "proba = gs_lr_tfidf.best_estimator_.predict_proba(new_review)\n",
    "print(\"Probability (Negative, Positive):\", proba[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c150f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b75242b7",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "Please not that `gs_lr_tfidf.best_score_` is the avergae k-fold cross-validation score. i.e., if we have a `GridSearchCV` object with 5-fold `cross-validation` (like the one above), the `best_score_` attribute returns the average score over the 5-folds of the best model. To illustrate this with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c131440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0.4, 0.6, 0.2, 0.6])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=6)\n",
    "y = [np.random.randint(3) for i in range(25)]\n",
    "X = (y + np.random.randn(25)).reshape(-1, 1)\n",
    "\n",
    "cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False).split(X, y))\n",
    "    \n",
    "lr = LogisticRegression()\n",
    "cross_val_score(lr, X, y, cv=cv5_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4880833c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 1), 25)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41dbd25",
   "metadata": {},
   "source": [
    "- By executing the code above, we created a simple data set of random integers that shall represent our class labels. Next, we fed the indices of `5 cross-validation` folds `(cv3_idx)` to the `cross_val_score` scorer, which returned `5 accuracy scores` -- these are the 5 accuracy values for the 5 test folds.\n",
    "\n",
    "- Next, let us use the `GridSearchCV` object and feed it the same `5` cross-validation sets (via the pre-generated `cv3_idx` indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9af923e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END ..................................., score=0.600 total time=   0.0s\n",
      "[CV 2/5] END ..................................., score=0.400 total time=   0.0s\n",
      "[CV 3/5] END ..................................., score=0.600 total time=   0.0s\n",
      "[CV 4/5] END ..................................., score=0.200 total time=   0.0s\n",
      "[CV 5/5] END ..................................., score=0.600 total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr = LogisticRegression()\n",
    "gs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246873c",
   "metadata": {},
   "source": [
    "- As we can see, the scores for the `5` folds are exactly the same as the ones from `cross_val_score` earlier.\n",
    "\n",
    "- Now, the `best_score_` attribute of the `GridSearchCV` object, which becomes available after fitting, returns the average accuracy score of the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d5cc1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caefa69",
   "metadata": {},
   "source": [
    "- As we can see, the result above is consistent with the average score computed with `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5ef9e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "print(cross_val_score(lr, X, y, cv=cv5_idx).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4a7c6",
   "metadata": {},
   "source": [
    "**END OF NOTE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74289b53",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be979c",
   "metadata": {},
   "source": [
    "## **Working with bigger data - online algorithms and out-of-core learning**\n",
    "\n",
    "- Out-of-core learning, allows us to work with large datasets by fitting the classifier incrementally on smaller batches of a dataset.\n",
    "\n",
    "- Here, we will make use of the `partial_fit` function of `SGDClassifier` in scikit-learn to stream the documents directly from our local drive and train a logistic regression model using small mini-batches of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2777d7c",
   "metadata": {},
   "source": [
    "- Define a tokenizer function that cleans the unprocessed text data from the `movies_data.csv` file that we constructed at the beginning, and separates it into word tokens while removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8987be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                 + ' '.join(emoticons).replace('-','')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07f733",
   "metadata": {},
   "source": [
    "- Next, we dill define a generator function, `stream_docs`, that reads in and returns one document at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c55a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be8ee6",
   "metadata": {},
   "source": [
    "- Let's read in the first document from the `movie_data.csv` file, which should return a tuple consisting of the review text as well as the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad7d0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I went and saw this movie last night after being coaxed to by a few friends of mine. I\\'ll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\"',\n",
       " 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e72ae",
   "metadata": {},
   "source": [
    "- Define a function `get_minibatch`, that will take a document stream from the `stream_docs` function and return a particular number of documents specified by the size parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff3992",
   "metadata": {},
   "source": [
    "- Unfortunately, we can’t use `CountVectorizer` for out-of-core learning since it requires holding the complete vocabulary in memory. Also, `TfidfVectorizer` needs to keep all the feature vectors of the training dataset in memory to calculate the inverse document frequencies. However, another useful vectorizer for text processing implemented in scikit-learn is `HashingVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='hinge', random_state=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0611f8d",
   "metadata": {},
   "source": [
    "- We initialized `HashingVectorizer` with our tokenizer function and set the number of features to `2**21`. Furthermore, we reinitialized a logistic regression classifier by setting the loss parameter of `SGDClassifier` to `log`. \n",
    "\n",
    "- Note that by choosing a large number of features in `HashingVectorizer`, we reduce the chance of causing hash collisions, but we also increase the number of coefficients in our logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e18b0",
   "metadata": {},
   "source": [
    "- Start the out-of-core learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Title: \n",
       "  Started: 10/16/2025 03:03:01\n",
       "  Finished: 10/16/2025 03:03:01\n",
       "  Total time elapsed: 00:00:00"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()\n",
    "\n",
    "pbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69872dfe",
   "metadata": {},
   "source": [
    "- we iterated over `45 mini-batches` of documents where each mini-batch consists of `1,000 documents`. Having completed the incremental learning process, we will use the last `5,000 documents` to evaluate the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87ba002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49054489",
   "metadata": {},
   "source": [
    "- Finally, we can use the last `5,000` documents to update our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667407d5",
   "metadata": {},
   "source": [
    "**The word2vec model**\n",
    "\n",
    "The `word2vec` algorithm is an `unsupervised learning` algorithm based on neural networks that attempts to automatically learn the relationship between words. The idea behind `word2vec` is to put words that have similar meanings into similar clusters, and via clever vector spacing, the model can reproduce certain words using simple vector math, for example, `king – man + woman = queen`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b235bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8b8e1",
   "metadata": {},
   "source": [
    "## **Topic modeling with Latent Dirichlet allocation**\n",
    "\n",
    "`Topic modeling` describes the broad task of assigning topics to unlabeled text documents. For example, a typical application is the categorization of documents in a large text corpus of newspaper articles. In applications of topic modeling, we then aim to assign category labels to those articles, for example, `sports`, `finance`, `world news`, `politics`, and `local news`.\n",
    "\n",
    "- Topic modeling is considered a clustering task, a subcategory of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c71ceb",
   "metadata": {},
   "source": [
    "### **Decomposing text documents with Latent Dirichlet Allocation**\n",
    "\n",
    "`LDA` is a generative probabilistic model that tries to find groups of words that appear frequently together across different documents. These frequently appearing words represent our topics, assuming that each document is a mixture of different words.\n",
    "\n",
    "Given a bag-of-words matrix as input, `LDA` decomposes it into two new matrices:\n",
    "\n",
    "- A document-to-topic matrix\n",
    "- A word-to-topic matrix\n",
    "\n",
    "LDA decomposes the `bag-of-words` matrix in such a way that if we multiply those two matrices together, we will be able to reproduce the input, the bag-of-words matrix, with the lowest possible error. In practice, we are interested in those topics that LDA found in the bag-of-words matrix. The only downside may be that we must define the number of topics beforehand—the number of topics is a hyperparameter of LDA that has to be specified manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f323f5",
   "metadata": {},
   "source": [
    "### **Latent Dirichlet Allocation with scikit-learn**\n",
    "\n",
    "In this subsection, we will use the `LatentDirichletAllocation` class implemented in scikit-learn to decompose the movie review dataset and categorize it into different topics. In the following example, we will restrict the analysis to 10 different topics, but readers are encouraged to experiment with the hyperparameters of the algorithm to further explore the topics that can be found in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ce7cb",
   "metadata": {},
   "source": [
    "- Load data into `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "981a6c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a85ba",
   "metadata": {},
   "source": [
    "- Next, we are going to use the already familiar `CountVectorizer` to create the `bag-of-words` matrix as input to the `LDA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee486b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "\n",
    "X = count_vec.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2780504 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16235162",
   "metadata": {},
   "source": [
    "- Notice that we set the maximum document frequency of words to be considered to 10 percent `(max_df=.1)` to exclude words that occur too frequently across documents. The rationale behind the removal of frequently occurring words is that these might be common words appearing across all documents that are, therefore, less likely to be associated with a specific topic category of a given document. Also, we limited the number of words to be considered to the most frequently occurring `5,000` words `(max_features=5000)`, to limit the dimensionality of this dataset to improve the inference performed by `LDA`. However, both `max_df=.1` and `max_features=5000` are hyperparameter values chosen arbitrarily, and readers are encouraged to tune them while comparing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd4e75",
   "metadata": {},
   "source": [
    "- fit a `LatentDirichletAllocation` estimator to the `bag-of-words` matrix and infer the 10 different topics from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "148087f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.50091416e-03, 9.77494779e-01, 2.50051158e-03, ...,\n",
       "        2.50034654e-03, 2.50053199e-03, 2.50077758e-03],\n",
       "       [8.33526593e-04, 8.33606132e-04, 1.02795734e-01, ...,\n",
       "        8.33575277e-04, 8.33607028e-04, 1.44434277e-01],\n",
       "       [1.28221566e-03, 2.66683358e-01, 2.12125823e-01, ...,\n",
       "        1.28258459e-03, 1.25237669e-01, 1.28244482e-03],\n",
       "       ...,\n",
       "       [4.55003922e-01, 1.04190729e-03, 1.04205009e-03, ...,\n",
       "        1.04185614e-03, 1.04205003e-03, 1.04195855e-03],\n",
       "       [4.07023117e-01, 1.81857522e-03, 2.44313216e-01, ...,\n",
       "        1.81854993e-03, 1.81860815e-03, 9.78587275e-02],\n",
       "       [9.72720999e-01, 3.03088846e-03, 3.03092197e-03, ...,\n",
       "        3.03082138e-03, 3.03090304e-03, 3.03089819e-03]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f7b3e",
   "metadata": {},
   "source": [
    "- By setting `learning_method='batch'`, we let the lda estimator do its estimation based on all available training data (the `bag-of-words` matrix) in one iteration, which is slower than the alternative 'online' learning method, but can lead to more accurate results (setting `learning_method='online'` is analogous to online or mini-batch learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e982310",
   "metadata": {},
   "source": [
    "- access the `components_` attribute of the `lda` instance, which stores a matrix containing the word importance (here, 5000) for each of the 10 topics in increasing order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcc8c543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1d59f",
   "metadata": {},
   "source": [
    "- To analyze the results, let's print the five most important words for each of the 10 topics. Note that the word importance values are ranked in increasing order. Thus, to print the top five words, we need to sort the topic array in reverse order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62847ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 91.50846593 103.00131299 352.6877031  ... 341.90195267 219.2361035\n",
      "  30.20576233]\n",
      "1 [29.23761791 13.10213299 43.38100074 ...  0.10000513  0.10000308\n",
      "  2.46842427]\n",
      "2 [1.78099605e+01 1.62127213e+02 1.29444216e+02 ... 1.00009529e-01\n",
      " 1.00010992e-01 4.39793174e+00]\n",
      "3 [ 0.62985605 20.64865459 61.08221094 ...  0.10001072  0.10001132\n",
      " 10.03641183]\n",
      "4 [ 56.65850322 217.61222717  36.61177975 ... 796.95703592 533.86776726\n",
      "  26.8114528 ]\n",
      "5 [ 1.24340354 22.34807824  8.57614113 ... 37.44094137 70.19605598\n",
      "  3.80168648]\n",
      "6 [2.03775633e+00 7.87278607e+00 1.29536844e+02 ... 1.00008379e-01\n",
      " 1.00008924e-01 1.00417956e-01]\n",
      "7 [9.78183711e-01 1.32804550e+01 1.00321571e+01 ... 1.00009451e-01\n",
      " 1.00010088e-01 2.00522899e+02]\n",
      "8 [ 8.79623014 29.22767486 59.64699724 ...  0.10001286  0.10001387\n",
      "  0.75251745]\n",
      "9 [ 0.10002268 30.77946524 98.00095046 ...  0.10001398  0.10001499\n",
      "  1.90249633]\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(topic_idx, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1583b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes script awful stupid\n",
      "Topic 2:\n",
      "family mother father girl children\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art feel\n",
      "Topic 5:\n",
      "police guy car murder dead\n",
      "Topic 6:\n",
      "horror house gore blood sex\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read effects\n",
      "Topic 10:\n",
      "action fight guy guys fun\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count_vec.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb624c",
   "metadata": {},
   "source": [
    "Based on reading the five most important words for each topic, you may guess that the `LDA` identified the following topics:\n",
    "\n",
    "```\n",
    "1. Generally bad movies (not really a topic category)\n",
    "2. Movies about families\n",
    "3. War movies\n",
    "4. Art movies\n",
    "5. Crime movies\n",
    "6. Horror movies\n",
    "7. Comedy movie reviews\n",
    "8. Movies somehow related to TV shows\n",
    "9. Movies based on books\n",
    "10. Action movies\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b1ca5",
   "metadata": {},
   "source": [
    "To confirm that the categories make sense based on the reviews, let's plot 5 movies from the horror movie category (category 6 at index position 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16db7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "\"House of the Damned\" (also known as \"Spectre\") is one of your low budget haunted house horror flicks, filled with mediocre performances and cheap effects. It is about a family that inherits an old Irish mansion, and after moving in begin to experience strange phenomenon and ghostly apparitions, inc ...\n",
      "\n",
      "Horror movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n",
      "\n",
      "Horror movie #4:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n",
      "\n",
      "Horror movie #5:\n",
      "THE DEVIL'S PLAYTHING is my second attempt at a Joseph Sarno production - and although I will say it is far more enjoyable than the painfully dull and unerotic Swedish WILDCATS, it is still a little slow and un-explicit for my taste.<br /><br />This one centers around a group of vampire girls who li ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:5]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09dae38",
   "metadata": {},
   "source": [
    "- Using the preceding code example, we printed the first 300 characters from the top five horror movies. The reviews—even though we don’t know which exact movie they belong to—sound like reviews of horror movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08a90d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d3ce2",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "In this chapter, you learned how to use machine learning algorithms to classify text documents based on their polarity, which is a basic task in sentiment analysis in the field of NLP. Not only did you learn how to encode a document as a feature vector using the bag-of-words model, but you also learned how to weight the term frequency by relevance using tf-idf.\n",
    "\n",
    "\n",
    "Working with text data can be computationally quite expensive due to the large feature vectors that are created during this process; in the last section, we covered how to utilize out-of-core or incremental learning to train a machine learning algorithm without loading the whole dataset into a computer’s memory.\n",
    "\n",
    "\n",
    "Lastly, you were introduced to the concept of topic modeling using LDA to categorize the movie reviews into different categories in an unsupervised fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99e706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
