{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce2d376",
   "metadata": {},
   "source": [
    "# **Going Deeper -- the Mechanics of PyTorch (Part 3/3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1127cd",
   "metadata": {},
   "source": [
    "### **Higher-level PyTorch APIs: a short introduction to PyTorch Lightning**\n",
    "\n",
    "- Explore PyTorch Lightning.\n",
    "- Makes training deep neural networks simpler by removing much of the boilerplate code.\n",
    "- Focus lies in simplicity and flexibility\n",
    "- It allows us to use many advanced features such as multi-GPU support and fast low-precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85d3ae",
   "metadata": {},
   "source": [
    "#### **Setting up the PyTorch Lightning model**\n",
    "\n",
    "- All that is required to implement a `Lightening` model is to use `LightningModule` instead of the regular PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27f2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchmetrics import __version__ as torchmetrics_version\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6b0db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.2\n"
     ]
    }
   ],
   "source": [
    "print(torchmetrics_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74d5c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Version('2.5.5')>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_version(pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67841767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # new PL attributes:\n",
    "        if parse_version(torchmetrics_version) > parse_version(\"0.8\"):\n",
    "            self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.valid_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "            self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        else:\n",
    "            self.train_acc = Accuracy() # track accuracy during training\n",
    "            self.valid_acc = Accuracy() # track accuracy during validation\n",
    "            self.test_acc = Accuracy() # track accuracy during testing\n",
    "        \n",
    "        # Model similar to previous section:\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2] \n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units: \n",
    "            layer = nn.Linear(input_size, hidden_unit) \n",
    "            all_layers.append(layer) \n",
    "            all_layers.append(nn.ReLU()) \n",
    "            input_size = hidden_unit \n",
    " \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "    \n",
    "    # forward pass (returns the logits)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    # defines a single forward pass during training\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "   # Conditionally define epoch end methods based on PyTorch Lightning version\n",
    "    if parse_version(pl.__version__) >= parse_version(\"2.0\"):\n",
    "        # For PyTorch Lightning 2.0 and above\n",
    "        def on_train_epoch_end(self):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def on_validation_epoch_end(self):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def on_test_epoch_end(self):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "\n",
    "    else:\n",
    "        # For PyTorch Lightning < 2.0\n",
    "        def training_epoch_end(self, outs):\n",
    "            self.log(\"train_acc\", self.train_acc.compute())\n",
    "            self.train_acc.reset()\n",
    "\n",
    "        def validation_epoch_end(self, outs):\n",
    "            self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "            self.valid_acc.reset()\n",
    "\n",
    "        def test_epoch_end(self, outs):\n",
    "            self.log(\"test_acc\", self.test_acc.compute())\n",
    "            self.test_acc.reset()\n",
    "    \n",
    "    \n",
    "    # def on_train_epoch_end(self, outs):\n",
    "    #     self.log(\"train_acc\", self.train_acc.compute())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(self(x), y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_acc.update(preds, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f8d7a",
   "metadata": {},
   "source": [
    "Let’s now discuss the different methods one by one. As you can see, the `__init__` constructor contains the same model code that we used in a previous subsection. What’s new is that we added the accuracy attributes such as `self.train_acc = Accuracy()`. These will allow us to track the accuracy during training. Accuracy was imported from the `torchmetrics` module, which should be automatically installed with Lightning More information can be found at https://torchmetrics.readthedocs.io/en/latest/pages/quickstart.html.\n",
    "\n",
    "\n",
    "The ``forward` method implements a simple forward pass that returns the logits (outputs of the last fully connected layer of our network before the softmax layer) when we call our model on the input data. The logits, computed via the forward method by calling `self(x)`, are used for the `training`, `validation`, and `test steps`, which we’ll describe next.\n",
    "\n",
    "\n",
    "The `training_step`, `training_epoch_end`, `validation_step`, `test_step`, and `configure_optimizers` methods are methods that are specifically recognized by Lightning. For instance, `training_step` defines a single forward pass during training, where we also keep track of the accuracy and loss so that we can analyze these later. Note that we compute the accuracy via `self.train_acc.update(preds, y)` but don’t log it yet. The `training_step` method is executed on each individual batch during training, and via the `training_epoch_end` method, which is executed at the end of each training epoch, we compute the training set accuracy from the accuracy values we accumulated via training.\n",
    "\n",
    "\n",
    "The `validation_step` and `test_step` methods define, analogous to the `training_step` method, how the validation and test evaluation process should be computed. Similar to `training_step`, each `validation_step` and `test_step` receives a single batch, which is why we log the accuracy via respective accuracy attributes derived from Accuracy of torchmetric. However, note that `validation_step` is only called in certain intervals, for example, after each training epoch. This is why we log the validation accuracy inside the validation step, whereas with the training accuracy, we log it after each training epoch, otherwise, the accuracy plot that we inspect later will look too noisy.\n",
    "\n",
    "Finally, via the configure_optimizers method, we specify the optimizer used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36fe004",
   "metadata": {},
   "source": [
    "#### **Setting up the data loaders for Lightning**\n",
    "\n",
    "Three ways to prepare the dataset for Lightning;\n",
    "\n",
    "- Make the dataset part of the model\n",
    "- Set up the data loaders as usual and feed them to the `fit` method of a Ligtning Trainer.\n",
    "- Create a `LightningDataModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea00f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    " \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad5630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path='../data'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        MNIST(root=self.data_path, download=True) \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # stage is either 'fit', 'validate', 'test', or 'predict'\n",
    "        # here note relevant\n",
    "        mnist_all = MNIST( \n",
    "            root=self.data_path,\n",
    "            train=True,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "        self.train, self.val = random_split(\n",
    "            mnist_all, [55000, 5000], generator=torch.Generator().manual_seed(1)\n",
    "        )\n",
    "\n",
    "        self.test = MNIST( \n",
    "            root=self.data_path,\n",
    "            train=False,\n",
    "            transform=self.transform,  \n",
    "            download=False\n",
    "        ) \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4, \n",
    "                          persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4,\n",
    "                          persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4,\n",
    "                          persistent_workers=True)\n",
    "    \n",
    "\n",
    "torch.manual_seed(1) \n",
    "mnist_dm = MnistDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fcfe78",
   "metadata": {},
   "source": [
    "- In the `prepare_data` method, we define general steps, such as downloading the dataset.\n",
    "- In the `setup` method, we define the datasets used for training, validation, and testing.\n",
    "- `55000` examples for training and `5000` examples for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293fece",
   "metadata": {},
   "source": [
    "#### **Training the model using the PyTorch Lightning Trainer class**\n",
    "\n",
    "- Lightning implements a Trainer class that makes the training model super convenient by taking care of all the intermediate steps, such as calling `zero_grad()`, `backward()`, and `optimizer.step()` for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f34ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:07<00:00, 120.50it/s, v_num=4, train_loss=0.248, valid_loss=0.166, valid_acc=0.951]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:07<00:00, 120.33it/s, v_num=4, train_loss=0.248, valid_loss=0.166, valid_acc=0.951]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "mnistclassifier = MultiLayerPerceptron()\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_acc\")] # save top 1 model\n",
    "\n",
    "if torch.backends.mps.is_available():          # Apple Silicon GPU\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1\n",
    "    )\n",
    "elif torch.cuda.is_available():                 # NVIDIA GPU\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1\n",
    "    )\n",
    "else:                                           # CPU fallback\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"cpu\"\n",
    "    )\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a54d38",
   "metadata": {},
   "source": [
    "#### **Evaluating the model using TensorBoard**\n",
    "\n",
    "- We can visualize our `logs` specified in our `Lightning` model earlier in TensorBoard.\n",
    "- Lightning also supports other loggers as well.\n",
    "\n",
    "\n",
    "- By default, Lightning tracks the training in a subfolder named `lightning_logs`.\n",
    "- To visualize the training runs, you can execute the following code in the command-line terminal, which will open `TensorBoard` in your browser:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir lightning_logs/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba30fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c76a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ce81d",
   "metadata": {},
   "source": [
    "- By looking at the training and validation accuracies in the tensorboard, we can hypothesize that training the model for a few additional epochs can improve performance.\n",
    "- Lightning allows us to load a trained model and train it for additional epochs conveniently.\n",
    "- Lightning tracks the individual training runs via subfolders.\n",
    "\n",
    "\n",
    "- we can use the following code to load the latest model checkpoint from this folder and train the model via fit: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "013ad822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/user/Projects/ML-PyTorch-SkLearn/venv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /Users/user/Projects/ML-PyTorch-SkLearn/chp13_Going_Deeper_The_Mechanics_of_PyTorch/lightning_logs/version_4/checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at lightning_logs/version_4/checkpoints/epoch=9-step=8600.ckpt\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at lightning_logs/version_4/checkpoints/epoch=9-step=8600.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 860/860 [00:08<00:00, 106.86it/s, v_num=5, train_loss=0.161, valid_loss=0.165, valid_acc=0.952]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 860/860 [00:08<00:00, 106.67it/s, v_num=5, train_loss=0.161, valid_loss=0.165, valid_acc=0.952]\n"
     ]
    }
   ],
   "source": [
    "path = 'lightning_logs/version_4/checkpoints/epoch=9-step=8600.ckpt'\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():          # Apple Silicon GPU\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=15,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1\n",
    "    )\n",
    "elif torch.cuda.is_available():                 # NVIDIA GPU\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=15,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1\n",
    "    )\n",
    "    \n",
    "else: \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"cpu\"\n",
    "    )\n",
    "\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm, ckpt_path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977a315",
   "metadata": {},
   "source": [
    "- `TensorBoard` allows us to show the results from the additional training epochs (version_1) next to the previous ones (version_0), which is very convenient. Indeed, we can see that training for five more epochs improved the validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8524d50",
   "metadata": {},
   "source": [
    "- Once we are finished with training, we can evaluate the model on the test set using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3d331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:01<00:00, 138.13it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9567000269889832\n",
      "        test_loss           0.14652474224567413\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.14652474224567413, 'test_acc': 0.9567000269889832}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=mnistclassifier, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f4878",
   "metadata": {},
   "source": [
    "- Note that PyTorch Lightning also saves the model automatically for us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
