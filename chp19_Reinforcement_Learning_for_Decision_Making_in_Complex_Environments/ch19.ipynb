{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2079661",
   "metadata": {},
   "source": [
    "## **Reinforcement Learning for Decision Making in Complex Environments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308f0fc",
   "metadata": {},
   "source": [
    "#### **Reinforcement Learning (Deep Dive)**\n",
    "\n",
    "\n",
    "**1. Core Idea**\n",
    "\n",
    "Reinforcement Learning (RL) is a paradigm of machine learning where an **agent** learns to make sequential decisions by interacting with an **environment** to maximize a **cumulative reward**.\n",
    "\n",
    "At each time step `t`:\n",
    "\n",
    "* The agent observes a **state** $`s_t`$.\n",
    "* Takes an **action** $`a_t`$ according to a **policy** $`\\pi(a_t|s_t)`$.\n",
    "* Receives a **reward** $`r_t`$ and transitions to the next **state** $`s_{t+1}`$.\n",
    "\n",
    "The goal is to learn a policy that maximizes the **expected return**:\n",
    "\n",
    "$$J(\\pi) = \\mathbb{E}_\\pi \\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$$\n",
    "\n",
    "where $`\\gamma \\in [0,1)`$ is the **discount factor**.\n",
    "\n",
    "\n",
    "\n",
    "**2. Mathematical Framework**\n",
    "\n",
    "**2.1 Markov Decision Process (MDP)**\n",
    "\n",
    "An MDP is defined as a 5-tuple:\n",
    "\n",
    "$$\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$$\n",
    "\n",
    "| Symbol          | Meaning         |                              |\n",
    "| --------------- | --------------- | ---------------------------- |\n",
    "| $`\\mathcal{S}`$ | State space     |                              |\n",
    "| $`\\mathcal{A}`$ | Action space    |                              |\n",
    "| $`P(s'          \\| s, a)`$         | State transition probability |\n",
    "| $`R(s, a)`$     | Reward function |                              |\n",
    "| $`\\gamma`$      | Discount factor |                              |\n",
    "\n",
    "\n",
    "The Markov property assumes:\n",
    "\n",
    "$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, \\dots) = P(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "\n",
    "\n",
    "**3. Value Functions**\n",
    "\n",
    "RL methods estimate the *expected future return* from states or state-action pairs:\n",
    "\n",
    "* **State-Value Function:**\n",
    "\n",
    "  $$V^\\pi(s) = \\mathbb{E}_\\pi \\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s\\right]$$\n",
    "\n",
    "* **Action-Value Function:**\n",
    "\n",
    "  $$Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0 = a\\right]$$\n",
    "\n",
    "* **Optimal Value Functions:**\n",
    "\n",
    "  $$V^*(s) = \\max_\\pi V^\\pi(s)$$\n",
    "  \n",
    "  $$Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)$$\n",
    "\n",
    "\n",
    "\n",
    "**4. Bellman Equations**\n",
    "\n",
    "Fundamental recursive relationships:\n",
    "\n",
    "* **Bellman Expectation Equation:**\n",
    "\n",
    "  $$V^\\pi(s) = \\sum_a \\pi(a|s)\\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "* **Bellman Optimality Equation:**\n",
    "\n",
    "  $$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma V^*(s')]$$\n",
    "\n",
    "\n",
    "\n",
    "**5. Solution Approaches**\n",
    "\n",
    "**5.1 Dynamic Programming (Model-Based)**\n",
    "\n",
    "* Requires full knowledge of transition probabilities $`P`$.\n",
    "* Algorithms:\n",
    "\n",
    "  * **Policy Iteration** (alternating evaluation and improvement)\n",
    "  * **Value Iteration** (updates via Bellman optimality)\n",
    "\n",
    "**5.2 Monte Carlo Methods (Model-Free)**\n",
    "\n",
    "* Learn from *episodes* of experience.\n",
    "* Estimate $`V^\\pi(s)`$ or $`Q^\\pi(s, a)`$ by sampling returns.\n",
    "\n",
    "**5.3 Temporal Difference (TD) Learning**\n",
    "\n",
    "* Combines ideas from DP and Monte Carlo.\n",
    "* Bootstraps estimates using the Bellman equation:\n",
    "\n",
    "  $$V(s_t) \\leftarrow V(s_t) + \\alpha[r_t + \\gamma V(s_{t+1}) - V(s_t)]$$\n",
    "\n",
    "  * **SARSA** (on-policy)\n",
    "  * **Q-Learning** (off-policy)\n",
    "\n",
    "\n",
    "\n",
    "**6. Policy Optimization Methods**\n",
    "\n",
    "Two main paradigms exist:\n",
    "\n",
    "**6.1 Value-Based (e.g., Q-Learning, DQN)**\n",
    "\n",
    "* Learn $`Q(s, a)`$ and derive policy as $`\\pi(s) = \\arg\\max_a Q(s,a)`$.\n",
    "\n",
    "**6.2 Policy-Based (e.g., REINFORCE)**\n",
    "\n",
    "* Directly optimize the policy parameters $`\\theta`$ to maximize expected reward:\n",
    "\n",
    "  $$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi_\\theta}(s,a)]$$\n",
    "\n",
    "**6.3 Actor-Critic**\n",
    "\n",
    "* Combines both:\n",
    "\n",
    "  * **Actor:** Updates policy parameters $`\\theta`$.\n",
    "  * **Critic:** Estimates value function parameters $`\\phi`$.\n",
    "\n",
    "\n",
    "\n",
    "**7. Deep Reinforcement Learning**\n",
    "\n",
    "Uses deep neural networks to approximate value or policy functions:\n",
    "\n",
    "| Algorithm   | Core Idea                                                     |\n",
    "| ----------- | ------------------------------------------------------------- |\n",
    "| **DQN**     | Deep Q-Networks for discrete actions.                         |\n",
    "| **DDPG**    | Deep Deterministic Policy Gradient for continuous control.    |\n",
    "| **A2C/A3C** | Advantage Actor-Critic with synchronous/asynchronous updates. |\n",
    "| **PPO**     | Proximal Policy Optimization (stable clipped objective).      |\n",
    "| **SAC**     | Soft Actor-Critic (maximizes reward + entropy).               |\n",
    "\n",
    "\n",
    "\n",
    "**8. Exploration vs. Exploitation**\n",
    "\n",
    "Agents must balance:\n",
    "\n",
    "* **Exploration:** Trying new actions to discover rewards.\n",
    "* **Exploitation:** Leveraging known actions to maximize returns.\n",
    "\n",
    "Common techniques:\n",
    "\n",
    "* **Œµ-greedy policy**\n",
    "* **Boltzmann exploration**\n",
    "* **Entropy regularization**\n",
    "\n",
    "\n",
    "\n",
    "**9. Reward Engineering and Credit Assignment**\n",
    "\n",
    "* **Sparse rewards:** Hard to learn due to delayed signals.\n",
    "* **Shaping:** Designing informative intermediate rewards.\n",
    "* **Credit assignment:** Identifying which past actions led to observed rewards.\n",
    "\n",
    "\n",
    "\n",
    "**10. Advanced Topics**\n",
    "\n",
    "| Concept             | Description                                                          |\n",
    "| ------------------- | -------------------------------------------------------------------- |\n",
    "| **Multi-Agent RL**  | Multiple interacting agents (cooperative/competitive).               |\n",
    "| **Meta-RL**         | Agents that learn how to learn (adapt quickly to new tasks).         |\n",
    "| **Hierarchical RL** | Decomposes tasks into sub-policies for efficiency.                   |\n",
    "| **Offline RL**      | Learning from fixed datasets without active environment interaction. |\n",
    "| **Inverse RL**      | Infers reward functions from expert demonstrations.                  |\n",
    "\n",
    "\n",
    "\n",
    "**11. Evaluation Metrics**\n",
    "\n",
    "* **Cumulative reward:** $`\\sum_t r_t`$\n",
    "* **Learning efficiency:** Speed of convergence.\n",
    "* **Stability:** Variance across runs.\n",
    "* **Generalization:** Performance in unseen environments.\n",
    "\n",
    "\n",
    "\n",
    "**12. Key Theoretical Foundations**\n",
    "\n",
    "* **Policy Gradient Theorem**\n",
    "* **Bellman Operator Contraction Property**\n",
    "* **Expected vs. Deterministic Policy Gradients**\n",
    "* **Entropy-Regularized Optimization (in SAC, PPO)**\n",
    "\n",
    "\n",
    "\n",
    "**13. Practical Implementation Stack**\n",
    "\n",
    "| Component     | Tools                                   |\n",
    "| ------------- | --------------------------------------- |\n",
    "| Environment   | OpenAI Gymnasium, PettingZoo, Isaac Gym |\n",
    "| Frameworks    | PyTorch, TensorFlow, JAX                |\n",
    "| Libraries     | Stable-Baselines3, RLlib, CleanRL       |\n",
    "| Visualization | TensorBoard, WandB                      |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cc3d6",
   "metadata": {},
   "source": [
    "### **Introduction: Learning from experience**\n",
    "\n",
    "- introduce the concept of `RL` as a branch of machine learning.\n",
    "- fundamental components of an `RL` system.\n",
    "- `RL` mathematical formulation based on the Markov decision process.\n",
    "\n",
    "\n",
    "\n",
    "#### **Understanding reinforcement learning**\n",
    "\n",
    "- centered around the concept of `learning by interaction`.\n",
    "- In RL, the model learns from interactions with an environment to maximize a `reward function`.\n",
    "- The `Agent` interacts with its environment, and by doing so generates a sequence of interactions that are together called an `episode`.\n",
    "- Through these interactions, the agent collects a series of rewards determined by the environment.\n",
    "\n",
    "- In `RL`, we cannot or do not teach an agent, computer, or robot how to do things, we can only specify what we want the agent to achieve.\n",
    "- Then, based on the outcome of a particular trail, we can determine rewards depending on the agent's success or failure.\n",
    "\n",
    "\n",
    "**Besides applications in games and robotics, examples of RL can also be found in nature. For example, training a dog involves RL. we hand out rewards (treats) to the dog when it performs certain desirable actions. Or consider a medical dog that is trained to warn its partner of an oncoming seizure. In this case, we do not know the exact mechanism by which the dog is able to detect an oncoming seizure, and we certainly wouldn‚Äôt be able to define a series of steps to learn seizure detection, even if we had precise knowledge of this mechanism. However, we can reward the dog with a treat if it successfully detects a seizure to reinforce this behavior!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f87a7",
   "metadata": {},
   "source": [
    "### **Defining the agent-environment interface of a reinforcement learning system**\n",
    "\n",
    "- An `agent` and an `environment`.\n",
    "- An `agent` is defined as an entity that learns how to make decisions and interacts with its surrounding environment by taking an action. In return, as a consequence of taking an action, the agent receives observations and a reward signal as governed by the environment.\n",
    "- The `environment` is anything that falls outside the agent. \n",
    "- The environment communicates with the agent and determines the reward signal for the agent's action as well as its observations.\n",
    "\n",
    "- The `reward signal` is the feedback that the agent receives from interacting with the environment, which is usually provided in the form of a scalar value and can be either positive or negative.\n",
    "- The purpose of the reward is to tell the agent how well it has performed.\n",
    "- The frequency at which the agent receives the reward depends on the given task or problem.\n",
    "\n",
    "\n",
    "![Interaction between the agent and its environment](./figures/19_01.png)\n",
    "\n",
    "\n",
    "- During the learning process, the agent must try different actions `(exploration)` so that it can progressively learn which actions to prefer and perform more often `(exploitation)` in order to maximize the total, cummulative reward.\n",
    "\n",
    "- `exploitation` will result in choosing actions with a greater short-term reward, whereas `exploration` can potentially result in greater total rewards in the long run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f3d49",
   "metadata": {},
   "source": [
    "### **The theoretical foundations of RL**\n",
    "\n",
    "- Examination of the mathematical formulation of \n",
    "  - `Markov decision processes`\n",
    "  - `episodic` versus `continuing tasks`\n",
    "  - dynamic programming using the `Bellman equation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c6e95",
   "metadata": {},
   "source": [
    "#### **Markov decision processes**\n",
    "\n",
    "- The type of problems that `RL` deals with are typically formulated as `Markov decision processes (MDPs)`.\n",
    "- The standard approach for solving `MDP` problems is by using `dynamic programming`.\n",
    "\n",
    "**Dynamic programming (Model-based)**\n",
    "- Dynamic programming refers to a set of computer algorithms and programming methods that was developed by Richard Bellman.\n",
    "- It is about `recursive` problem solving-solving relatively complicated problems by breaking them down into smaller subproblems.\n",
    "- dynamic programming stores the results of subproblems so that they can be accessed in constant time it they are encountered again in future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5df4a",
   "metadata": {},
   "source": [
    "#### **The mathematical formulation of Markov decision processes**\n",
    "\n",
    "The types of problems that require learning an interactive and sequential decision-making process, where decision at time step $t$ affects the subsequent situations, are mathematically formalized as `MDPs`\n",
    "\n",
    "- case of the `agent/environment` interactions in `RL`.\n",
    "- denote agent's starting state as $S_0$, the interactions between the agent and the environment result in a sequence as follows:\n",
    "\n",
    "{$`{S_0, A_0, R_1}`$}, {$`S_1, A_1, R_2`$}, {$`S_2, A_2, R_3`$}, ...\n",
    "\n",
    "- $S_t$ and $A_t$ stand for the state and the action taken at time step $t$.\n",
    "- $R_{t+1}$ denotes the reward received from the environment after performing action $A_t$.\n",
    "- $S_t$, $R_{t+1}$, and $A_t$ are time-dependent random variables that take values from predefined finite sets denoted by $s \\in \\hat{S}$, $r \\in \\hat{R}$, $a \\in \\hat{A}$, respectively.\n",
    "- In an MDP, these time-dependent random variables, $S_t$ and $R_{t+1}$, have probability distributions that only depend on their values at the preceding time step, $t - 1$.\n",
    "- The probability distribution for $S_{t+1} = s'$ and $R_{t+1} = r$ can be written as a conditional probability over the preceding state $(S_t)$ and taken action $(A_t)$ as follows:\n",
    "\n",
    "\n",
    "$$p(s', r \\mid s, a) \\overset{\\mathrm{def}}{=} P(S_{t+1} = s', R_{t+1} = r \\mid S_t = s, A_t = a)$$\n",
    "\n",
    "\n",
    "- This probability distribution completely defines the `dynamics of the environment` (or model of the environment) because, based on this distribution, all transition probabilities of the environment can be computed.\n",
    "- The environment dynamics are a central criterion for categorizing different `RL` methods.\n",
    "- The types of `RL` methods that require a model of the environment or try to learn a model of the environment (that is, the environment dynamics) are called `model-based` methods as opposed to `model-free` methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271ceec",
   "metadata": {},
   "source": [
    "**Model-free and model-based RL**\n",
    "\n",
    "- When the probability $p(s', r \\mid s, a)$ is known, then the learning task can be solved with dynamic programming.\n",
    "- When the dynamics of the environment are not known, as is the case in many real-world problems, then we would need to acquire a large number of samples by interacting with the environment to compensate for the unknown environment dynamics.\n",
    "\n",
    "Two main approaches for dealing with this problem are the `model-free Monte Carlo (MC)` and `Temporal Difference (TD)` methods\n",
    "\n",
    "\n",
    "![Different models to use based on the environment dynamics](./figures/19_02.png)\n",
    "\n",
    "- The environment dynamics can be considered deterministic if particular actions for given states are always or never taken, that is, $p(s', r \\mid s, a) \\in \\{0, 1\\}$. Otherwise, in the more general case, the environment would have stochastic behavior.\n",
    "\n",
    "- Let's consider the probability of observing the future state $S_{t + 1} = s'$ conditioned on the current state $S_t = s$ and the performed action $A_t = a$. This is denoted by:\n",
    "\n",
    "$$p(s' \\mid s, a) \\overset{\\mathrm{def}}{=} P(S_{t+1} = s' \\mid S_t = s, A_t = a)$$\n",
    "\n",
    "\n",
    "It can be computed as a marginal probability by taking the sum over all possible rewards:\n",
    "\n",
    "$$p(s' \\mid s, a) \\overset{\\mathrm{def}}{=} \\sum_{r \\in \\hat{R}} {p(s', r \\mid s, a)}$$\n",
    "\n",
    "- This probability is called `state-transition probability`.\n",
    "- Based on the state-transition probability, if the environment dynamics are deterministic, then it means that when the agent takes action $A_t = a$ at state $S_t = s$, the transition to the next state, $S_{t + 1} = s'$, will be `100` percent certain, that is, $p(s' \\mid s, a) = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce06876",
   "metadata": {},
   "source": [
    "**Monte Carlo (MC) methods (Model-free)**: Learn value functions by averaging returns from complete episodes, relying solely on sampled experience rather than a model of the environment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Temporal Difference (TD) Learning**: Updates value estimates using bootstrapping‚Äîlearning from partial episodes by combining observed rewards with predicted future values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b27ba",
   "metadata": {},
   "source": [
    "#### **Visualization of a Markov process**\n",
    "\n",
    "- A Markov process can be represented as a directed cyclic graph in which the nodes in the graph represent the different states of the environment.\n",
    "\n",
    "- The edges of the graph (that is, the connections between the nodes) represent the transition probabilities between the states.\n",
    "\n",
    "\n",
    "`For example, let‚Äôs consider a student deciding between three different situations: (A) studying for an exam at home, (B) playing video games at home, or (C) studying at the library. Furthermore, there is a terminal state (T) for going to sleep. The decisions are made every hour, and after making a decision, the student will remain in a chosen situation for that particular hour. Then, assume that when staying at home (state A), there is a 50 percent likelihood that the student switches the activity to playing video games. On the other hand, when the student is at state B (playing video games), there is a relatively high chance (80 percent) that the student will continue playing video games in the subsequent hours.`\n",
    "\n",
    "\n",
    "- The dynamics of the student's behavior is shown as a `Markov` process, which includes a cyclic graph and a transition table:\n",
    "\n",
    "\n",
    "![The Markov process of the student](./figures/19_03.png)\n",
    "\n",
    "\n",
    "- The values on the edges of the graph represent the transition probabilities of the student‚Äôs behavior, and their values are also shown in the table to the right. When considering the rows in the table, please note that the transition probabilities coming out of each state (node) always sum to `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afaacb",
   "metadata": {},
   "source": [
    "#### **Episodic versus continuing tasks**\n",
    "\n",
    "- As the agent interacts with the environment, the sequence of observations or states forms a trajectory. There are two types of trajectories;\n",
    "  - If an agent's trajectory can be divided into subparts such that each starts at time $t = 0$ and ends in a terminal state $S_T$ (at $t = T$) the task is called an `episodic task`.\n",
    "  - On the other hand, if the trajectory is infinitely continuous without a terminal state, the task is called a `continuing task`.\n",
    "\n",
    "- The task related to a learning agent for the game of chess is an episodic task, whereas a cleaning robot that is keeping a house tidy is typically performing a continuing task.\n",
    "\n",
    "- In episodic tasks, an `episode` is a sequence or trajectory that an agent takes from a starting state, $S_0$, to a terminal state, $S_T$:\n",
    "\n",
    "$$S_0, A_0, R_1, S_1, A_1, R_2, ..., S_t, A_t, R_{t+1}, ..., S_{t‚Äì1}, A_{t‚Äì1}, R_t, S_t$$\n",
    "\n",
    "\n",
    "- For the Markov process, which depicts the task of a student studying for an exam, we may encounter episodes like the following three examples:\n",
    "\n",
    "```\n",
    "Episode 1: BBCCCCBAT    -> pass  (final reward = +1)\n",
    "Episode 2: ABBBBBBBBBBT -> fail  (final reward = ‚àí1)\n",
    "Episode 3: BCCCCCT      -> pass  (final reward = +1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d7458",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb5548",
   "metadata": {},
   "source": [
    "## **RL terminology: return, policy, and value function**\n",
    "\n",
    "\n",
    "#### **The return**\n",
    "\n",
    "The so-called return at time $t$ is the cumulated reward obtained from the entire duration of an episode. Recall that $R_{t+1} = r$ is the immediate reward obtained after performing an action, $A_t$, at time $t$; the `subsequent` rewards are $R_{t+2}$, $R_{t+3}$, and so forth.\n",
    "\n",
    "The return at time t can then be calculated from the immediate reward as well as the subsequent ones, as follows:\n",
    "\n",
    "\n",
    "$$G_t \\overset{\\mathrm{def}}{=} R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "\n",
    "The discount factor $`\\gamma \\in [0, 1]`$ determines how much future rewards are worth at the current time step $`t`$.\n",
    "\n",
    "* When $`\\gamma = 0`$, only the immediate reward matters ‚Äî the agent is **short-sighted**, ignoring all future rewards.\n",
    "* When $`\\gamma = 1`$, all future rewards are equally weighted, giving an **undiscounted** total return.\n",
    "\n",
    "The return can also be expressed recursively as:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma G_{t+1} = r + \\gamma G_{t+1}$$\n",
    "\n",
    "\n",
    "This means that the return at time $t$ is equal to the immediate reward $r$ plus the discounted future return at time $t + 1$. This is a very important property, which facilitates the computations of the return.\n",
    "\n",
    "\n",
    "![An example of a discount factor based on the value of a $100 bill over time](./figures/19_04.png)\n",
    "\n",
    "\n",
    "Assume $`\\gamma = 0.9`$ and that the only reward is given at the end of the episode:\n",
    "\n",
    "* **+1** for passing the exam\n",
    "* **‚Äì1** for failing\n",
    "* Intermediate rewards are **0**\n",
    "\n",
    "**Episode 1: *Pass* (final reward = +1)**\n",
    "\n",
    "The return at each time step is:\n",
    "\n",
    "$`G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\cdots + \\gamma^6 R_7 = \\gamma^6 = 0.9^6 = 0.531`$\n",
    "\n",
    "$`G_1 = \\gamma^5 = 0.590`$\n",
    "\n",
    "$`G_2 = \\gamma^4 = 0.656`$\n",
    "\n",
    "$`\\vdots`$\n",
    "\n",
    "$`G_6 = \\gamma = 0.9`$\n",
    "\n",
    "$`G_7 = 1`$\n",
    "\n",
    "\n",
    "**Interpretation:**\n",
    "As the agent moves closer to the terminal state (exam result), the discounted return increases ‚Äî earlier time steps value the final reward less due to discounting.\n",
    "\n",
    "\n",
    "\n",
    "**Episode 2: *Fail* (final reward = ‚àí1)**\n",
    "\n",
    "Assume $`\\gamma = 0.9`$ and that the only reward is received at the end of the episode (failing the exam).\n",
    "\n",
    "The return at each time step is:\n",
    "\n",
    "$`G_0 = -1 \\times \\gamma^8 = -0.9^8 = -0.430`$\n",
    "\n",
    "$`G_1 = -1 \\times \\gamma^7 = -0.9^7 = -0.478`$\n",
    "\n",
    "$`G_2 = -1 \\times \\gamma^6 = -0.9^6 = -0.531`$\n",
    "\n",
    "$`\\vdots`$\n",
    "\n",
    "$`G_9 = -1 \\times \\gamma = -0.9`$\n",
    "\n",
    "$`G_{10} = -1`$\n",
    "\n",
    "\n",
    "**Interpretation:**\n",
    "The discounted return becomes less negative as the agent moves closer to the terminal state, since earlier steps discount the final negative reward more heavily.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy**\n",
    "\n",
    "\n",
    "A **policy**, denoted as $`\\pi(a \\mid s)`$, defines how an agent selects actions given a state. It can be:\n",
    "\n",
    "* **Deterministic:** Always selects the same action for a given state.\n",
    "* **Stochastic:** Defines a probability distribution over possible actions.\n",
    "\n",
    "For a stochastic policy:\n",
    "\n",
    "$$\\pi(a \\mid s) \\overset{\\mathrm{def}}{=} P[A_t = a \\mid S_t = s]$$\n",
    "\n",
    "During learning, the policy evolves as the agent gains experience ‚Äî typically starting from a random policy (uniform probabilities) and improving toward the **optimal policy** $`\\pi^*(a \\mid s)`$, which maximizes expected return.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Value function**\n",
    "\n",
    "\n",
    "The **value function** (or **state-value function**) measures how good it is to be in a given state under a specific policy ‚Äî that is, the expected return from that state.\n",
    "\n",
    "Based on the return $`G_t`$, the value function under policy $`\\pi`$ is defined as:\n",
    "\n",
    "\n",
    "$$v_{\\pi}(s) \\overset{\\mathrm{def}}{=} \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\,\\middle|\\, S_t = s\\right]$$\n",
    "\n",
    "In practice, the value function can be estimated **tabularly** ‚Äî using arrays, lists, or dictionaries, where each state maps to its estimated value $`V(s)`$.\n",
    "\n",
    "\n",
    "Similarly, the **action-value function** (or **Q-function**) defines the expected return for taking action $`a`$ in state $`s`$ under policy $`\\pi`$:\n",
    "\n",
    "\n",
    "$$q_{\\pi}(s, a) \\overset{\\mathrm{def}}{=} \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a]$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Extending the definition of the **state-value function** to **state-action pairs**, the **action-value function** (or **Q-function**) is defined as the expected return when the agent starts in state $`s`$, takes action $`a`$, and thereafter follows policy $`\\pi`$:\n",
    "\n",
    "$$q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a \\right]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $`s`$ ‚Üí Current **state**\n",
    "* $`a`$ ‚Üí **Action** taken in state $`s`$\n",
    "* $`\\gamma`$ ‚Üí **Discount factor** determining the present value of future rewards\n",
    "* $`R_{t+k+1}`$ ‚Üí **Reward** received after $`k`$ future steps\n",
    "* $`G_t`$ ‚Üí **Return**, the cumulative discounted future reward\n",
    "\n",
    "This formulation measures the **expected return** for taking a specific action in a given state under policy $`\\pi`$, and it generalizes the state-value function:\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a \\mid s) q_{\\pi}(s, a)$$\n",
    "\n",
    "Analogously, for the **optimal policy** $`\\pi^*`$, we define:\n",
    "\n",
    "* **Optimal state-value function:** $`v^*(s) = \\max_{\\pi} v_{\\pi}(s)`$\n",
    "* **Optimal action-value function:** $`q^*(s, a) = \\max_{\\pi} q_{\\pi}(s, a)`$\n",
    "\n",
    "These optimal functions form the foundation for reinforcement learning algorithms like **Q-learning** and **SARSA**, which aim to approximate $`q^*(s, a)`$ to derive the best possible policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22a0a7",
   "metadata": {},
   "source": [
    "**Difference Between Reward, Return, and Value Function**\n",
    "\n",
    "| Concept            | Definition                                                                                | Mathematical Representation                                   | Key Idea                                                                     | Example (Chess)                                                                                |\n",
    "| ------------------ | ----------------------------------------------------------------------------------------- | ------------------------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **Reward**         | The immediate scalar feedback signal received after taking an action in a specific state. | $`R_{t+1}`$                                                   | Measures the **instantaneous gain or loss** from performing an action.       | Reward = +1 for winning, 0 for intermediate moves, ‚Äì1 for losing.                              |\n",
    "| **Return**         | The cumulative (possibly discounted) sum of future rewards obtained from time $t$ onward. | $`G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots`$ | Reflects the **total long-term outcome** following a sequence of actions.    | The **final game result** (win/loss) represents the return after all moves.                    |\n",
    "| **Value Function** | The expected return from a state (or state-action pair) under a given policy.             | $`v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s]`$           | Represents the **average desirability** of being in a state, given a policy. | The **position after capturing the queen** has a high value because it often leads to victory. |\n",
    "\n",
    "**Summary**\n",
    "\n",
    "* **Reward** ‚Üí Immediate feedback (short-term signal).\n",
    "* **Return** ‚Üí Total future rewards from a point in time (long-term outcome).\n",
    "* **Value Function** ‚Üí Expected long-term return (goodness) of a state or state-action pair under a specific policy.\n",
    "\n",
    "In essence, **rewards are instant**, **returns are cumulative**, and **values are expectations** over many possible outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1457a8",
   "metadata": {},
   "source": [
    "### **Dynamic programming using the Bellman equation**\n",
    "\n",
    "**Bellman Equation (Recursive Value Definition)**\n",
    "\n",
    "The **Bellman equation** expresses the value of a state in terms of the **immediate reward** and the **expected discounted value of the next state**, providing a recursive way to compute value functions.\n",
    "\n",
    "\n",
    "**State-Value Function Form**\n",
    "\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]$$\n",
    "\n",
    "Expanding with environment dynamics:\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "\n",
    "\n",
    "**Action-Value Function Form**\n",
    "\n",
    "$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a]$$\n",
    "\n",
    "Or equivalently,\n",
    "\n",
    "$$q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') q_{\\pi}(s',a')]$$\n",
    "\n",
    "\n",
    "\n",
    "**Key Insight**\n",
    "\n",
    "The Bellman equation **links the current value to future values**, removing the need to compute full episode returns.\n",
    "It forms the foundation for **Dynamic Programming**, **Monte Carlo**, and **Temporal-Difference** learning methods in reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b04af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef27dc",
   "metadata": {},
   "source": [
    "## **Reinforcement learning algorithms**\n",
    "\n",
    "\n",
    "Reinforcement learning algorithms differ mainly by **how they estimate value functions** and **whether they know the environment dynamics**.\n",
    "\n",
    "\n",
    "![Different types of RL](./figures/19_05.png)\n",
    "\n",
    "\n",
    "**1. Dynamic Programming (DP)**\n",
    "\n",
    "* **Assumes known environment dynamics** ‚Äî the transition probability $`p(s', r | s, a)`$ is available.\n",
    "\n",
    "* Computes exact value functions using **Bellman equations**.\n",
    "\n",
    "* Example methods: *Policy Evaluation*, *Policy Iteration*, *Value Iteration*.\n",
    "\n",
    "* **Limitation:** Not practical for large or unknown environments.\n",
    "\n",
    "\n",
    "\n",
    "**2. Monte Carlo (MC) Methods**\n",
    "\n",
    "* **Model-free:** Does **not** require knowledge of transition dynamics.\n",
    "  \n",
    "* Learns value estimates from **complete episodes** of experience.\n",
    "  \n",
    "* Updates values based on **averaged returns** observed after visiting a state/action.\n",
    "  \n",
    "* Works well for episodic tasks.\n",
    "\n",
    "\n",
    "\n",
    "**3. Temporal Difference (TD) Learning**\n",
    "\n",
    "* **Model-free** and **bootstraps** from current estimates (updates before episode ends).\n",
    "  \n",
    "* Combines ideas from DP and MC.\n",
    "  \n",
    "* Core update:\n",
    "\n",
    "  $$V(s_t) \\leftarrow V(s_t) + \\alpha [r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]$$\n",
    "\n",
    "\n",
    "\n",
    "**4. Q-Learning (Off-policy TD Control)**\n",
    "\n",
    "* Learns the **optimal action-value function** $`Q^*(s, a)`$ directly.\n",
    "  \n",
    "* Core update:\n",
    "  \n",
    "  $$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)]$$\n",
    "\n",
    "* Does **not follow the same policy** it evaluates (off-policy).\n",
    "\n",
    "\n",
    "\n",
    "**5. Deep Q-Learning (DQN)**\n",
    "\n",
    "* Extension of Q-learning using **deep neural networks** to approximate $`Q(s, a)`$.\n",
    "\n",
    "* Enables learning in **high-dimensional state spaces** (e.g., images).\n",
    "\n",
    "* Uses techniques like **experience replay** and **target networks** for stability.\n",
    "\n",
    "\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "| Method              | Model-Based | Learns from Episodes | Bootstrapping | Typical Use                   |\n",
    "| ------------------- | ----------- | -------------------- | ------------- | ----------------------------- |\n",
    "| Dynamic Programming | Yes         | No                   | Yes           | Small, known environments     |\n",
    "| Monte Carlo         | No          | Yes                  | No            | Episodic tasks                |\n",
    "| Temporal Difference | No          | No                   | Yes           | Online learning               |\n",
    "| Q-Learning          | No          | No                   | Yes           | Control (find optimal policy) |\n",
    "| Deep Q-Learning     | No          | No                   | Yes           | Large/continuous state spaces |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65bdca",
   "metadata": {},
   "source": [
    "### **Solving RL Problems under Known Dynamics (Dynamic programming)**\n",
    "\n",
    "This section focuses on reinforcement learning under two key assumptions:\n",
    "\n",
    "1. **Full knowledge of environment dynamics** ‚Äî all transition probabilities $`p(s', r | s, a)`$ are known.\n",
    "2. **Markov property holds** ‚Äî the next state and reward depend only on the current state and action.\n",
    "\n",
    "With these assumptions, RL problems can be mathematically formulated as **Markov Decision Processes (MDPs)**. Using the Bellman equation and value functions $`v_\\pi(s)`$, we can solve for state values exactly.\n",
    "\n",
    "However, **dynamic programming (DP)** is mainly of *theoretical importance*:\n",
    "\n",
    "* It‚Äôs **impractical** for most real-world RL tasks since environment dynamics are rarely known.\n",
    "* It‚Äôs **educationally valuable**, forming the foundation for understanding more advanced, model-free methods such as Monte Carlo, TD, and Q-learning.\n",
    "\n",
    "\n",
    "\n",
    "**Core Objectives**\n",
    "\n",
    "1. **Policy Evaluation (Prediction Task)**\n",
    "\n",
    "   * Estimate the *true state-value function* $`v_\\pi(s)`$ for a given policy $`\\pi`$.\n",
    "   * Measures how good it is to follow $`\\pi`$ starting from each state.\n",
    "\n",
    "2. **Generalized Policy Iteration (Control Task)**\n",
    "\n",
    "   * Find the *optimal value function* $`v^*(s)`$ and corresponding *optimal policy* $`\\pi^*`$.\n",
    "   * Alternates between policy evaluation and policy improvement until convergence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Evaluation ‚Äì Predicting the Value Function with Dynamic Programming**\n",
    "\n",
    "\n",
    "When the environment dynamics are known, **policy evaluation** estimates the value function for a given policy $`\\pi`$ using the **Bellman expectation equation**.\n",
    "\n",
    "Starting with an initial guess $`v^{‚ü®0‚ü©}(s)`$ (often all zeros), the value function is updated iteratively:\n",
    "\n",
    "\n",
    "$$v^{‚ü®i+1‚ü©}(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma v^{‚ü®i‚ü©}(s')]$$\n",
    "\n",
    "\n",
    "As $`i ‚Üí ‚àû`$, $`v^{‚ü®i‚ü©}(s)`$ converges to the true state-value function $`v_\\pi(s)`$.\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "* **No environment interaction** is needed‚Äîtransition probabilities are known.\n",
    "* **Iterative updates** use only the previous estimates of state values.\n",
    "* **Goal:** obtain the expected return for each state when following policy $`\\pi`$.\n",
    "\n",
    "Once the value function is computed, it can guide **policy improvement**‚Äîevaluating whether better actions exist than those chosen by the current policy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Improvement Using the Estimated Value Function**\n",
    "\n",
    "\n",
    "Once the value function $`v_\\pi(s)`$ for a policy $`\\pi`$ is known, the next step is to **improve the policy** by using that value function to make better action choices.\n",
    "\n",
    "The goal is to find a new policy $`\\pi'`$ such that:\n",
    "\n",
    "$$v_{\\pi'}(s) \\ge v_\\pi(s), \\ \\forall s$$\n",
    "\n",
    "To achieve this, the **action-value function** $`q_\\pi(s, a)`$ is computed using the estimated $`v_\\pi(s)`$:\n",
    "\n",
    "$$q_\\pi(s, a) = \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "For each state $`s`$, the action $`a`$ that maximizes $`q_\\pi(s, a)`$ is chosen:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a q_\\pi(s, a)$$\n",
    "\n",
    "This process‚Äî**evaluating** the policy and then **improving** it‚Äîforms the basis of **policy iteration**. Repeated alternation between these two steps leads to the **optimal policy** $`\\pi^*`$ that maximizes the expected return across all states.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Iteration**\n",
    "\n",
    "Policy Iteration is an iterative process that alternates between two key steps‚Äî**policy evaluation** and **policy improvement**‚Äîto find the optimal policy, $`\\pi^*`$.\n",
    "\n",
    "1. **Policy Evaluation:**\n",
    "   Compute the value function $`v_\\pi(s)`$ for the current policy $`\\pi`$ by estimating expected returns for all states.\n",
    "\n",
    "2. **Policy Improvement:**\n",
    "   Update the policy to choose the action that maximizes the action-value function:\n",
    "   $$\\pi'(s) = \\arg\\max_a q_\\pi(s, a)$$\n",
    "\n",
    "If the updated policy $`\\pi'`$ is the same as $`\\pi`$, the algorithm has converged to the **optimal policy**, satisfying:\n",
    "\n",
    "$`v_\\pi(s) = v_{\\pi'}(s) = v^*(s)`$ for all $`s`$.\n",
    "\n",
    "This process guarantees convergence to the optimal policy through repeated evaluation and improvement steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Value Iteration**\n",
    "\n",
    "\n",
    "Value Iteration streamlines the process of finding the optimal policy by combining **policy evaluation** and **policy improvement** into a single update step.\n",
    "\n",
    "Instead of fully evaluating a policy before improving it, Value Iteration directly updates the state-value function using the **Bellman Optimality Equation**:\n",
    "\n",
    "$$v_{i+1}(s) = \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v_i(s')]$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $`p(s', r | s, a)`$ = transition probability of reaching next state $`s'`$ with reward $`r`$ after action $`a`$.\n",
    "* The **max** operator ensures selection of the optimal action at each step.\n",
    "\n",
    "Once $`v^*(s)`$ converges, the **optimal policy** is derived as:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma v^*(s')]$$\n",
    "\n",
    "Thus, Value Iteration efficiently converges to the optimal value function and policy simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb667d0",
   "metadata": {},
   "source": [
    "### **Reinforcement Learning with Monte Carlo (MC) Methods**\n",
    "\n",
    "\n",
    "Monte Carlo methods address one of the main limitations of **Dynamic Programming (DP)** ‚Äî the need to know the environment‚Äôs full transition dynamics $`p(s', r | s, a)`$. Instead, MC methods learn **directly from experience** by interacting with the environment and estimating value functions based on **sampled returns**.\n",
    "\n",
    "Key characteristics of Monte Carlo methods:\n",
    "\n",
    "1. **No knowledge of environment dynamics**\n",
    "\n",
    "   * The agent does not need transition probabilities or reward distributions.\n",
    "   * Learning occurs through **simulated episodes** ‚Äî sequences of state, action, reward tuples $(s_t, a_t, r_{t+1}, s_{t+1}, ‚Ä¶)$.\n",
    "\n",
    "2. **Learning from complete episodes**\n",
    "\n",
    "   * Each episode runs until a terminal state is reached.\n",
    "   * The **return** from time step *t* is computed as:\n",
    "     \n",
    "     $$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$$\n",
    "\n",
    "3. **Estimating value functions**\n",
    "\n",
    "   * For each state *s* visited, the value function is estimated as the **average of observed returns**:\n",
    "     \n",
    "     $$v_\\pi(s) ‚âà \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_t^{(i)}$$\n",
    "     \n",
    "     where $`N(s)`$ is the number of times state *s* has been visited under policy $`\\pi`$.\n",
    "\n",
    "4. **Policy evaluation and improvement**\n",
    "\n",
    "   * MC methods can be used for **policy evaluation** (estimating $`v_\\pi(s)`$) and **policy control** (improving $`\\pi`$ based on observed returns).\n",
    "\n",
    "In summary, Monte Carlo reinforcement learning removes the need for explicit environment modeling and instead **learns from empirical experience**, making it suitable for problems with unknown or complex dynamics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **State-Value Function Estimation Using Monte Carlo (MC)**\n",
    "\n",
    "Monte Carlo methods estimate the **state-value function** $`V(s)`$ directly from sampled episodes, without requiring knowledge of the environment dynamics.\n",
    "\n",
    "**First-Visit MC Value Prediction:**\n",
    "\n",
    "1. Generate multiple episodes by following a policy $`\\pi`$.\n",
    "\n",
    "2. For each state $`s`$ in an episode, identify the **first time step $t$** that $`s`$ is visited.\n",
    "\n",
    "3. Compute the **return** from that time step onward:\n",
    "   \n",
    "   $$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$$\n",
    "\n",
    "\n",
    "4. Update the value function $`V(s)`$ as the **average of all first-visit returns** for state $`s`$:\n",
    "   \n",
    "   $$V(s) ‚âà \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_t^{(i)}$$\n",
    "\n",
    "   where $`N(s)`$ is the number of first visits to $`s`$ across episodes.\n",
    "\n",
    "This approach allows estimating the expected return for each state empirically, purely from **experience**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Action-Value Function Estimation Using Monte Carlo (MC)**\n",
    "\n",
    "\n",
    "When the environment dynamics are unknown, the **action-value function** $`Q(s, a)`$ cannot be inferred from the state-value function alone. Monte Carlo methods estimate $`Q(s, a)`$ directly from **experience**:\n",
    "\n",
    "1. **First-Visit MC for State-Action Pairs:**\n",
    "\n",
    "   * Generate episodes by following a policy $`\\pi`$.\n",
    "\n",
    "   * For each **state-action pair** $(s, a)$, consider the **first time step $t$** in the episode where the agent is in state $s$ and takes action $a$.\n",
    "\n",
    "   * Compute the **return** from that time step onward:\n",
    "\n",
    "     $$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots$$\n",
    "\n",
    "   * Update $`Q(s, a)`$ as the **average of all first-visit returns** for that state-action pair:\n",
    "\n",
    "     $$Q(s, a) ‚âà \\frac{1}{N(s, a)} \\sum_{i=1}^{N(s, a)} G_t^{(i)}$$\n",
    "\n",
    "2. **Handling Insufficient Exploration:**\n",
    "\n",
    "   * Some actions may rarely be selected, leading to poor estimates.\n",
    "   * Solutions include:\n",
    "\n",
    "     * **Exploring starts:** Ensure every $(s, a)$ pair has a non-zero probability of being chosen initially.\n",
    "     * **ùúñ-greedy policies:** Introduce a small probability ùúñ of choosing a random action to encourage exploration (discussed in policy improvement).\n",
    "\n",
    "This approach allows empirical estimation of the **expected return for each state-action pair**, forming the basis for **MC control** and policy improvement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Finding an Optimal Policy Using Monte Carlo (MC) Control**\n",
    "\n",
    "\n",
    "MC control is the procedure for **optimizing a policy** by alternating between **policy evaluation** and **policy improvement**, similar to policy iteration in dynamic programming. The process works as follows:\n",
    "\n",
    "1. **Start with a random policy** $`\\pi_0`$.\n",
    "\n",
    "2. **Policy Evaluation:** Estimate the action-value function $`Q^{\\pi_0}(s, a)`$ using MC methods (first-visit or every-visit).\n",
    "\n",
    "3. **Policy Improvement:** Update the policy by choosing actions that **maximize the estimated action-value**:\n",
    "   \n",
    "   $$\\pi_1(s) = \\arg\\max_a Q^{\\pi_0}(s, a)$$\n",
    "\n",
    "4. **Repeat:** Alternate evaluation and improvement:\n",
    "   \n",
    "   $$\\pi_0 \\xrightarrow{Eval} Q^{\\pi_0} \\xrightarrow{Imp} \\pi_1 \\xrightarrow{Eval} Q^{\\pi_1} \\xrightarrow{Imp} \\pi_2 \\dots$$\n",
    "\n",
    "5. **Convergence:** Continue until the policy stabilizes at the **optimal policy** $`\\pi^*`$, and the corresponding action-value function converges to $`Q^*`$.\n",
    "\n",
    "This iterative approach ensures that, with sufficient exploration, MC control converges to the **optimal policy and optimal action-value function**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Improvement ‚Äì Greedy and ùúñ-greedy Policies**\n",
    "\n",
    "Given an **action-value function** $`q(s, a)`$, a **greedy policy** chooses the action with the highest value:\n",
    "\n",
    "$`\\pi(s) \\; \\equiv \\; \\arg\\max_a q(s, a)`$\n",
    "\n",
    "To ensure **exploration** of all state-action pairs and avoid getting stuck in suboptimal actions, we use the **ùúñ-greedy policy**:\n",
    "\n",
    "* The **optimal action** at state $s$ is chosen with probability:\n",
    "  \n",
    "  $$1 - \\frac{(|A(s)| - 1)\\epsilon}{|A(s)|}$$\n",
    "\n",
    "* Each **non-optimal action** has a small probability:\n",
    "  \n",
    "  $$\\frac{\\epsilon}{|A(s)|}$$\n",
    "\n",
    "This way, the agent mostly exploits the best-known action but still explores other actions occasionally, balancing **exploration and exploitation**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d31a5e",
   "metadata": {},
   "source": [
    "### **Temporal Difference (TD) Learning**\n",
    "\n",
    "TD learning is a model-free RL method that combines aspects of **Monte Carlo (MC)** and **dynamic programming (DP)**:\n",
    "\n",
    "* Like MC, it **learns from experience** and does **not require knowledge of environment dynamics**.\n",
    "\n",
    "* Unlike MC, it **updates estimates before the episode ends**, using **bootstrapping**: it updates a value based on other learned values.\n",
    "\n",
    "TD addresses two main tasks:\n",
    "\n",
    "1. **Value Prediction:** Estimating the value function $`v_\\pi(s)`$ from experience.\n",
    "\n",
    "2. **Control:** Improving the policy using the estimated values.\n",
    "\n",
    "This allows the agent to learn **online**, step by step, without waiting for full episode returns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **TD Prediction (Value Estimation with TD Learning)**\n",
    "\n",
    "TD learning estimates the **state-value function** by updating values **step by step** rather than waiting for the episode to end, unlike Monte Carlo (MC) methods.\n",
    "\n",
    "\n",
    "**MC update:**\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_{t:T} - V(S_t))$$\n",
    "\n",
    "* $`G_{t:T}`$ is the actual return from step t to the end of the episode.\n",
    "* Requires the episode to finish.\n",
    "\n",
    "\n",
    "**TD(0) update:**\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$$\n",
    "\n",
    "* Uses the **observed reward** $`R_{t+1}`$ and the **estimated value of the next state** $`V(S_{t+1})`$.\n",
    "* Updates **online**, one step at a time.\n",
    "\n",
    "\n",
    "\n",
    "**n-step TD generalization:**\n",
    "\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha (G_{t:t+n} - V(S_t))$$\n",
    "\n",
    "* $`G_{t:t+n}`$ includes the **weighted sum of n future rewards** plus the value estimate at step t+n.\n",
    "* If $`n=1`$, it reduces to TD(0).\n",
    "* If $`n \\to \\text{episode length}`$, it becomes equivalent to MC.\n",
    "\n",
    "\n",
    "**Key idea:** TD methods **bootstrap** by combining observed rewards and current estimates to update values continuously, making learning more efficient than MC.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **On-Policy TD Control (SARSA) vs. Off-Policy TD Control (Q-Learning)**\n",
    "\n",
    "\n",
    "**SARSA (On-Policy TD Control):**\n",
    "\n",
    "* Estimates the **action-value function** $`Q(s, a)`$ using the **same policy** that the agent is following.\n",
    "\n",
    "* Update rule (one-step TD/SARSA):\n",
    "\n",
    "  $$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\n",
    "\n",
    "* Uses the quintuple $`(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})`$ the next action $`A_{t+1}`$ is **chosen according to the current policy**.\n",
    "\n",
    "* Policy improvement is usually done using an **ùúñ-greedy** approach on the current Q-values.\n",
    "\n",
    "* Called **on-policy** because learning is based on the policy the agent actually follows.\n",
    "\n",
    "\n",
    "\n",
    "**Q-Learning (Off-Policy TD Control):**\n",
    "\n",
    "* Also estimates the **action-value function**, but **does not rely on the agent‚Äôs current policy** for the next step.\n",
    "\n",
    "* Update rule:\n",
    "\n",
    "  $$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$\n",
    "\n",
    "* Instead of using $`A_{t+1}`$ from the policy, it uses the **best possible action** in $`S_{t+1}`$ to update Q.\n",
    "\n",
    "* Called **off-policy** because it learns about the optimal policy **independently of the agent‚Äôs current behavior**.\n",
    "\n",
    "\n",
    "**Key Difference:**\n",
    "\n",
    "* SARSA is **on-policy** ‚Üí updates depend on the action actually taken.\n",
    "\n",
    "* Q-learning is **off-policy** ‚Üí updates depend on the **greedy action**, not necessarily taken by the agent.\n",
    "\n",
    "This distinction affects **exploration behavior**: SARSA considers risky actions taken under the current policy, while Q-learning always aims toward the optimal policy regardless of exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8f684",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6023f",
   "metadata": {},
   "source": [
    "### **Implementing our first RL algorithm**\n",
    "\n",
    "\n",
    "**Introducing the OpenAI Gym toolkit**\n",
    "\n",
    "OpenAI Gym is a specialized toolkit for facilitating the development of RL models. OpenAI Gym comes with several predefined environments. Some basic examples are CartPole and MountainCar, where the tasks are to balance a pole and to move a car up a hill, respectively, as the names suggest. There are also many advanced robotics environments for training a robot to fetch, push, and reach for items on a bench or training a robotic hand to orient blocks, balls, or pens.\n",
    "\n",
    "\n",
    "**Working with the existing environments in OpenAI Gym**\n",
    "\n",
    "For practice with the Gym environments, let‚Äôs create an environment from CartPole-v1, which already exists in OpenAI Gym. In this example environment, there is a pole attached to a cart that can move horizontally, as shown below;\n",
    "\n",
    "![The CartPole example in Gym](./figures/19_06.png)\n",
    "\n",
    "The movements of the pole are governed by the laws of physics, and the goal for RL agents is to learn how to move the cart to stabilize the pole and prevent it from tipping over to either side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acb3f8",
   "metadata": {},
   "source": [
    "- Now, let‚Äôs look at some properties of the CartPole environment in the context of RL, such as its state (or observation) space, action space, and how to execute an action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adfcae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e4cb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium\n",
    "env = gymnasium.make('CartPole-v1')\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eee516e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705b7466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.03785994, 0.03869196, 0.00572113, 0.02838193], dtype=float32), {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c228bc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03863377, -0.15651157,  0.00628876,  0.3228644 ], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27106f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.03550354, 0.03852027, 0.01274605, 0.03217134], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd30be6",
   "metadata": {},
   "source": [
    "### **A grid world example**\n",
    "\n",
    "After introducing the CartPole environment as a warm-up exercise for working with the OpenAI Gym toolkit, we will now switch to a different environment. We will work with a grid world example, which is a simplistic environment with m rows and n columns. Considering m = 5 and n = 6, we can summarize this environment as shown below;\n",
    "\n",
    "\n",
    "![An example of a grid world environment](./figures/19_07.png)\n",
    "\n",
    "\n",
    "In this environment, there are 30 different possible states. Four of these states are terminal states: a pot of gold at state 16 and three traps at states 10, 15, and 22. Landing in any of these four terminal states will end the episode, but with a difference between the gold and trap states. Landing on the\n",
    "gold state yields a positive reward, +1, whereas moving the agent onto one of the trap states yields a negative reward, ‚Äì1. All other states have a reward of 0. The agent always starts from state 0. Therefore, every time we reset the environment, the agent will go back to state 0. The action space consists of\n",
    "four directions: move up, down, left, and right.\n",
    "\n",
    "When the agent is at the outer boundary of the grid, selecting an action that would result in leaving the grid will not change the state.\n",
    "\n",
    "\n",
    "\n",
    "![ A visualization of our grid world environment](./figures/19_08.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f15ff3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d138f",
   "metadata": {},
   "source": [
    "#### **Solving the grid world problem with Q-learning**\n",
    "\n",
    "After focusing on the theory and the development process of RL algorithms, as well as setting up the environment via the OpenAI Gym toolkit, we will now implement the currently most popular RL algorithm, Q-learning. For this, we will use the grid world example that we already implemented in\n",
    "the script `gridworld_env.py`.\n",
    "\n",
    "\n",
    "![The agent‚Äôs number of moves and rewards](./figures/19_09.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919fc05",
   "metadata": {},
   "source": [
    "### **A glance at deep Q-learning**\n",
    "\n",
    "\n",
    "Deep Q-Learning extends classical Q-learning by using **deep neural networks** to approximate the Q-value function $(Q(s, a; \\theta))$, enabling reinforcement learning in **high-dimensional or continuous state spaces** where tabular methods fail. Here's a structured deep dive:\n",
    "\n",
    "\n",
    "![An example of a DQN](./figures/19_10.png)\n",
    "\n",
    "\n",
    "\n",
    "##### 1. Motivation\n",
    "\n",
    "* **Classical Q-learning limitation**: Tabular methods require storing $(Q(s, a))$ for every state-action pair ‚Üí infeasible for large/continuous state spaces.\n",
    "* **Solution**: Approximate $(Q(s, a))$ with a neural network parameterized by $(\\theta)$ (weights and biases).\n",
    "\n",
    "\n",
    "\n",
    "##### 2. Q-Learning Recap\n",
    "\n",
    "Classical Q-learning updates the action-value function as:\n",
    "\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\big]$$\n",
    "\n",
    "* $(\\alpha)$: learning rate\n",
    "\n",
    "* $(\\gamma)$: discount factor\n",
    " \n",
    "* $(R_{t+1})$: reward at next step\n",
    " \n",
    "* $(\\max_a Q(S_{t+1}, a))$: estimate of the optimal future return\n",
    "\n",
    "\n",
    "\n",
    "##### 3. Deep Q-Network (DQN)\n",
    "\n",
    "**3.1 Neural Network Approximation**\n",
    "\n",
    "* Input: state $(s)$\n",
    "\n",
    "* Output: Q-values for all possible actions $(a)$\n",
    "\n",
    "* Loss function (Mean Squared Error):\n",
    "\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\mathbb{E}_{s,a,r,s'} \\Big[ \\big( y - Q(s, a; \\theta) \\big)^2 \\Big]$$\n",
    "\n",
    "where the target (y) is:\n",
    "\n",
    "\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "* $(\\theta^-)$ are parameters of the **target network**, a stabilized copy of the main network.\n",
    "\n",
    "\n",
    "\n",
    "**3.2 Key Techniques for Stability**\n",
    "\n",
    "1. **Experience Replay**\n",
    "\n",
    "   * Store agent experiences $((s, a, r, s'))$ in a replay buffer.\n",
    "   \n",
    "   * Sample mini-batches randomly to break correlation between consecutive steps.\n",
    "   \n",
    "   * Reduces variance and improves stability.\n",
    "\n",
    "2. **Target Network**\n",
    "\n",
    "   * Maintain a separate network $(Q(s, a; \\theta^-))$ to compute the target $(y)$.\n",
    "   \n",
    "   * Update $(\\theta^-)$ periodically instead of at every step to reduce oscillations.\n",
    "\n",
    "3. **$(\\epsilon)$ -greedy Policy**\n",
    "\n",
    "   * Encourages exploration: choose random action with probability (\\epsilon), otherwise take (\\arg\\max_a Q(s, a)).\n",
    "\n",
    "\n",
    "\n",
    "##### 4. DQN Algorithm (Stepwise)\n",
    "\n",
    "1. Initialize main network $(Q(s, a; \\theta))$ and target network $(Q(s, a; \\theta^-))$.\n",
    "\n",
    "2. Initialize replay buffer.\n",
    "\n",
    "3. For each episode:\n",
    "\n",
    "   1. Reset environment $(s_0)$.\n",
    "   2. For each step:\n",
    "\n",
    "      * Select action $(a_t)$ using $(\\epsilon)$-greedy policy.\n",
    "  \n",
    "      * Observe reward $(r_{t+1})$ and next state $(s_{t+1})$.\n",
    "  \n",
    "      * Store $((s_t, a_t, r_{t+1}, s_{t+1}))$ in replay buffer.\n",
    "  \n",
    "      * Sample mini-batch from buffer.\n",
    "  \n",
    "      * Compute target $(y = r + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta^-))$.\n",
    "  \n",
    "      * Update network $(\\theta)$ by minimizing $((y - Q(s_t, a_t; \\theta))^2)$.\n",
    "  \n",
    "      * Periodically update $(\\theta^- \\gets \\theta)$.\n",
    "\n",
    "\n",
    "\n",
    "##### 5. Extensions and Improvements\n",
    "\n",
    "* **Double DQN**: Reduces overestimation by decoupling action selection and evaluation.\n",
    "\n",
    "* **Dueling DQN**: Separates state-value and advantage streams in the network.\n",
    "\n",
    "* **Prioritized Experience Replay**: Samples more important transitions with higher probability.\n",
    "\n",
    "* **Rainbow DQN**: Combines several improvements for state-of-the-art performance.\n",
    "\n",
    "\n",
    "\n",
    "##### 6. Summary\n",
    "\n",
    "* DQN allows **Q-learning to scale to high-dimensional inputs** (images, complex states).\n",
    "\n",
    "* Stability is achieved with **experience replay** and a **target network**.\n",
    "\n",
    "* Policy is implicit: $(\\pi(s) = \\arg\\max_a Q(s, a))$.\n",
    "\n",
    "* Widely used in **Atari games, robotics, and control tasks**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e080345",
   "metadata": {},
   "source": [
    "### **Training a DQN Model Using the Q-Learning Algorithm**\n",
    "\n",
    "Deep Q-Learning (DQN) adapts the classical Q-learning algorithm to **high-dimensional state spaces** by approximating the action-value function (Q(s, a)) with a neural network. Here‚Äôs a structured explanation of the training procedure:\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Core Idea\n",
    "\n",
    "In classical Q-learning, the update rule is:\n",
    "\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\big[ R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \\big]$$\n",
    "\n",
    "For DQN, the Q-values are approximated using a neural network (Q(s, a; \\theta)), so the update becomes a **supervised learning problem**:\n",
    "\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\Big[ \\big( y - Q(s, a; \\theta) \\big)^2 \\Big]$$\n",
    "\n",
    "where the target $(y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-))$, and $(\\theta^-)$ are parameters of a **target network**.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Key Components\n",
    "\n",
    "1. **Replay Buffer**\n",
    "\n",
    "   * Stores transitions $((s_t, a_t, r_{t+1}, s_{t+1}))$.\n",
    "   \n",
    "   * Mini-batches are sampled randomly for network updates to **break correlation** and stabilize learning.\n",
    "\n",
    "2. **Target Network**\n",
    "\n",
    "   * A separate network $(Q(s, a; \\theta^-))$ used to compute targets $(y)$.\n",
    "   \n",
    "   * Updated periodically (not every step) to reduce oscillations.\n",
    "\n",
    "3. **$(\\epsilon)$-Greedy Policy**\n",
    "\n",
    "   * Exploration strategy: with probability (\\epsilon) take a random action; otherwise, choose $(\\arg\\max_a Q(s, a))$.\n",
    "   \n",
    "   * $(\\epsilon)$ typically decays over time.\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Training Steps\n",
    "\n",
    "1. **Initialize networks and replay buffer**\n",
    "\n",
    "   * Main Q-network: $(Q(s, a; \\theta))$\n",
    "   \n",
    "   * Target Q-network: $(Q(s, a; \\theta^-))$\n",
    "   \n",
    "   * Replay buffer $(\\mathcal{D})$\n",
    "\n",
    "2. **For each episode**\n",
    "\n",
    "   1. Reset the environment: $(s_0 \\gets env.reset())$\n",
    "   2. For each time step $(t)$:\n",
    "\n",
    "      * Choose action $(a_t)$ using $(\\epsilon)$-greedy policy.\n",
    "      \n",
    "      * Execute action $(a_t)$ and observe $(r_{t+1}, s_{t+1})$.\n",
    "      \n",
    "      * Store transition $((s_t, a_t, r_{t+1}, s_{t+1}))$ in replay buffer.\n",
    "      \n",
    "      * Sample mini-batch from buffer.\n",
    "      \n",
    "      * Compute target: $(y = r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta^-))$\n",
    "      \n",
    "      * Update main network $(\\theta)$ by minimizing $((y - Q(s_t, a_t; \\theta))^2)$.\n",
    "      \n",
    "      * Every fixed number of steps, update target network: $(\\theta^- \\gets \\theta)$.\n",
    "      \n",
    "      * Update state: $(s_t \\gets s_{t+1})$\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Important Hyperparameters\n",
    "\n",
    "| Hyperparameter                  | Typical Range      | Role                             |\n",
    "| ------------------------------- | ------------------ | -------------------------------- |\n",
    "| $(\\gamma)$ (discount factor)    | 0.9 ‚Äì 0.99         | Future reward weighting          |\n",
    "| $(\\alpha)$ (learning rate)      | 1e-4 ‚Äì 1e-3        | Step size for gradient updates   |\n",
    "| Replay buffer size              | 10^4 ‚Äì 10^6        | Storage for past transitions     |\n",
    "| Mini-batch size                 | 32 ‚Äì 128           | Samples per update               |\n",
    "| Target network update frequency | 1000 ‚Äì 10000 steps | Stabilize training               |\n",
    "| $(\\epsilon)$ (exploration)      | 1 ‚Üí 0.01 (decay)   | Balance exploration/exploitation |\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Summary\n",
    "\n",
    "* DQN combines **Q-learning** with **deep neural networks**, **experience replay**, and a **target network**.\n",
    "\n",
    "* The network approximates the action-value function $(Q(s, a))$, and the target uses a separate stabilized network.\n",
    "\n",
    "* Training alternates between **environment interaction** and **network updates** using sampled mini-batches.\n",
    "\n",
    "* Over time, the Q-network converges to the optimal Q-values, allowing the greedy policy $(\\pi(s) = \\arg\\max_a Q(s, a))$ to perform optimally.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fac28b",
   "metadata": {},
   "source": [
    "### **Replay Memory in Deep Q-Learning**\n",
    "\n",
    "When approximating (Q(s, a)) with a neural network, updating the weights for one state-action pair can inadvertently change the Q-values of other states. Additionally, standard neural network training assumes **IID (independent and identically distributed) samples**, which is violated when learning from sequential transitions in RL.\n",
    "\n",
    "**Replay memory** solves these problems:\n",
    "\n",
    "![The replay memory process](./figures/19_11.png)\n",
    "\n",
    "\n",
    "#### 1. Purpose\n",
    "\n",
    "1. **Break temporal correlations:** By storing transitions and sampling randomly, we reduce the sequential correlation between states in episodes.\n",
    "\n",
    "2. **Revisit past experiences:** Earlier state-action pairs remain in memory and can be used multiple times for training.\n",
    "\n",
    "3. **Stabilize learning:** Random sampling creates a more stable and diverse training dataset, making gradient updates more reliable.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Structure\n",
    "\n",
    "* **Transition tuple:** $((s_t, a_t, r_{t+1}, s_{t+1}, done))$\n",
    "\n",
    "* **Memory buffer:** Fixed-size container (e.g., a Python list or deque).\n",
    "\n",
    "* **Replacement policy:** When full, remove the oldest transition to make room for new ones.\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Workflow\n",
    "\n",
    "1. **Collect transition:** After taking action $(a_t)$ in state $(s_t)$ and observing reward $(r_{t+1})$ and next state $(s_{t+1})$, store the tuple in the replay memory.\n",
    "\n",
    "2. **Sample mini-batch:** Randomly select a batch of transitions from the replay memory.\n",
    "\n",
    "3. **Train network:** Compute targets using the sampled transitions and update the network weights using gradient descent.\n",
    "\n",
    "4. **Repeat:** Continue interacting with the environment, adding new transitions, and updating the network with mini-batches from the memory.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Pseudocode\n",
    "\n",
    "```\n",
    "Initialize replay memory D to capacity N\n",
    "for each episode:\n",
    "    s = env.reset()\n",
    "    for each step:\n",
    "        a = select_action(s)  # epsilon-greedy\n",
    "        s', r, done = env.step(a)\n",
    "        store (s, a, r, s', done) in D\n",
    "        sample random mini-batch from D\n",
    "        compute target y = r + gamma * max_a' Q(s', a'; theta_target)\n",
    "        update network using (y - Q(s, a; theta))^2\n",
    "        s = s'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Key Advantages of Replay Memory**\n",
    "\n",
    "* Converts correlated sequences into IID-like training samples.\n",
    "\n",
    "* Enables multiple learning passes on past experiences.\n",
    "\n",
    "* Reduces variance in gradient updates, improving stability.\n",
    "\n",
    "Replay memory is thus **essential** for stabilizing deep Q-learning when using neural networks as function approximators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2724be3",
   "metadata": {},
   "source": [
    "### **Determining Target Values for DQN Loss Computation**\n",
    "\n",
    "In tabular Q-learning, the update rule is simple because we directly modify the Q-value for a specific state-action pair. In **Deep Q-Networks (DQN)**, the Q-function is approximated by a neural network (Q_w(s, a)), so we need to adapt the update procedure for gradient-based learning.\n",
    "\n",
    "\n",
    "\n",
    "![Determining the target value using the DQN](./figures/19_12.png)\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Transition Quintuple\n",
    "\n",
    "Each transition stored in replay memory has the form:\n",
    "\n",
    "\n",
    "$$T = (x_s, a, r, x_{s'}, \\text{done})$$\n",
    "\n",
    "\n",
    "* $(x_s)$: current state features\n",
    "\n",
    "* $(a)$: action taken\n",
    "\n",
    "* $(r)$: reward observed\n",
    "\n",
    "* $(x_{s'})$: next state features\n",
    "\n",
    "* $(\\text{done})$: boolean indicating episode termination\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Forward Passes\n",
    "\n",
    "1. **Current state pass:** Input $(x_s)$ into the DQN to get predicted Q-values for all actions:\n",
    "\n",
    "\n",
    "$$Q_w(x_s, :) = [Q_w(x_s, a_1), Q_w(x_s, a_2), \\dots, Q_w(x_s, a_n)]$$\n",
    "\n",
    "\n",
    "2. **Next state pass:** Input $(x_{s'})$ into the DQN to get predicted Q-values for the next state:\n",
    "\n",
    "\n",
    "$$Q_w(x_{s'}, :) = [Q_w(x_{s'}, a_1), Q_w(x_{s'}, a_2), \\dots, Q_w(x_{s'}, a_n)]$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Computing Target Values\n",
    "\n",
    "The **scalar target** for the chosen action (a) is computed using the Q-learning update formula:\n",
    "\n",
    "\n",
    "$$y = r + \\gamma \\max_{a'} Q_w(x_{s'}, a')$$\n",
    "\n",
    "* $(r)$ is the observed reward.\n",
    "\n",
    "* $(\\gamma)$ is the discount factor.\n",
    "\n",
    "* $(\\max_{a'} Q_w(x_{s'}, a'))$ approximates the best future value.\n",
    "\n",
    "* If $(\\text{done} = \\text{True})$, then $(y = r)$ because no future state exists.\n",
    "\n",
    "Instead of updating only a scalar, we form a **target Q-value vector**:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = Q_w(x_s, :)$$\n",
    "\n",
    "\n",
    "Then, we **replace only the element corresponding to action (a)** with the target (y):\n",
    "\n",
    "\n",
    "$$y_a = r + \\gamma \\max_{a'} Q_w(x_{s'}, a')$$\n",
    "\n",
    "All other actions $(a' \\neq a)$ retain their original predicted Q-values.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Why a Target Vector?\n",
    "\n",
    "* Preserves the Q-values of actions **not taken**, preventing unnecessary updates.\n",
    "\n",
    "* Enables **batch-wise gradient computation** with standard loss functions (e.g., MSE) in deep learning frameworks.\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Loss Computation\n",
    "\n",
    "Finally, the loss is computed as:\n",
    "\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N \\left| \\mathbf{y}_i - Q_w(x_s^i, :) \\right|^2$$\n",
    "\n",
    "* $(N)$ is the mini-batch size.\n",
    "\n",
    "* Only the chosen actions are effectively updated due to the construction of $(\\mathbf{y}_i)$.\n",
    "\n",
    "This approach integrates **tabular Q-learning logic** with **neural network training**, ensuring stable and efficient updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4d537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fae86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ade6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a959f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffd4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11695b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92e049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab0dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70688481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92b159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
