{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee42789",
   "metadata": {},
   "source": [
    "# **Generative Adversarial Networks for Synthesizing New Data (Part 1/2)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290db3fa",
   "metadata": {},
   "source": [
    "### **Generative Adversarial Networks (GANs)**\n",
    "\n",
    "\n",
    "**1. Core Idea**\n",
    "\n",
    "A GAN consists of **two neural networks** trained **simultaneously but with opposing objectives**:\n",
    "\n",
    "| Component             | Role                                   | Goal                                                            |\n",
    "| --------------------- | -------------------------------------- | --------------------------------------------------------------- |\n",
    "| **Generator (G)**     | Produces synthetic samples             | Fool the discriminator into thinking generated samples are real |\n",
    "| **Discriminator (D)** | Classifies inputs as real or generated | Correctly distinguish real samples from generated samples       |\n",
    "\n",
    "You can think of this as a **zero-sum game**:\n",
    "\n",
    "* The **generator** tries to *minimize* the probability of being detected.\n",
    "* The **discriminator** tries to *maximize* its classification accuracy.\n",
    "\n",
    "\n",
    "\n",
    "**2. Mathematical Formulation**\n",
    "\n",
    "The original GAN objective (Goodfellow et al., 2014):\n",
    "\n",
    "$$\\min_G \\max_D \\; V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log (1 - D(G(z)))]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $`p_{\\text{data}}`$ is the real data distribution\n",
    "* $`p_z`$ is a simple noise distribution (e.g., Gaussian or Uniform)\n",
    "* $`z`$ is a random noise vector fed to the generator\n",
    "* $`G(z)`$ is the generated fake sample\n",
    "* $`D(x)`$ outputs a probability that $`x`$ is real\n",
    "\n",
    "The training process alternates:\n",
    "\n",
    "1. Update $`D`$ to maximize correct classification.\n",
    "2. Update $`G`$ to minimize the discriminator’s ability to detect fakes.\n",
    "\n",
    "\n",
    "\n",
    "**3. Generator Network (G)**\n",
    "\n",
    "**Input:** Noise vector $`z \\sim p_z`$ (e.g., $`z ∈ \\mathbb{R}^{128}`$)\n",
    "\n",
    "**Output:** Synthetic example in data space (e.g., $`28×28`$ image)\n",
    "\n",
    "Goal: Learn a **mapping**:\n",
    "\n",
    "$$G: \\mathbb{R}^k \\rightarrow \\mathbb{R}^n$$\n",
    "\n",
    "* If generating images: often uses **transposed convolution layers** to upsample noise to image resolution.\n",
    "* Without guidance, the generator tries to **match the real data distribution**.\n",
    "\n",
    "\n",
    "\n",
    "**4. Discriminator Network (D)**\n",
    "\n",
    "**Input:** Either real or generated sample\n",
    "\n",
    "**Output:** A scalar $`D(x) ∈ [0, 1]`$ (probability real)\n",
    "\n",
    "Goal: Learn a **decision boundary** that distinguishes:\n",
    "\n",
    "* Real data $`x ~ p_{\\text{data}}`$\n",
    "* Fake data $`G(z)`$\n",
    "\n",
    "Typically uses **standard convolutional networks** for image data.\n",
    "\n",
    "\n",
    "\n",
    "**5. Training Instability Issues**\n",
    "\n",
    "GANs are known to be **unstable** due to the adversarial optimization structure.\n",
    "\n",
    "Common issues:\n",
    "\n",
    "| Issue                   | Description                                                              |\n",
    "| ----------------------- | ------------------------------------------------------------------------ |\n",
    "| **Mode Collapse**       | Generator produces limited diversity (e.g., same face for all inputs)    |\n",
    "| **Non-convergence**     | Loss oscillates because the networks chase each other                    |\n",
    "| **Vanishing Gradients** | Discriminator becomes too strong → generator receives no learning signal |\n",
    "\n",
    "\n",
    "\n",
    "**6. Improvements to Stabilize Training**\n",
    "\n",
    "| Improvement                    | Idea                                                | Key Objective Change                                                        |\n",
    "| ------------------------------ | --------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| **WGAN** (Wasserstein GAN)     | Measure distance using Earth-Mover distance         | $`\\min_G \\max_{D \\in 1\\text{-Lip}} \\mathbb{E}[D(x)] - \\mathbb{E}[D(G(z))]`$ |\n",
    "| **Gradient Penalty (WGAN-GP)** | Enforces Lipschitz constraint using gradient norm   | Stabilizes gradients and prevents explosion                                 |\n",
    "| **Spectral Normalization**     | Constrain weight matrix norms                       | Keeps discriminator from being overly sharp                                 |\n",
    "| **Progressive Growing**        | Start small and grow networks (for high-res images) | Improves training stability for large outputs                               |\n",
    "\n",
    "\n",
    "\n",
    "**7. Why GANs Work (Game-Theoretic Perspective)**\n",
    "\n",
    "GAN training seeks a **Nash equilibrium**:\n",
    "\n",
    "* Generator’s best strategy matches the data distribution\n",
    "* Discriminator’s best strategy outputs $`1/2`$ everywhere\n",
    "\n",
    "At equilibrium:\n",
    "\n",
    "$$p_G = p_{\\text{data}}$$\n",
    "\n",
    "Meaning the generator has learned to **model the true data distribution**.\n",
    "\n",
    "\n",
    "\n",
    "**8. Intuition Summary**\n",
    "\n",
    "* The **generator** learns by **seeing where it fails**, not by being told the correct answer.\n",
    "* The **discriminator** provides **dense, informative gradients**, unlike traditional supervised labels.\n",
    "* The learning process is **self-supervised**: no real labels are required.\n",
    "* GANs are powerful for **distribution learning** and **sample generation**.\n",
    "\n",
    "\n",
    "\n",
    "**9. Where GANs Are Used**\n",
    "\n",
    "| Application                | Explanation                                         |\n",
    "| -------------------------- | --------------------------------------------------- |\n",
    "| Image synthesis            | Generate realistic human faces, landscapes, objects |\n",
    "| Image-to-image translation | e.g., Sketch → Photo, Day → Night                   |\n",
    "| Super-resolution           | Convert low-res images to high-res                  |\n",
    "| Data augmentation          | Generate synthetic training data                    |\n",
    "| Audio and Music synthesis  | Generate realistic sound samples                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a384a06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5f8ab",
   "metadata": {},
   "source": [
    "## **Introducing generative adversarial networks**\n",
    "\n",
    "- Objective of `GAN` is to synthesize new data samples that has the same distribution as its training dataset. \n",
    "- `GANs` are considered as **unsupervised learning** models since they do not require labeled data for training.\n",
    "- `GANs` are used in various applications including \n",
    "  - image generation,\n",
    "  - image-to-image translation,\n",
    "  - super-resolution,\n",
    "  - data augmentation, and\n",
    "  - audio synthesis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25136f34",
   "metadata": {},
   "source": [
    "### **Starting with autoencoders**\n",
    "\n",
    "- `Autoencoders` are neural networks that learn to compress data into a lower-dimensional representation and then reconstruct the original data from this representation.\n",
    "- They consist of two main components:\n",
    "  - **Encoder**: Maps input data to a latent space.\n",
    "  - **Decoder**: Reconstructs the input data from the latent representation.\n",
    "- The `encoder` acts as a feature extractor or data compression function, while the `decoder` serves as a generative model that can produce new data samples from the latent space.\n",
    "\n",
    "\n",
    "![Autoencoder Architecture](./figures/17_01.png)\n",
    "\n",
    "\n",
    "- We can add multiple hidden layers with nonlinearities (as in a multilayer NN) to create a **deep autoencoder**, that can learn more complex representations of the data.\n",
    "- Autoencoders uses Convolutional layers for image data to better capture spatial hierarchies and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415825b",
   "metadata": {},
   "source": [
    "### **Generative models for synthesizing new data**\n",
    "\n",
    "- `Autoencoders` are deterministic models that learn to reconstruct input data, but they do not inherently generate new, diverse samples. After an autoencoder is trained, given an input, $x$, it will be able to reconstruct the input from its compressed version in a lower-dimensional space. Therefore, it cannot generate entirely new samples that are different from the training data.\n",
    "\n",
    "- A `generative model` can generate new data samples, $\\hat{x}$, from a random noise vector, $z$, sampled from a simple distribution, such as a `Gaussian` or `uniform` distribution. The goal of the generative model is to learn a mapping from the noise space to the data space, such that the generated samples resemble the real data distribution. For example, each element of $z$ can be sampled from a standard normal distribution, i.e., $z_i \\sim \\mathcal{N}(0, 1)$ or from a uniform distribution, i.e., $z_i \\sim \\text{Uniform}(-1, 1)$.\n",
    "\n",
    "\n",
    "![Generative Model Mapping](./figures/17_02.png)\n",
    "\n",
    "\n",
    "- Both the `autoencoder` and the `generative model` learn a mapping from a lower-dimensional space to the data space. However, the key difference is that the autoencoder learns to reconstruct specific input data, while the generative model learns to produce new samples that resemble the overall data distribution.\n",
    "\n",
    "- It is possible to generalize an autoencoder into a generative model by introducing stochasticity into the latent space. This can be achieved using a `variational autoencoder (VAE)`, which learns a probabilistic mapping from the latent space to the data space. In a VAE, the encoder outputs parameters of a probability distribution (e.g., mean and variance of a Gaussian) instead of a single point in the latent space. During training, random samples are drawn from this distribution to generate new data samples through the decoder. This allows the VAE to generate diverse samples that capture the underlying data distribution, similar to other generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a4b22",
   "metadata": {},
   "source": [
    "### **Generating new samples with GANs**\n",
    "\n",
    "To understand what `GANs` are, we first need to understand their two main components: the **generator** and the **discriminator**. \n",
    "\n",
    "- **Generator (G)**: The generator is a neural network that takes a random noise vector, $z$, as input and produces synthetic data samples, $\\hat{x} = G(z)$. The goal of the generator is to create samples that are indistinguishable from real data.\n",
    "\n",
    "- **Discriminator (D)**: The discriminator is another neural network that takes either real data samples or generated samples as input and outputs a probability score, $D(x)$, indicating whether the input is real (from the training data) or fake (produced by the generator). The goal of the discriminator is to correctly classify real and fake samples.\n",
    "\n",
    "You can think of this as a **zero-sum game**:\n",
    "* The **generator** tries to *minimize* the probability of being detected.\n",
    "* The **discriminator** tries to *maximize* its classification accuracy.\n",
    "\n",
    "The training process involves alternating updates to the generator and discriminator:\n",
    "1. Update the discriminator to maximize its ability to distinguish real from fake samples.\n",
    "2. Update the generator to minimize the discriminator's ability to detect fake samples.\n",
    "\n",
    "\n",
    "![Discriminator distinguishing real vs fake samples](./figures/17_03.png)\n",
    "\n",
    "\n",
    "- In a `GAN` model, the generator and discriminator are trained simultaneously in an adversarial manner. The generator aims to produce realistic samples that can fool the discriminator, while the discriminator strives to become better at distinguishing real samples from those generated by the generator. This adversarial training process continues until the generator produces samples that are indistinguishable from real data, and the discriminator can no longer reliably tell them apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113644f8",
   "metadata": {},
   "source": [
    "### **Understanding the loss functions for the generator and discriminator networks in a GAN model**\n",
    "\n",
    "- The original `GAN` objective (Goodfellow et al., 2014):\n",
    "\n",
    "$$\\min_G \\max_D \\; V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log (1 - D(G(z)))]$$\n",
    "\n",
    "Where:\n",
    "* $`p_{\\text{data}}`$ is the real data distribution\n",
    "* $`p_z`$ is a simple noise distribution (e.g., Gaussian or Uniform)\n",
    "* $`z`$ is a random noise vector fed to the generator\n",
    "* $`G(z)`$ is the generated fake sample\n",
    "* $`D(x)`$ outputs a probability that $`x`$ is real or fake\n",
    "\n",
    "The expression consists of two parts:\n",
    "\n",
    "1. **Discriminator Loss**: $`\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)]`$ - This term encourages the discriminator to assign high probabilities to real data samples.\n",
    "\n",
    "2. **Generator Loss**: $`\\mathbb{E}_{z \\sim p_z}[\\log (1 - D(G(z)))]`$ - This term encourages the generator to produce samples that the discriminator classifies as real (i.e., low probabilities for fake samples).\n",
    "\n",
    "\n",
    "A practical way of training GANs is to alternate between updating the discriminator and the generator:\n",
    "\n",
    "1. **Update Discriminator (D)**: Maximize the objective with respect to $`D`$ while keeping $`G`$ fixed. This involves maximizing the likelihood of correctly classifying real and fake samples.\n",
    "\n",
    "2. **Update Generator (G)**: Minimize the objective with respect to $`G`$ while keeping $`D`$ fixed. This involves minimizing the likelihood of the discriminator correctly identifying fake samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f4cbf",
   "metadata": {},
   "source": [
    "### **Optimization objective of the generator network in a GAN model**\n",
    "\n",
    "\n",
    "**1. The GAN Value Function**\n",
    "\n",
    "The original GAN objective is a **minimax game**:\n",
    "\n",
    "$$\\min_G \\max_D V(D, G)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p_z}\\left[\\log(1 - D(G(z)))\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "**2. Training Alternates Between Two Steps**\n",
    "\n",
    "| Step                   | Network Updated | What’s Fixed | Goal                                        |\n",
    "| ---------------------- | --------------- | ------------ | ------------------------------------------- |\n",
    "| **Discriminator Step** | $D$             | $G$          | Maximize ability to tell real vs fake apart |\n",
    "| **Generator Step**     | $G$             | $D$          | Generate samples that fool $D$              |\n",
    "\n",
    "This alternating training keeps the learning signal flowing to both networks.\n",
    "\n",
    "\n",
    "**3. Updating the Discriminator**\n",
    "\n",
    "When **$G$ is fixed**, the discriminator receives two types of samples:\n",
    "\n",
    "| Sample Source    | Label | Training Signal         |\n",
    "| ---------------- | ----- | ----------------------- |\n",
    "| Real data $x$    | 1     | Encourage $D(x)$ → 1    |\n",
    "| Fake data $G(z)$ | 0     | Encourage $D(G(z))$ → 0 |\n",
    "\n",
    "So we **maximize**:\n",
    "\n",
    "$$\\log D(x) + \\log(1 - D(G(z)))$$\n",
    "\n",
    "Equivalently, we **minimize binary cross-entropy** with targets:\n",
    "\n",
    "* Real → label **1**\n",
    "* Fake → label **0**\n",
    "\n",
    "\n",
    "\n",
    "![The steps in building a GAN model](./figures/17_04.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**4. Updating the Generator (Problem and Solution)**\n",
    "\n",
    "When **$D$ is fixed**, the generator is updated using:\n",
    "\n",
    "$$\\min_G \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "But early in training:\n",
    "\n",
    "* $G(z)$ looks nothing like real data.\n",
    "* $D(G(z)) \\approx 0$ with high confidence.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\log(1 - D(G(z))) \\approx \\log 1 = 0$$\n",
    "\n",
    "and **its gradient is almost zero** → **no learning signal**.\n",
    "\n",
    "This is the **saturation problem**.\n",
    "\n",
    "\n",
    "\n",
    "**5. Solution: Replace the Loss (Non-Saturating Loss)**\n",
    "\n",
    "Instead of minimizing:\n",
    "\n",
    "$$\\log(1 - D(G(z)))$$\n",
    "\n",
    "we **maximize**:\n",
    "\n",
    "$$\\log D(G(z))$$\n",
    "\n",
    "which is equivalent to training $G$ as if the fake samples were labeled **real (label = 1)**.\n",
    "\n",
    "So the generator’s new loss is:\n",
    "\n",
    "$$\\min_G -\\mathbb{E}_{z \\sim p_z}\\left[\\log D(G(z))\\right]$$\n",
    "\n",
    "This provides a **strong gradient** even when $D(G(z))$ is small.\n",
    "\n",
    "\n",
    "\n",
    "**6. Final Label Assignment Summary**\n",
    "\n",
    "| Network           | Input              | Target Label | Why                          |\n",
    "| ----------------- | ------------------ | ------------ | ---------------------------- |\n",
    "| **Discriminator** | Real sample $x$    | 1            | Encourage real → real        |\n",
    "| **Discriminator** | Fake sample $G(z)$ | 0            | Encourage fake → fake        |\n",
    "| **Generator**     | Fake sample $G(z)$ | **1**        | Encourage fake → appear real |\n",
    "\n",
    "So the **generator’s labels are intentionally “flipped”** — this is called **non-saturating generator loss**.\n",
    "\n",
    "\n",
    "\n",
    "**7. Intuition Summary**\n",
    "\n",
    "* The **discriminator** learns to **identify fake vs. real**.\n",
    "* The **generator** learns to **fool the discriminator**, not to match real images directly.\n",
    "* In early training, $D$ is strong → gradients vanish → we **swap labels** for generator.\n",
    "* This gives the generator a **healthy gradient** to update from.\n",
    "\n",
    "\n",
    "\n",
    "**Where This Fits Into GAN Stabilization Theory**\n",
    "\n",
    "This non-saturating loss is foundational and is used in **almost all modern GANs**:\n",
    "\n",
    "* DCGAN\n",
    "* StyleGAN\n",
    "* BigGAN\n",
    "* CycleGAN\n",
    "\n",
    "It is part of making GANs **trainable and stable**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5241466",
   "metadata": {},
   "source": [
    "## **Implementing a GAN from scratch**\n",
    "\n",
    "- The original `GAN` paper by Goodfellow et al. (2014) introduced the concept of training two neural networks, a **generator** and a **discriminator**, in an adversarial manner to synthesize new data samples that resemble the training data distribution. \n",
    "\n",
    "\n",
    "### **Implementing the generator and the discriminator networks**\n",
    "\n",
    "- implementation of our first GAN model with a `generator` and a `discriminator` as two fully connected neural networks with one or more hidden layers.\n",
    "\n",
    "\n",
    "![GAN model with fully connected networks](./figures/17_08.png)\n",
    "\n",
    "\n",
    "- For each hidden layer, we use the `Leaky ReLU` activation function, which introduces non-linearity and helps the networks learn complex patterns in the data.\n",
    "  \n",
    "- The use of `ReLU` results in sparse gradients, which can lead to dead neurons during training. `Leaky ReLU` allows a small, non-zero gradient when the unit is not active, helping to keep the neurons alive and improving learning.\n",
    "\n",
    "- In the `discriminator network`, each hidden layer is followed by a `dropout` layer. This regularization technique randomly sets a fraction of the input units to zero during training, which helps prevent overfitting and improves the model's generalization to unseen data.\n",
    "\n",
    "- The `output layer` in the `generator` uses the hyperbolic tangent `Tanh` activation function to ensure that the generated samples are in the same range as the normalized input data (typically between `-1` and `1` for image data). This helps the generator produce outputs that are more realistic and consistent with the training data distribution.\n",
    "\n",
    "- The ouput layer in the discriminator uses the `sigmoid` activation function to output a probability score between `0` and `1`, indicating whether the input sample is `real` or `fake`. This probabilistic output is essential for the binary classification task that the discriminator performs.\n",
    "\n",
    "\n",
    "![Leaky ReLU activation function](./figures/17_17.png)\n",
    "\n",
    "- The `Leaky ReLU` activation function is defined as:\n",
    "  - $$f(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ \\alpha x & \\text{if } x < 0 \\end{cases}$$\n",
    "  - where `α` is a small constant (e.g., `0.01`) that determines the slope of the function for negative input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d63e27",
   "metadata": {},
   "source": [
    "### **Defining the training dataset**\n",
    "\n",
    "- For training our `GAN` model, we will use the `MNIST` dataset, which consists of grayscale images of handwritten digits (0-9). The dataset contains `60,000` training images and `10,000` test images, each with a resolution of `28x28` pixels.\n",
    "- We will normalize the pixel values to the range `[-1, 1]` to match the output range of the generator network, which uses the `Tanh` activation function in its output layer.\n",
    "- This normalization helps the generator produce outputs that are more realistic and consistent with the training data distribution.\n",
    "- Use `torchvision.transforms.ToTensor` to convert images to PyTorch tensors and `torchvision.transforms.Normalize` to scale pixel values to the desired range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f6ba4",
   "metadata": {},
   "source": [
    "**Refer to the code implementation in the notebook (chp17_part1_GPU.ipynb) for details on defining the dataset and data loader.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed93af",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
