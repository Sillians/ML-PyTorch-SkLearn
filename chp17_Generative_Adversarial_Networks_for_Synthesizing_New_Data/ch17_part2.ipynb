{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eaeb08",
   "metadata": {},
   "source": [
    "## **Generative Adversarial Networks for Synthesizing New Data (Part 2/2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5a8da",
   "metadata": {},
   "source": [
    "### **Improving the quality of synthesized images using a convolutional and Wasserstein GAN**\n",
    "\n",
    "- we will implement a `DCGAN`, which will enable us to improve the performance we saw in the previous GAN example. Additionally, we will briefly talk about an extra key technique, `Wasserstein GAN (WGAN)`.\n",
    "\n",
    "- This technique includes;\n",
    "  - Transposed convolution\n",
    "  - Batch normalization (BatchNorm)\n",
    "  - WGAN loss function\n",
    "\n",
    "- `DCGAN` stands for Deep Convolutional Generative Adversarial Network. It is a type of GAN that uses convolutional neural networks (CNNs) in both the generator and discriminator architectures. The key idea behind DCGANs is to leverage the power of CNNs to capture spatial hierarchies in image data, which helps in generating more realistic images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0992fef",
   "metadata": {},
   "source": [
    "#### **Transposed convolution**\n",
    "\n",
    "- `Transposed convolution`, also known as `deconvolution` or `fractionally strided convolution`, is a technique used in neural networks to upsample feature maps. It is commonly used in the generator network of GANs to increase the spatial dimensions of the generated images.\n",
    "\n",
    "- In a standard convolution operation, we apply a filter to an input feature map to produce a smaller output feature map. In contrast, transposed convolution works in the opposite direction: it takes a smaller input feature map and produces a larger output feature map by applying learned filters in a way that \"reverses\" the convolution process.\n",
    "\n",
    "- This operation allows the generator to create higher-resolution images from `lower-dimensional latent vectors`, enabling it to produce more detailed and realistic outputs.\n",
    "\n",
    "- Transposed convolution is particularly useful in `GANs` because it helps the generator learn to create images that closely resemble the training data by progressively refining and upsampling the generated images through multiple layers of transposed convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172296b",
   "metadata": {},
   "source": [
    "- To understand the `transposed convolution` operation, let’s go through a simple thought experiment. Assume that we have an input feature map of size `n×n`. Then, we apply a `2D convolution` operation with certain `padding` and `stride` parameters to this `n×n` input, resulting in an output feature map of size `m×m`. Now, the question is, how we can apply another convolution operation to obtain a feature map with the initial dimension `n×n` from this `m×m` output feature map while maintaining the connectivity patterns between the input and output? Note that only the shape of the `n×n` input matrix is recovered and not the actual matrix values.\n",
    "\n",
    "\n",
    "![Transposed Convolution Operation](./figures/17_09.png)\n",
    "\n",
    "\n",
    "- Upsampling feature maps using `transposed convolution` is a crucial technique in the generator network of GANs, as it allows the model to create high-resolution images that closely resemble the training data.\n",
    "\n",
    "- It works by inserting zeros between the elements of the input feature map, effectively increasing its spatial dimensions. Then, a standard convolution operation is applied to this expanded feature map using learned filters, which helps to refine and generate the final output image.\n",
    "\n",
    "\n",
    "\n",
    "![Applying transposed convolution to a 4×4 input](./figures/17_10.png)\n",
    "\n",
    "\n",
    "\n",
    "- In summary, `transposed convolution` is a powerful technique that enables the generator in GANs to produce high-quality images by progressively upsampling and refining feature maps through learned convolutional filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43639138",
   "metadata": {},
   "source": [
    "#### **Batch normalization**\n",
    "\n",
    "- `Batch normalization (BatchNorm)` is a technique used in deep learning to stabilize and accelerate the training of neural networks. It works by normalizing the inputs of each layer to have a mean of zero and a standard deviation of one, which helps to reduce internal covariate shift.\n",
    "\n",
    "- In the context of GANs, `BatchNorm` is commonly applied to both the generator and discriminator networks. By normalizing the activations within each mini-batch, `BatchNorm` helps to improve the convergence of the training process and allows for the use of higher learning rates.\n",
    "\n",
    "- `BatchNorm` also helps to mitigate issues such as mode collapse in GANs, where the generator produces limited diversity in its outputs. By stabilizing the training dynamics, `BatchNorm` encourages the generator to explore a wider range of outputs, leading to more diverse and realistic synthesized images.\n",
    "\n",
    "Assume that we have the net preactivation feature maps obtained after a convolutional layer in a four-dimensional tensor, $Z$, with the shape `[m×c×h×w]`, where `m` is the number of examples in the batch (i.e., batch size), `h×w` is the spatial dimension of the feature maps, and $c$ is the number of channels. `BatchNorm` can be summarized in three steps, as follows:\n",
    "\n",
    "1. **Compute the mean and variance** for each channel across the mini-batch and spatial dimensions:\n",
    "   $$\n",
    "   \\mu_c = \\frac{1}{m \\cdot h \\cdot w} \\sum_{i=1}^{m} \\sum_{j=1}^{h} \\sum_{k=1}^{w} Z_{i,c,j,k}\n",
    "   $$\n",
    "   \n",
    "   $$\n",
    "   \\sigma_c^2 = \\frac{1}{m \\cdot h \\cdot w} \\sum_{i=1}^{m} \\sum_{j=1}^{h} \\sum_{k=1}^{w} (Z_{i,c,j,k} - \\mu_c)^2\n",
    "   $$\n",
    "   where $Z_{i,c,j,k}$ represents the value of the feature map at the `i`-th example in the batch, `c`-th channel, and spatial location `(j, k)`.\n",
    "   \n",
    "   And, $\\mu_c$ and $\\sigma_c^2$ are the mean and variance for channel `c`, respectively.\n",
    "\n",
    "2. **Normalize the feature maps** using the computed mean and variance: \n",
    "   $$\n",
    "   \\hat{Z}_{i,c,j,k} = \\frac{Z_{i,c,j,k} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "   where $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "3. **Scale and shift** the normalized feature maps using learnable parameters $\\gamma_c$ and $\\beta_c$:\n",
    "   $$\n",
    "   Y_{i,c,j,k} = \\gamma_c \\hat{Z}_{i,c,j,k} + \\beta_c\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc1cd3",
   "metadata": {},
   "source": [
    "![Batch Normalization Process](./figures/17_11.png)\n",
    "\n",
    "- The PyTorch API provides a class, `nn.BatchNorm2d()` (`nn.BatchNorm1d()` for 1D input), that we can use as a layer when defining our models; it will perform all of the steps that we described for `BatchNorm`.\n",
    "- Note that the behavior for updating the learnable parameters, $\\gamma$ and $\\beta$ , depends on whether the model is a training model not. These parameters are learned only during training and are then used for normalization during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc86483",
   "metadata": {},
   "source": [
    "### **Implementing the generator and discriminator**\n",
    "\n",
    "The architectures of the generator and discriminator networks are summarized in the following two figures.\n",
    "\n",
    "\n",
    "- The generator takes a vector, $z$, of size `100` as input. Then, a series of `transposed convolutions` using `nn.ConvTranspose2d()` upsamples the feature maps until the spatial dimension of the resulting feature maps reaches `28×28`. The number of channels is reduced by half after each transposed convolutional\n",
    "layer, except the last one, which uses only one output filter to generate a grayscale image. Each transposed convolutional layer is followed by `BatchNorm` and `leaky ReLU` activation functions, except the last one, which uses `tanh activation` (without BatchNorm).\n",
    "\n",
    "\n",
    "**Architecture of the Generator**\n",
    "\n",
    "![Arch. of the generator](./figures/17_12.png)\n",
    "\n",
    "\n",
    "The discriminator receives images of size `1×28×28`, which are passed through four `convolutional layers`. The first three convolutional layers reduce the spatial dimensionality by `4` while increasing the number of channels of the feature maps. Each convolutional layer is also followed by `BatchNorm`\n",
    "and `leaky ReLU` activation. The last convolutional layer uses kernels of size `7×7` and a single filter to reduce the spatial dimensionality of the output to `1×1×1`. Finally, the convolutional output is followed by a `sigmoid function` and squeezed to one dimension:\n",
    "\n",
    "\n",
    "**Architecture of the Discriminator**\n",
    "\n",
    "![Arch. of the discriminator](./figures/17_13.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad59477",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bdd1a",
   "metadata": {},
   "source": [
    "### **Dissimilarity measures between two distributions**\n",
    "\n",
    "- The goal of generative model is to learn how to synthesize new samples that have the same distribution as the distribution of the training dataset.\n",
    "\n",
    "![methods to measure the dissimilarity between distributions P and Q](./figures/17_14.png)\n",
    "\n",
    "**Let's gain an understanding of these measures by briefly stating what they are trying to accomplish in simple words:**\n",
    "\n",
    "- The first one, `TV` distance, measures the largest difference between the two distributions at each point.\n",
    "- The `EM` distance can be interpreted as the minimal amount of work needed to transform one distribution into the other.\n",
    "- The `Kullback-Leibler (KL)` and `Jensen-Shannon (JS)` divergence measures come from the field of information theory. `KL` divergence is not symmetric, that is, $KL(P||Q) \\neq KL(Q||P)$ in contrast to JS divergence.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10706d5b",
   "metadata": {},
   "source": [
    "![Example of calculating the different dissimilarity measures](./figures/17_15.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c68f4e",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Earth Mover’s Distance (EMD)**\n",
    "\n",
    "**1. Problem Context: Why EMD Was Introduced**\n",
    "\n",
    "In classical GANs, the generator and discriminator are trained using a divergence measure between the **real data distribution** $`p_{\\mathrm{data}}`$ and the **generated distribution** $`p_{G}`$:\n",
    "\n",
    "* Jensen–Shannon Divergence (JSD)\n",
    "\n",
    "However:\n",
    "\n",
    "* When the supports of $`p_{\\mathrm{data}}`$ and $`p_{G}`$ **do not overlap** (as is common early in training),\n",
    "\n",
    "  * $`\\mathrm{JSD}(p_{\\mathrm{data}} \\,\\|\\, p_{G})`$ becomes **constant** → gradients become **zero**.\n",
    "\n",
    "This leads to *unstable training* and *vanishing gradients*.\n",
    "\n",
    "A different way was needed to measure “distance” between distributions **even when they do not overlap**.\n",
    "\n",
    "This leads us to the **Earth Mover’s Distance (EMD).**\n",
    "\n",
    "\n",
    "\n",
    "**2. Definition of Earth Mover’s Distance**\n",
    "\n",
    "Think of two distributions as piles of **dirt**.\n",
    "\n",
    "* $`p_{\\mathrm{data}}`$ is one pile.\n",
    "* $`p_{G}`$ is another.\n",
    "\n",
    "The **Earth Mover’s Distance** asks:\n",
    "\n",
    "> **What is the minimum amount of work needed to move one pile into the shape of the other?**\n",
    "\n",
    "Work = (amount of dirt) × (distance moved)\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\\mathrm{EMD}(p_{\\mathrm{data}}, p_{G})\n",
    "\\;\\;=\\;\\;\n",
    "\\inf_{\\gamma \\in \\Pi(p_{\\mathrm{data}}, p_{G})}\n",
    "\\mathbb{E}_{(x,y)\\sim\\gamma} \\left[\\,\\|x - y\\|\\,\\right]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $`\\gamma`$ is a **transport plan** describing how mass moves from $x$ to $y$\n",
    "* $`\\Pi(p_{\\mathrm{data}}, p_{G})`$ is the set of all valid transport plans.\n",
    "\n",
    "\n",
    "\n",
    "**3. Why EMD is Better for GAN Training**\n",
    "\n",
    "If two distributions **do not overlap**, EMD still gives a meaningful measure:\n",
    "\n",
    "* It provides a **smooth gradient**\n",
    "* It reflects **how far** generated samples must move to match the real ones\n",
    "* Gradient does **not vanish** even when distributions are disjoint\n",
    "\n",
    "This is crucial for learning.\n",
    "\n",
    "\n",
    "\n",
    "**4. From EMD to the WGAN Objective**\n",
    "\n",
    "Directly computing EMD is expensive.\n",
    "So WGAN uses a dual form known as the **Kantorovich–Rubinstein Duality**:\n",
    "\n",
    "$$\n",
    "\\mathrm{EMD}(p_{\\mathrm{data}}, p_G)\n",
    "=\n",
    "\n",
    "\\sup_{|f|*{L \\le 1}}\n",
    "\\mathbb{E}*{x\\sim p_{\\mathrm{data}}}[f(x)]\n",
    "-\n",
    "\n",
    "\\mathbb{E}_{x\\sim p_G}[f(x)]\n",
    "$$\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "* $`f`$ is restricted to be **1-Lipschitz**\n",
    "* In WGAN, $`f`$ is represented by a neural network (called the **critic**; not a discriminator)\n",
    "\n",
    "So the WGAN objective becomes:\n",
    "\n",
    "$$\n",
    "\\max_{w \\in \\mathrm{Lip}(1)}\n",
    "\\mathbb{E}*{x\\sim p*{\\mathrm{data}}}[D_w(x)]\n",
    "-\n",
    "\n",
    "\\mathbb{E}_{z\\sim p_z}[D_w(G(z))]\n",
    "$$\n",
    "\n",
    "And the **generator** minimizes:\n",
    "\n",
    "$`\\min_G \\mathbb{E}_{z\\sim p_z}[D_w(G(z))]`$\n",
    "\n",
    "Notice:\n",
    "\n",
    "* No log terms\n",
    "* No probabilities\n",
    "* No sigmoid activation\n",
    "* No binary cross-entropy\n",
    "\n",
    "The critic outputs a **real number**, not a probability.\n",
    "\n",
    "\n",
    "\n",
    "**5. Why the Lipschitz Constraint Matters**\n",
    "\n",
    "WGAN requires $`D_w`$ to be **1-Lipschitz**, meaning:\n",
    "\n",
    "$$|D_w(x_1) - D_w(x_2)| \\le \\|x_1 - x_2\\|$$\n",
    "\n",
    "This restriction ensures the output behaves like a distance function — enabling **stable gradients**.\n",
    "\n",
    "**How to enforce it:**\n",
    "\n",
    "| Method                 | Used In         | Notes                     |\n",
    "| ---------------------- | --------------- | ------------------------- |\n",
    "| Weight Clipping        | WGAN (original) | Simple but harms capacity |\n",
    "| Gradient Penalty       | WGAN-GP         | Standard and stable       |\n",
    "| Spectral Normalization | SN-GAN          | Common in image models    |\n",
    "\n",
    "\n",
    "\n",
    "**6. Key Conceptual Difference**\n",
    "\n",
    "| Model         | Discriminator Output    | Optimization Goal                          |\n",
    "| ------------- | ----------------------- | ------------------------------------------ |\n",
    "| Classical GAN | Probability (0 to 1)    | Classify real vs fake                      |\n",
    "| WGAN          | Real number (unbounded) | Measure *difference* between distributions |\n",
    "\n",
    "The WGAN critic does **not** try to classify.\n",
    "It tries to assign **higher scores** to real data than generated data.\n",
    "\n",
    "\n",
    "\n",
    "**7. Intuition Summary**\n",
    "\n",
    "* EMD measures how much *work* is needed to turn fake samples into real ones.\n",
    "* WGAN approximates EMD via the critic.\n",
    "* Because EMD has **smooth gradients**, WGAN avoids:\n",
    "\n",
    "  * Vanishing gradients\n",
    "  * Training collapse\n",
    "  * Oscillation instability\n",
    "\n",
    "This is why `WGAN` is significantly more stable than classical GANs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d16884",
   "metadata": {},
   "source": [
    "### **Using EM distance in practice for GANs**\n",
    "\n",
    "**Using Earth Mover’s Distance (EMD) in Practice for GANs\n",
    "→ The WGAN / WGAN-GP Training Procedure**\n",
    "\n",
    "**1. Key Reminder**\n",
    "\n",
    "Although the theoretical metric is *Earth Mover’s Distance*, **we do *not* compute EMD directly** in practice.\n",
    "\n",
    "Instead, we use the **Wasserstein GAN (WGAN)** formulation, which provides a **trainable approximation** of EMD using a *critic network* (not a discriminator).\n",
    "\n",
    "\n",
    "\n",
    "**2. Replace the Discriminator With a Critic**\n",
    "\n",
    "| Classical GAN                                    | WGAN                                                  |\n",
    "| ------------------------------------------------ | ----------------------------------------------------- |\n",
    "| Discriminator outputs probability `D(x) ∈ [0,1]` | Critic outputs a real score `C(x) ∈ ℝ`                |\n",
    "| Uses BCE loss                                    | Uses Wasserstein loss                                 |\n",
    "| Classifies real vs fake                          | Computes distance between real and fake distributions |\n",
    "\n",
    "No sigmoid activation is used in the critic’s final layer.\n",
    "\n",
    "\n",
    "\n",
    "**3. Wasserstein Loss Functions (Practical Training Objective)**\n",
    "\n",
    "**Critic loss (maximize difference)**\n",
    "\n",
    "$$L_C = -\\mathbb{E}_{x \\sim p_{\\mathrm{data}}}[C(x)] + \\mathbb{E}_{z \\sim p_z}[C(G(z))]$$\n",
    "\n",
    "**Generator loss (minimize critic score for fake samples)**\n",
    "\n",
    "$$L_G = -\\mathbb{E}_{z \\sim p_{z}}[C(G(z))]$$\n",
    "\n",
    "These work as *minimization* losses in code:\n",
    "\n",
    "```python\n",
    "loss_critic = -(torch.mean(C_real) - torch.mean(C_fake))\n",
    "loss_generator = -torch.mean(C_fake)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**4. Enforcing the 1-Lipschitz Constraint**\n",
    "\n",
    "**This is the critical practical step.**\n",
    "The critic **must** satisfy the Lipschitz condition:\n",
    "\n",
    "$$|C(x_1)-C(x_2)| \\le \\|x_1-x_2\\|$$\n",
    "\n",
    "There are 3 standard enforcement mechanisms:\n",
    "\n",
    "| Method                 | Name          | Stability         | Most Common Today      |\n",
    "| ---------------------- | ------------- | ----------------- | ---------------------- |\n",
    "| Weight Clipping        | Original WGAN | Unstable          | ❌ Not recommended      |\n",
    "| Gradient Penalty       | **WGAN-GP**   | Very stable       | ✅ Standard approach    |\n",
    "| Spectral Normalization | SN-GAN        | Stable, efficient | ✅ Often used in images |\n",
    "\n",
    "\n",
    "\n",
    "**5. WGAN-GP Gradient Penalty Term**\n",
    "\n",
    "Add the penalty:\n",
    "\n",
    "$$\\lambda \\cdot \\mathbb{E}_{\\hat{x}\\sim p_{\\hat{x}}}\n",
    "\\left[\n",
    "(\\lVert \\nabla_{\\hat{x}} C(\\hat{x}) \\rVert_2 - 1)^2\n",
    "\\right]$$\n",
    "\n",
    "where $`\\hat{x}`$ is an interpolation between real and fake samples.\n",
    "\n",
    "In code:\n",
    "\n",
    "```python\n",
    "epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "x_hat = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "x_hat.requires_grad_(True)\n",
    "\n",
    "critic_scores = critic(x_hat)\n",
    "gradients = torch.autograd.grad(\n",
    "    outputs=critic_scores,\n",
    "    inputs=x_hat,\n",
    "    grad_outputs=torch.ones_like(critic_scores),\n",
    "    create_graph=True,\n",
    "    retain_graph=True,\n",
    "    only_inputs=True,\n",
    ")[0]\n",
    "\n",
    "grad_norm = gradients.view(gradients.size(0), -1).norm(2, dim=1)\n",
    "gradient_penalty = ((grad_norm - 1) ** 2).mean()\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "loss_critic = (torch.mean(C_fake) - torch.mean(C_real)) + λ * gradient_penalty\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**6. Training Loop Structure**\n",
    "\n",
    "WGAN trains the critic **more often** than the generator:\n",
    "\n",
    "```python\n",
    "for step in training_steps:\n",
    "    for _ in range(n_critic):  # e.g., 5\n",
    "        update critic (minimize loss_critic)\n",
    "    update generator (minimize loss_generator)\n",
    "```\n",
    "\n",
    "Typical values:\n",
    "\n",
    "* `n_critic = 5`\n",
    "* `λ = 10` (gradient penalty weight)\n",
    "\n",
    "\n",
    "\n",
    "**7. Practical Signs of Correct Training**\n",
    "\n",
    "| Symptom                                | Meaning                             | Fix                         |\n",
    "| -------------------------------------- | ----------------------------------- | --------------------------- |\n",
    "| Critic loss → large negative values    | Critic winning too strongly         | Increase generator updates  |\n",
    "| Generator loss → large negative values | Generator overpowering critic       | Increase critic updates     |\n",
    "| Critic gradient norms explode          | Lipschitz violation                 | Increase gradient penalty λ |\n",
    "| Mode collapse (repeated outputs)       | Generator ignoring critic gradients | Increase n_critic           |\n",
    "\n",
    "\n",
    "\n",
    "**8. Summary**\n",
    "\n",
    "| Classical GAN                  | WGAN / WGAN-GP                        |\n",
    "| ------------------------------ | ------------------------------------- |\n",
    "| Uses Jensen-Shannon            | Uses Earth Mover (Wasserstein-1)      |\n",
    "| Probabilities & BCE loss       | Real-valued critic & Wasserstein loss |\n",
    "| Can suffer vanishing gradients | Stable gradients throughout           |\n",
    "| Often unstable training        | Much more robust optimization         |\n",
    "\n",
    "**In practice:**\n",
    "\n",
    "* Use **WGAN-GP**\n",
    "* Use **gradient penalty**\n",
    "* Train **critic multiple times per generator step**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90db317",
   "metadata": {},
   "source": [
    "### **Gradient Penalty (WGAN-GP)**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "In the Wasserstein GAN (WGAN) framework, the discriminator is replaced by a **critic** that estimates the Wasserstein-1 distance between the real and generated data distributions. For this distance to be computed correctly, the critic must satisfy the **1-Lipschitz constraint**, meaning that its gradients with respect to inputs must be bounded by:\n",
    "\n",
    "$$||\\nabla_x f(x)|| \\le 1$$\n",
    "\n",
    "Originally, WGAN enforced this using **weight clipping**, but this caused optimization issues (capacity loss, gradient explosion, and training instability).\n",
    "Thus, **WGAN-GP** introduced **Gradient Penalty**, which enforces the Lipschitz constraint *smoothly*.\n",
    "\n",
    "\n",
    "\n",
    "**Core Idea**\n",
    "\n",
    "During training, the critic receives real samples $`x_{\\mathrm{real}}`$ and generated samples $`x_{\\mathrm{fake}}`$.\n",
    "A **linear interpolation** is computed:\n",
    "\n",
    "$`\\hat{x} = \\epsilon x_{\\mathrm{real}} + (1 - \\epsilon) x_{\\mathrm{fake}}, \\quad \\epsilon \\sim \\mathrm{Uniform}(0, 1)`$\n",
    "\n",
    "The gradient of the critic w.r.t. this interpolated sample is computed:\n",
    "\n",
    "$$g = \\nabla_{\\hat{x}} f(\\hat{x})$$\n",
    "\n",
    "The **gradient norm** is:\n",
    "\n",
    "$$\\|g\\|_2 = \\sqrt{\\sum_i g_i^2}$$\n",
    "\n",
    "The gradient penalty term is then:\n",
    "\n",
    "$$\\lambda (\\|g\\|_2 - 1)^2$$\n",
    "\n",
    "This term is added to the critic loss.\n",
    "The full WGAN-GP critic loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\mathrm{critic}} = \\mathbb{E}[f(x_{\\mathrm{fake}})] - \\mathbb{E}[f(x_{\\mathrm{real}})] + \\lambda \\, \\mathbb{E}_{\\hat{x}}[(\\|\\nabla_{\\hat{x}} f(\\hat{x})\\|_2 - 1)^2]$$\n",
    "\n",
    "\n",
    "\n",
    "**Why Interpolate Between Real and Fake?**\n",
    "\n",
    "The optimal critic under Wasserstein distance is **1-Lipschitz almost everywhere**, and the Lipschitz constraint is tightest on the **shortest transport path** between real and generated distributions.\n",
    "Interpolating forces the constraint **where the critic must be most accurate**.\n",
    "\n",
    "\n",
    "\n",
    "**Role of the Coefficient $`\\lambda`$**\n",
    "\n",
    "* $`\\lambda`$ controls the strength of the Lipschitz penalty.\n",
    "* Typical value: $`\\lambda = 10`$\n",
    "* Too small → Lipschitz condition violated → unstable training.\n",
    "* Too large → critic becomes too smooth → weaker gradients → slow generator updates.\n",
    "\n",
    "\n",
    "\n",
    "**Why Gradient Penalty Works Better than Weight Clipping**\n",
    "\n",
    "| Method           | Effect                                                                  |\n",
    "| ---------------- | ----------------------------------------------------------------------- |\n",
    "| Weight Clipping  | Forces critic weights into a narrow range → underfits → poor gradients. |\n",
    "| Gradient Penalty | Allows full network capacity while **gently enforcing** Lipschitzness.  |\n",
    "\n",
    "Thus, **WGAN-GP** produces **more stable training**, **better gradients**, and **higher-quality samples**.\n",
    "\n",
    "\n",
    "\n",
    "**Key Intuition**\n",
    "\n",
    "The gradient penalty does **not** force every gradient to be exactly 1.\n",
    "Instead, it **encourages** the critic to behave like a **smooth, well-conditioned metric function**, rather than a sharp classifier.\n",
    "This is exactly what is required to measure Wasserstein distance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318596e",
   "metadata": {},
   "source": [
    "##### **Mode collapse**\n",
    "\n",
    "Due to the adversarial nature of GAN models, it is notoriously hard to train them. One common cause of failure in training GANs is when the generator gets stuck in a small subspace and learns to generate similar samples. This is called `mode collapse`.\n",
    "\n",
    "- The synthesized examples in this figure are not cherry-picked. This shows that the generator has failed to learn the entire data distribution, and instead, has taken a lazy approach focusing on a subspace:\n",
    "\n",
    "\n",
    "![Example of mode collapse](./figures/17_16.png)\n",
    "\n",
    "\n",
    "Besides the vanishing and exploding gradient problems that we saw previously, there are some further aspects that can also make training GAN models difficult (indeed, it is an art). Here are a few suggested tricks from GAN artists.\n",
    "\n",
    "- One approach is called `mini-batch discrimination`, which is based on the fact that batches consisting of only real or fake examples are fed separately to the discriminator. In mini-batch discrimination, we let the discriminator compare examples across these batches to see whether a batch is real or fake. The diversity of a batch consisting of only real examples is most likely higher than the diversity of a fake batch if a model suffers from mode collapse.\n",
    "\n",
    "- Another technique that is commonly used for stabilizing GAN training is `feature matching`. In feature matching, we make a slight modification to the objective function of the generator by adding an extra term that minimizes the difference between the original and synthesized images based on intermediate representations (feature maps) of the discriminator.\n",
    "\n",
    "\n",
    "- During the training, a GAN model can also get stuck in several modes and just hop between them. To avoid this behavior, you can store some old examples and feed them to the discriminator to prevent the generator from revisiting previous modes. This technique is referred to as experience replay. Furthermore, you can train multiple GANs with different random seeds so that the combination of all of them covers a larger part of the data distribution than any single one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c8baa",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
