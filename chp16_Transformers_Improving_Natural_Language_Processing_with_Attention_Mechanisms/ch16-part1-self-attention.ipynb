{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ad5d78",
   "metadata": {},
   "source": [
    "# **Transformers – Improving Natural Language Processing with Attention Mechanisms (Part 1/3)**\n",
    "\n",
    "Transformers are a type of deep learning model that have revolutionized the field of natural language processing (NLP) by introducing attention mechanisms. Unlike traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), transformers do not rely on sequential data processing, allowing for greater parallelization and efficiency.\n",
    "\n",
    "This section covers the following topics:\n",
    "- Improving RNNs with Attention Mechanisms\n",
    "- Introducing the stand-alone attention mechanism\n",
    "- Understanding the original transformer architecture\n",
    "- Comparing transformer-based scale language models\n",
    "- Fine-tuning BERT for sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623501f",
   "metadata": {},
   "source": [
    "## **Adding an attention mechanism to RNNs**\n",
    "\n",
    "Attention mechanisms allow models to focus on specific parts of the input sequence when making predictions, rather than treating all parts equally. This is particularly useful in NLP tasks where certain words or phrases may carry more significance than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecca65",
   "metadata": {},
   "source": [
    "### **Attention helps RNNs with accessing information**\n",
    "\n",
    "To understand the development of an attention mechanism, let's first consider the limitations of RNNs. RNNs process sequences of data one element at a time, maintaining a hidden state that captures information about previous elements. However, as the sequence length increases, it becomes challenging for the RNN to retain relevant information from earlier in the sequence due to issues like vanishing gradients.\n",
    "\n",
    "![A traditional RNN encoder-decoder architecture for a seq2seq modeling task](./figures/16_01.png)\n",
    "\n",
    "\n",
    "Why is the RNN parsing the entire input sequence into a single fixed-length vector? This design choice can lead to information bottlenecks, especially for long sequences, as the model may struggle to retain all relevant information in a single vector.\n",
    "\n",
    "\n",
    "![word by word transalation can lead to grammatical errors](./figures/16_02.png)\n",
    "\n",
    "\n",
    "RNN encoder-decoder architectures can struggle with long sequences, as they must compress all input information into a single fixed-length vector. This can lead to loss of important context, resulting in errors such as incorrect grammar in translations.\n",
    "\n",
    "In contrast to a regualr RNN encoder-decoder architecture, an attention mechanism allows the decoder to access all hidden states of the encoder directly. This means that at each step of the decoding process, the model can \"attend\" to different parts of the input sequence, effectively allowing it to focus on the most relevant information for generating the next output token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb3a59",
   "metadata": {},
   "source": [
    "### **The original attention mechanism for RNNs**\n",
    "\n",
    "The attention mechanism introduced by Bahdanau et al. (2015) computes a context vector for each output time step by taking a weighted sum of all encoder hidden states. The weights, known as attention scores, are calculated based on the relevance of each encoder hidden state to the current decoder hidden state.\n",
    "\n",
    "Given an input sequence $x = (x_1, x_2, \\ldots, x_T)$, the encoder processes this sequence and produces a set of hidden states $h = (h_1, h_2, \\ldots, h_T)$. The decoder then generates the output sequence $y = (y_1, y_2, \\ldots, y_{T'})$. The attention mechanism assigns a weight to each element of the input sequence and helps the model identify which parts of the input are most relevant for generating each part of the output. For example, suppose our input is a sentence, and a word with a larger weight contributes more to our understanding of the whole sentence.\n",
    "\n",
    "![RNN with attention mechanism](./figures/16_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f52b3",
   "metadata": {},
   "source": [
    "#### Processing the inputs using a bidirectional RNN\n",
    "\n",
    "A bidirectional RNN processes the input sequence in both forward and backward directions, allowing the model to capture context from both past and future tokens. This is particularly useful for NLP tasks where understanding the full context of a word is important. \n",
    "\n",
    "#### Generating outputs from context vectors\n",
    "\n",
    "At each decoding step, the decoder computes a context vector as a weighted sum of the encoder's hidden states. The weights are determined by the attention scores, which indicate the relevance of each encoder hidden state to the current decoder state. The context vector is then combined with the decoder's hidden state to generate the output token.\n",
    "\n",
    "#### Computing the attention weights\n",
    "\n",
    "The attention weights are computed using a scoring function that measures the similarity between the decoder's current hidden state and each of the encoder's hidden states. Common scoring functions include dot product, scaled dot product, and additive attention. The scores are then normalized using a softmax function to produce the attention weights, which sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf40c2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390cad3",
   "metadata": {},
   "source": [
    "## **Introducing the self-attention mechanism**\n",
    "\n",
    "`Self-attention`, also known as `intra-attention`, is a mechanism that allows a model to weigh the importance of different parts of a single sequence when encoding it. Unlike traditional attention mechanisms that operate between two different sequences (e.g., encoder and decoder), self-attention focuses on the relationships within the same sequence.\n",
    "\n",
    "`Self-attention` works by computing a set of attention scores for each token in the sequence with respect to all other tokens. This allows the model to capture dependencies and relationships between words, regardless of their distance from each other in the sequence. It focuses only on the input sequence itself, enabling the model to understand the context and relationships between words more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db521bec",
   "metadata": {},
   "source": [
    "**Starting with a basic form of self-attention**\n",
    "\n",
    "Consider an input sequence represented as a matrix $( X )$ of shape $( (T, d_{model}) )$, where $( T )$ is the sequence length and $( d_{model} )$ is the dimensionality of the input embeddings. The self-attention mechanism computes three matrices: queries $( Q )$, keys $( K )$, and values $( V )$ by multiplying the input matrix $( X )$ with learned weight matrices $( W_{Q} )$, $( W_{K} )$, and $( W_{V} )$:\n",
    "\n",
    "$$Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V$$\n",
    "\n",
    "The attention scores are computed by taking the dot product of the queries and keys, followed by scaling and applying a softmax function to obtain the attention weights:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "where $( d_{k} )$ is the dimensionality of the keys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3411d",
   "metadata": {},
   "source": [
    "#### Starting with a basic form of self-attention\n",
    "\n",
    "\n",
    "To introduce self-attention, let's assume we have an input sequence of length $T$, $x = (x^{(1)}, x^{(2)}, \\ldots, x^{(T)})$, where each $x^{(i)}$ is a word embedding vector of dimension $d$. The self-attention mechanism computes a new representation for each input element by attending to all other elements in the sequence. And, the output of the self-attention mechanism is a set of context vectors $z = (z^{(1)}, z^{(2)}, \\ldots, z^{(T)})$, where each context vector $z^{(i)}$ is computed as a weighted sum of all input elements:\n",
    "\n",
    "$$z^{(i)} = \\sum_{j=1}^{T} \\alpha_{ij} x^{(j)}$$\n",
    "\n",
    "Here, $\\alpha_{ij}$ represents the attention weight from the $i$th input element to the $j$th input element, indicating how much attention the model should pay to $x^{(j)}$ when computing $z^{(i)}$.\n",
    "\n",
    "For a seq2seq modeling task, the self-attention mechanism allows each word in the input sequence to attend to all other words, enabling the model to capture dependencies and relationships between words, regardless of their distance from each other in the sequence.\n",
    "\n",
    "self-attention mechanism can be broken down into the following steps:\n",
    "\n",
    "- 1. **Derive the attention weights $\\alpha_{ij}$ for each pair of input elements.**\n",
    "\n",
    "To compute the attention weights $\\alpha_{ij}$, we first calculate similarity scores $w_{ij}$ between each pair of input elements using dot products:\n",
    "\n",
    "$$w_{ij} = x^{(i)^T} x^{(j)}$$\n",
    "\n",
    "- 2. **We normalize the weights using the softmax function.**\n",
    "\n",
    "These scores are then normalized using the softmax function to obtain the attention weights:\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp(w_{ij})}{\\sum_{k=1}^{T} \\exp(w_{ik})}$$\n",
    "\n",
    "The `attention_weights` matrix has shape `(T, T)`.\n",
    "\n",
    "- 3. **Compute the context vectors $z^{(i)}$ as weighted sums of the input elements.**\n",
    "\n",
    "Finally, we compute the context vector $z^{(i)}$ for each input element as a weighted sum of all input elements using the attention weights:\n",
    "\n",
    "$$z^{(i)} = \\sum_{j=1}^{T} \\alpha_{ij} x^{(j)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a58744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 7, 1, 2, 5, 6, 4, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# input sequence / sentence:\n",
    "#  \"Can you help me to translate this sentence\"\n",
    "\n",
    "sentence = torch.tensor(\n",
    "    [0, # can\n",
    "     7, # you     \n",
    "     1, # help\n",
    "     2, # me\n",
    "     5, # to\n",
    "     6, # translate\n",
    "     4, # this\n",
    "     3] # sentence\n",
    ")\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff5639",
   "metadata": {},
   "source": [
    "- Next, assume we have an embedding of the words, i.e., the words are represented as real vectors.\n",
    "\n",
    "- Here, our embedding size is `16`, and we assume that the dictionary size is `10`.\n",
    "\n",
    "- Since we have `8` words, there will be `8` vectors. Each vector is 16-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20232b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(10, 16)\n",
    "embedded_sentence = embed(sentence).detach()\n",
    "embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9539ccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
       "         0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7abd3529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentence.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56085f",
   "metadata": {},
   "source": [
    "- The goal is to compute the context vectors $`z^{(i)} = \\sum_{j = 1}^{T} \\alpha_{ij}x^{(j)}`$ , which involve attention weights $`\\alpha_{ij}`$.\n",
    "\n",
    "- In turn, the attention weights $`\\alpha_{ij}`$ involve the $`w_{ij}`$ values.\n",
    "\n",
    "- Let's start with the $`w_{ij}`$'s first, which are computed as dot-products:\n",
    "\n",
    "$$w_{ij} = (x^{(i)})^T x^{(j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2782eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 embedding: tensor([ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "         0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692])\n",
      "  Dot product with word 0: 9.760122299194336\n",
      "  Dot product with word 1: 1.7326233386993408\n",
      "  Dot product with word 2: 4.75434684753418\n",
      "  Dot product with word 3: -1.3586798906326294\n",
      "  Dot product with word 4: 0.47519540786743164\n",
      "  Dot product with word 5: -1.6716841459274292\n",
      "  Dot product with word 6: 1.0226718187332153\n",
      "  Dot product with word 7: -0.12858974933624268\n",
      "Word 1 embedding: tensor([-9.4053e-01, -4.6806e-01,  1.0322e+00, -2.8300e-01,  4.9275e-01,\n",
      "        -1.4078e-02, -2.7466e-01, -7.6409e-01,  1.3966e+00, -9.9491e-01,\n",
      "        -1.5822e-03,  1.2471e+00, -7.7105e-02,  1.2774e+00, -1.4596e+00,\n",
      "        -2.1595e+00])\n",
      "  Dot product with word 0: 1.7326233386993408\n",
      "  Dot product with word 1: 16.07872772216797\n",
      "  Dot product with word 2: 9.064151763916016\n",
      "  Dot product with word 3: -0.3370445966720581\n",
      "  Dot product with word 4: 1.1368274688720703\n",
      "  Dot product with word 5: 1.1972055435180664\n",
      "  Dot product with word 6: 1.6485464572906494\n",
      "  Dot product with word 7: -1.2788965702056885\n",
      "Word 2 embedding: tensor([-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "         0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400])\n",
      "  Dot product with word 0: 4.75434684753418\n",
      "  Dot product with word 1: 9.064151763916016\n",
      "  Dot product with word 2: 22.661468505859375\n",
      "  Dot product with word 3: -0.8519423604011536\n",
      "  Dot product with word 4: 7.77992582321167\n",
      "  Dot product with word 5: 2.748345375061035\n",
      "  Dot product with word 6: -0.6831648349761963\n",
      "  Dot product with word 7: 1.6236070394515991\n",
      "Word 3 embedding: tensor([-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "         0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850])\n",
      "  Dot product with word 0: -1.3586798906326294\n",
      "  Dot product with word 1: -0.3370445966720581\n",
      "  Dot product with word 2: -0.8519423604011536\n",
      "  Dot product with word 3: 13.947339057922363\n",
      "  Dot product with word 4: -1.4198040962219238\n",
      "  Dot product with word 5: 10.965932846069336\n",
      "  Dot product with word 6: -0.5887424945831299\n",
      "  Dot product with word 7: 2.3869481086730957\n",
      "Word 4 embedding: tensor([ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "        -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729])\n",
      "  Dot product with word 0: 0.47519540786743164\n",
      "  Dot product with word 1: 1.1368274688720703\n",
      "  Dot product with word 2: 7.77992582321167\n",
      "  Dot product with word 3: -1.4198040962219238\n",
      "  Dot product with word 4: 13.751073837280273\n",
      "  Dot product with word 5: -6.8567891120910645\n",
      "  Dot product with word 6: -2.5113699436187744\n",
      "  Dot product with word 7: -3.3468308448791504\n",
      "Word 5 embedding: tensor([-2.2150, -1.3193, -2.0915,  0.9629, -0.0319, -0.4790,  0.7668,  0.0275,\n",
      "         1.9929,  1.3708, -0.5009, -0.2793, -2.0628,  0.0064, -0.9896,  0.7016])\n",
      "  Dot product with word 0: -1.6716841459274292\n",
      "  Dot product with word 1: 1.1972055435180664\n",
      "  Dot product with word 2: 2.748345375061035\n",
      "  Dot product with word 3: 10.965932846069336\n",
      "  Dot product with word 4: -6.8567891120910645\n",
      "  Dot product with word 5: 24.673830032348633\n",
      "  Dot product with word 6: -3.829355001449585\n",
      "  Dot product with word 7: 4.958143711090088\n",
      "Word 6 embedding: tensor([ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "         0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465])\n",
      "  Dot product with word 0: 1.0226718187332153\n",
      "  Dot product with word 1: 1.6485464572906494\n",
      "  Dot product with word 2: -0.6831648349761963\n",
      "  Dot product with word 3: -0.5887424945831299\n",
      "  Dot product with word 4: -2.5113699436187744\n",
      "  Dot product with word 5: -3.829355001449585\n",
      "  Dot product with word 6: 15.869112014770508\n",
      "  Dot product with word 7: 2.0269148349761963\n",
      "Word 7 embedding: tensor([ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "         2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293])\n",
      "  Dot product with word 0: -0.12858974933624268\n",
      "  Dot product with word 1: -1.2788965702056885\n",
      "  Dot product with word 2: 1.6236070394515991\n",
      "  Dot product with word 3: 2.3869481086730957\n",
      "  Dot product with word 4: -3.3468308448791504\n",
      "  Dot product with word 5: 4.958143711090088\n",
      "  Dot product with word 6: 2.0269148349761963\n",
      "  Dot product with word 7: 18.738170623779297\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(embedded_sentence):\n",
    "    print(f\"Word {i} embedding: {x_i}\")\n",
    "    for j, x_j in enumerate(embedded_sentence):\n",
    "        print(f\"  Dot product with word {j}: {torch.dot(x_i, x_j)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe054d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = torch.empty(8, 8)\n",
    "\n",
    "for i, x_i in enumerate(embedded_sentence):\n",
    "    for j, x_j in enumerate(embedded_sentence):\n",
    "        omega[i, j] = torch.dot(x_i, x_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c6e38e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.7601,  1.7326,  4.7543, -1.3587,  0.4752, -1.6717,  1.0227, -0.1286],\n",
       "        [ 1.7326, 16.0787,  9.0642, -0.3370,  1.1368,  1.1972,  1.6485, -1.2789],\n",
       "        [ 4.7543,  9.0642, 22.6615, -0.8519,  7.7799,  2.7483, -0.6832,  1.6236],\n",
       "        [-1.3587, -0.3370, -0.8519, 13.9473, -1.4198, 10.9659, -0.5887,  2.3869],\n",
       "        [ 0.4752,  1.1368,  7.7799, -1.4198, 13.7511, -6.8568, -2.5114, -3.3468],\n",
       "        [-1.6717,  1.1972,  2.7483, 10.9659, -6.8568, 24.6738, -3.8294,  4.9581],\n",
       "        [ 1.0227,  1.6485, -0.6832, -0.5887, -2.5114, -3.8294, 15.8691,  2.0269],\n",
       "        [-0.1286, -1.2789,  1.6236,  2.3869, -3.3468,  4.9581,  2.0269, 18.7382]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eea9b75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb08cd",
   "metadata": {},
   "source": [
    "- Actually, let's compute this more efficiently by replacing the nested for-loops with a matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39521965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.7601,  1.7326,  4.7543, -1.3587,  0.4752, -1.6717,  1.0227, -0.1286],\n",
       "        [ 1.7326, 16.0787,  9.0642, -0.3370,  1.1368,  1.1972,  1.6485, -1.2789],\n",
       "        [ 4.7543,  9.0642, 22.6615, -0.8519,  7.7799,  2.7483, -0.6832,  1.6236],\n",
       "        [-1.3587, -0.3370, -0.8519, 13.9473, -1.4198, 10.9659, -0.5887,  2.3869],\n",
       "        [ 0.4752,  1.1368,  7.7799, -1.4198, 13.7511, -6.8568, -2.5114, -3.3468],\n",
       "        [-1.6717,  1.1972,  2.7483, 10.9659, -6.8568, 24.6738, -3.8294,  4.9581],\n",
       "        [ 1.0227,  1.6485, -0.6832, -0.5887, -2.5114, -3.8294, 15.8691,  2.0269],\n",
       "        [-0.1286, -1.2789,  1.6236,  2.3869, -3.3468,  4.9581,  2.0269, 18.7382]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_mat = torch.matmul(embedded_sentence, embedded_sentence.T)\n",
    "omega_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f9ec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07eb76d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(omega, omega_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a43190",
   "metadata": {},
   "source": [
    "- Next, let's compute the attention weights by normalizing the \"omega\" values so they sum to 1\n",
    "\n",
    "$$\\alpha{ij} = \\frac {exp(w_{ij})}{\\sum_{j = 1}^{T}exp{(w_{ij})}} = softmax([w_{ij}]_{j=1...T})$$\n",
    "\n",
    "\n",
    "- Hence, due to applying this `softmax` function, the weights will sum to `1` after this normalization, that is,\n",
    "\n",
    "$$\\sum_{j=1}^{T} \\alpha_{ij} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57adf89c",
   "metadata": {},
   "source": [
    "- We can compute the attention weights using PyTorch’s softmax function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e24d7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights = F.softmax(omega, dim=1)\n",
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1770932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9270e-01, 3.2398e-04, 6.6502e-03, 1.4723e-05, 9.2135e-05, 1.0766e-05,\n",
       "         1.5929e-04, 5.0374e-05],\n",
       "        [5.8773e-07, 9.9910e-01, 8.9788e-04, 7.4187e-08, 3.2391e-07, 3.4407e-07,\n",
       "         5.4033e-07, 2.8926e-08],\n",
       "        [1.6712e-08, 1.2438e-06, 1.0000e+00, 6.1412e-11, 3.4437e-07, 2.2482e-09,\n",
       "         7.2703e-11, 7.3008e-10],\n",
       "        [2.1438e-07, 5.9550e-07, 3.5585e-07, 9.5172e-01, 2.0167e-07, 4.8272e-02,\n",
       "         4.6299e-07, 9.0760e-06],\n",
       "        [1.7110e-06, 3.3158e-06, 2.5448e-03, 2.5720e-07, 9.9745e-01, 1.1195e-09,\n",
       "         8.6338e-08, 3.7443e-08],\n",
       "        [3.6165e-12, 6.3713e-11, 3.0053e-10, 1.1136e-06, 2.0250e-14, 1.0000e+00,\n",
       "         4.1805e-13, 2.7390e-09],\n",
       "        [3.5667e-07, 6.6694e-07, 6.4779e-08, 7.1194e-08, 1.0410e-08, 2.7865e-09,\n",
       "         1.0000e+00, 9.7366e-07],\n",
       "        [6.4013e-09, 2.0263e-09, 3.6918e-08, 7.9205e-08, 2.5622e-10, 1.0361e-06,\n",
       "         5.5258e-08, 1.0000e+00]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf00c1",
   "metadata": {},
   "source": [
    "- The `attention_weights` matrix has shape `(T, T)`.\n",
    "\n",
    "- The `attention_weights` is an `8 x 8` matrix, where each entry $\\alpha_{ij}$ represents the attention weight from the `i`th word to the `j`th word. \n",
    "\n",
    "- These attention weights indicate how relevant each word is to the `ith` word. \n",
    "\n",
    "- Hence, the columns in this attention matrix should sum to 1, which we can confirm via the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26957ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(dim=1)  # each row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959e84a",
   "metadata": {},
   "source": [
    "- let us recap and summarize the three main steps behind the self-attention operation:\n",
    "\n",
    "1. For a given input element $`x^{(i)}`$, compute the similarity scores $`w_{ij}`$ with all other input elements $`x^{(j)}`$ using dot products. $`x^{(i)^T}x^{(j)}`$.\n",
    "2. Normalize these scores using the softmax function to obtain attention weights $`\\alpha_{ij}`$.\n",
    "3. Compute the context vector $`z^{(i)}`$ as a weighted sum of the input elements using the attention weights: $`z^{(i)} = \\sum_{j=1}^{T} \\alpha_{ij} x^{(j)}`$.\n",
    "\n",
    "\n",
    "![A basic Self-attention mechanism](./figures/16_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73306ce2",
   "metadata": {},
   "source": [
    "- Now that we have the attention weights, we can compute the context vectors $`z^{(i)} = \\sum_{j=1}^{T} \\alpha_{ij} x^{(j)}`$, which involve attention weights $`\\alpha_{ij}`$. \n",
    "\n",
    "- For instance, to compute the context-vector of the 2nd input element (the element at index 1), we can perform the following computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80532d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.3975e-01, -4.6856e-01,  1.0311e+00, -2.8192e-01,  4.9373e-01,\n",
       "        -1.2896e-02, -2.7327e-01, -7.6358e-01,  1.3958e+00, -9.9543e-01,\n",
       "        -7.1287e-04,  1.2449e+00, -7.8077e-02,  1.2765e+00, -1.4589e+00,\n",
       "        -2.1601e+00])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1, :]\n",
    "context_vec_2 = torch.zeros(x_2.shape)\n",
    "\n",
    "for j in range(8):\n",
    "    x_j = embedded_sentence[j, :]\n",
    "    context_vec_2 += attention_weights[1, j] * x_j\n",
    "    \n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2871105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]), torch.Size([16]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2.shape, context_vec_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb154c",
   "metadata": {},
   "source": [
    "- Again, we can achieve this more efficiently by using matrix multiplication. Using the following code, we are computing the context vectors for all eight input words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "871fcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vectors = torch.matmul(\n",
    "        attention_weights, embedded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a53ecfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(context_vec_2, context_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ce5c961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.3420e-01, -1.8324e-01, -3.0218e-01, -5.7772e-01,  3.5662e-01,\n",
       "           6.6452e-01, -2.0998e-01, -3.7798e-01,  7.6537e-01, -1.1946e+00,\n",
       "           6.9960e-01, -1.4067e+00,  1.7021e-01,  1.8838e+00,  4.8729e-01,\n",
       "           2.4730e-01],\n",
       "         [-9.3975e-01, -4.6856e-01,  1.0311e+00, -2.8192e-01,  4.9373e-01,\n",
       "          -1.2896e-02, -2.7327e-01, -7.6358e-01,  1.3958e+00, -9.9543e-01,\n",
       "          -7.1287e-04,  1.2449e+00, -7.8077e-02,  1.2765e+00, -1.4589e+00,\n",
       "          -2.1601e+00],\n",
       "         [-7.7021e-02, -1.0205e+00, -1.6895e-01,  9.1776e-01,  1.5810e+00,\n",
       "           1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n",
       "           9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n",
       "          -2.8400e+00],\n",
       "         [-1.3679e+00,  1.0614e-01, -2.1317e+00,  1.0480e+00, -3.7127e-01,\n",
       "          -9.1234e-01, -4.3802e-01, -1.0329e+00,  9.3425e-01,  1.5453e+00,\n",
       "           5.7218e-01, -1.8049e-01, -6.0455e-03, -8.8691e-02,  2.0559e-01,\n",
       "          -5.2292e-01],\n",
       "         [ 2.5444e-01, -5.5082e-01,  1.0012e+00,  8.2745e-01, -3.8978e-01,\n",
       "           4.9129e-01, -2.1302e-01, -1.7432e+00, -1.5972e+00, -1.0776e+00,\n",
       "           9.0331e-01, -7.2292e-01, -5.9652e-01, -7.0857e-01,  6.1977e-01,\n",
       "          -1.3766e+00],\n",
       "         [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1862e-02,\n",
       "          -4.7896e-01,  7.6681e-01,  2.7467e-02,  1.9929e+00,  1.3708e+00,\n",
       "          -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3744e-03, -9.8955e-01,\n",
       "           7.0161e-01],\n",
       "         [ 5.1463e-01,  9.9376e-01, -2.5873e-01, -1.0825e+00, -4.4383e-02,\n",
       "           1.6236e+00, -2.3229e+00,  1.0878e+00,  6.7156e-01,  6.9329e-01,\n",
       "          -9.4872e-01, -7.6506e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n",
       "          -1.4465e+00],\n",
       "         [ 8.7683e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n",
       "           1.3139e+00,  1.0533e+00,  1.3881e-01,  2.2473e+00, -8.0363e-01,\n",
       "          -2.8084e-01,  7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n",
       "           2.2935e-01]]),\n",
       " torch.Size([8, 16]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors, context_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32810a54",
   "metadata": {},
   "source": [
    "### **Parameterizing the self-attention mechanism: scaled dot-product attention**\n",
    "\n",
    "- more advanced self-attention mechanisms, such as scaled dot-product attention, introduce learnable parameters to enhance the model's ability to capture complex relationships within the input sequence.\n",
    "\n",
    "- In scaled dot-product attention, we introduce three learnable weight matrices: $W_Q$, $W_K$, and $W_V$. These matrices are used to project the input embeddings into three different spaces: queries, keys, and values.\n",
    "\n",
    "- The three weight matrices are learned during training, allowing the model to adaptively focus on different aspects of the input data.\n",
    "  - Query Sequence: A set of vectors representing the elements for which we want to compute attention scores.\n",
    "    - $Q^{(i)} = X^{(i)} W_Q$\n",
    "  - Key Sequence: A set of vectors representing the elements against which we want to compute attention scores.\n",
    "    - $K^{(i)} = X^{(i)} W_K$\n",
    "  - Value Sequence: A set of vectors representing the elements that will be combined to produce the final output.\n",
    "    - $V^{(i)} = X^{(i)} W_V$\n",
    "  \n",
    "\n",
    "![Context-aware self-attention mechanism with learnable parameters](./figures/16_05.png)\n",
    "\n",
    "\n",
    "- Here, both the queries and keys are scaled by the square root of the dimensionality of the keys, $d_k$, to prevent excessively large dot product values that could lead to vanishing gradients during training.\n",
    "\n",
    "- We can initialize these projection matrices as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "752a4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "W_Q = torch.randn(d, d)\n",
    "W_K = torch.randn(d, d)\n",
    "W_V = torch.randn(d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "074d8f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ee976",
   "metadata": {},
   "source": [
    "- we can compute the query sequence, key sequence, and value sequence using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f0c5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = embedded_sentence[1, :]\n",
    "q_2 = torch.matmul(x_2, W_Q)\n",
    "k_2 = torch.matmul(x_2, W_K)\n",
    "v_2 = torch.matmul(x_2, W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a222c546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-4.0813, -1.6130, -2.5060, -3.3268, -4.1174, -2.3729, -2.6083,  2.3683,\n",
       "         -4.1584,  9.9378,  3.5163, -2.2705,  4.6320, -4.2101, -0.5922,  4.6235]),\n",
       " tensor([-4.8231, -4.6870, -0.2487, 13.0042, -2.5805, -1.9165,  0.7472,  2.6754,\n",
       "          4.7989,  0.1297, -2.5521,  3.9984,  4.0280,  3.7667,  0.2393, -3.2154]),\n",
       " tensor([ 4.1296, -0.5803,  1.4880,  1.8756, -0.3079, -6.6038, -5.6030,  4.7419,\n",
       "          3.5117,  2.3469, -3.6096, -2.3465, -4.7263,  4.6613, -2.4629, -1.5542]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_2, k_2, v_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c3ae4",
   "metadata": {},
   "source": [
    "- We also need the key and value sequences for all other input elements, which we can compute as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad677a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = torch.matmul(embedded_sentence, W_K)\n",
    "torch.allclose(k_2, keys[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "501eaa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(embedded_sentence, W_V)\n",
    "torch.allclose(v_2, values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea8565",
   "metadata": {},
   "source": [
    "- the unnormalized attention weights, $w_{ij} = {q^{(i)} \\cdot k^{(j)}}$, using the scaled dot-product between the query and key vectors.\n",
    "\n",
    "- the following code computes the unnormalized attention weights, $w_{23}$, that is, the attention weight from the 2nd input element to the 3rd input element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b60de8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.0975)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_23 = torch.dot(q_2, keys[2])\n",
    "omega_23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81c75e",
   "metadata": {},
   "source": [
    "- We can scale up this computation to all keys at once using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40ecb017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-35.2682, -44.7632,  16.0975,  21.0103,  27.2076,  33.4731,  11.9149,\n",
       "         55.0160])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omega_2 = torch.matmul(q_2, keys.T)\n",
    "omega_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e6c9d",
   "metadata": {},
   "source": [
    "- Going from unnormalized attention weights to the normalized attention weights involves applying the softmax function to the unnormalized weights. This ensures that the attention weights sum to one, allowing them to be interpreted as probabilities.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp(w_{ij} / \\sqrt{d_k})}{\\sum_{k=1}^{T} \\exp(w_{ik} / \\sqrt{d_k})} = softmax\\left(\\frac{[w_{ij}]_{j=1...T}}{\\sqrt{d_k}}\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f520f6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5667e-10, 1.4591e-11, 5.9150e-05, 2.0200e-04, 9.5108e-04, 4.5550e-03,\n",
       "        2.0789e-05, 9.9421e-01])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights_2 = F.softmax(omega_2 / torch.sqrt(torch.tensor(d, dtype=torch.float32)), dim=0)\n",
    "attention_weights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6967059",
   "metadata": {},
   "source": [
    "- Finally, the output is a weighted average of the value vectors, where the weights are given by the normalized attention weights:\n",
    "\n",
    "$$z^{(i)} = \\sum_{j=1}^{T} \\alpha_{ij} v^{(j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6615cfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.7645,  6.1684, -8.1683, -6.4059,  3.0102,  5.7119, -1.4577,  1.6116,\n",
       "         1.6057, -4.7039,  4.0043,  0.5080,  3.5367,  2.7837,  2.8228, -7.7864])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2 @ values\n",
    "context_vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bd2a0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc9018",
   "metadata": {},
   "source": [
    "- This section has introduced the self-attention mechanism, a powerful tool for capturing relationships within sequences. By computing attention weights and context vectors, self-attention allows models to focus on relevant parts of the input, enhancing their ability to understand and generate complex data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93287b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb18f5",
   "metadata": {},
   "source": [
    "## **Attention is all we need: introducing the original transformer architecture**\n",
    "\n",
    "The transformer architecture, introduced by Vaswani et al. in the seminal paper \"Attention is All You Need,\" revolutionized natural language processing by relying entirely on self-attention mechanisms, eliminating the need for recurrent or convolutional layers. This architecture has since become the foundation for many state-of-the-art models in NLP, including BERT and GPT. \n",
    "\n",
    "\n",
    "![The original transformer architecture](./figures/16_06.png)\n",
    "\n",
    "\n",
    "We will explore the transformer architecture in detail in the following subsections, by decomposing it into its key components: the encoder and decoder, multi-head self-attention, positional encoding, and feed-forward neural networks. Each of these components plays a crucial role in enabling the transformer to effectively process and generate natural language text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338baf4",
   "metadata": {},
   "source": [
    "### **Encoding context embeddings via multi-head attention**\n",
    "\n",
    "The overall goal of the encoder is to transform the input sequence into a set of context-aware embeddings that capture the relationships between words in the sequence. This is achieved through multiple layers of multi-head self-attention and feed-forward neural networks.\n",
    "\n",
    "The `encoder` consists of several identical layers, each containing two main sub-layers:\n",
    "1. Multi-head self-attention mechanism\n",
    "2. Position-wise feed-forward neural network\n",
    "3. Each sub-layer is followed by a residual connection and layer normalization to facilitate training and improve convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3ed2a",
   "metadata": {},
   "source": [
    "**Multi-head self-attention mechanism** \n",
    "\n",
    "This allows the model to attend to different parts of the input sequence simultaneously. Instead of computing a single set of queries, keys, and values, the multi-head attention mechanism computes multiple sets, or \"heads,\" each with its own learned projection matrices. This enables the model to capture different types of relationships and dependencies within the input sequence.\n",
    "\n",
    "\n",
    "- As indicated by its name, the multi-head self-attention mechanism consists of multiple attention heads, each of which computes self-attention independently. The outputs of these attention heads are then concatenated and linearly transformed to produce the final output of the multi-head attention layer.\n",
    "\n",
    "\n",
    "To explain the concept of multi-head self-attention, let's consider an input sequence represented as a matrix $X$ of shape $(T, d_{model})$, where $T$ is the sequence length and $d_{model}$ is the dimensionality of the input embeddings. The multi-head self-attention mechanism computes $h$ different sets of queries, keys, and values by multiplying the input matrix $X$ with learned weight matrices for each head:\n",
    "\n",
    "$$Q_i = X W_{Q_i}, \\quad K_i = X W_{K_i}, \\quad V_i = X W_{V_i} \\quad \\text{for } i = 1, 2, \\ldots, h$$\n",
    "\n",
    "where $W_{Q_i}$, $W_{K_i}$, and $W_{V_i}$ are the learned weight matrices for the $ith$ head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012694d",
   "metadata": {},
   "source": [
    "#### **Multi-Head Attention in Transformers**\n",
    "\n",
    "A structured deep dive with precise notation, clean progression, and interpretation.\n",
    "\n",
    "\n",
    "**1. Context and Purpose**\n",
    "\n",
    "In Transformer architectures, **Multi-Head Attention (MHA)** is the core mechanism replacing recurrence (RNNs) and convolution (CNNs). It enables a model to **compare all positions in a sequence simultaneously** and determine how much each token should focus on every other token.\n",
    "\n",
    "MHA allows the model to learn **different types of relational patterns** at different representation subspaces.\n",
    "\n",
    "\n",
    "\n",
    "**2. Input Representation**\n",
    "\n",
    "Consider an input sequence of length $`T`$:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "x_1 \\\n",
    "x_2 \\\n",
    "\\vdots \\\n",
    "x_T\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{T \\times d_{\\mathrm{model}}}$$\n",
    "\n",
    "Each token embedding is $`d_{\\mathrm{model}}`$ dimensional (e.g., 512 or 768).\n",
    "\n",
    "\n",
    "\n",
    "**3. Projection to Query, Key, and Value**\n",
    "\n",
    "For attention, we learn three linear projections:\n",
    "\n",
    "$$Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $`W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\mathrm{model}} \\times d_k}`$\n",
    "* $`d_k`$ is the dimensionality of each head (commonly $`d_k = d_{\\mathrm{model}}/h`$)\n",
    "\n",
    "\n",
    "\n",
    "**4. Scaled Dot-Product Attention**\n",
    "\n",
    "For one attention head:\n",
    "\n",
    "$$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V$$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* $`QK^\\top`$ measures **similarity** between tokens.\n",
    "* $`\\sqrt{d_k}`$ scaling stabilizes gradients.\n",
    "* Softmax normalizes to produce weights.\n",
    "* Weighted sum with $`V`$ produces contextualized representations.\n",
    "\n",
    "\n",
    "\n",
    "**5. Multi-Head Extension**\n",
    "\n",
    "Instead of one attention computation, we perform **h** parallel attentions:\n",
    "\n",
    "$$\\mathrm{head}_i = \\mathrm{Attention}(Q_i, K_i, V_i),\\quad i = 1,\\ldots,h$$\n",
    "\n",
    "Each head learns **different** patterns (e.g., syntax, long-range dependency, sentiment cues).\n",
    "\n",
    "Outputs are concatenated:\n",
    "\n",
    "$$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head}_1, \\ldots, \\mathrm{head}_h) W_O$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $`W_O \\in \\mathbb{R}^{(h d_k) \\times d_{\\mathrm{model}}}`$\n",
    "\n",
    "\n",
    "\n",
    "**6. Dimensional Relationships**\n",
    "\n",
    "| Component                 | Notation                      | Shape                         | Notes                     |\n",
    "| ------------------------- | ----------------------------- | ----------------------------- | ------------------------- |\n",
    "| Input sequence            | $X$                           | $T \\times d_{\\mathrm{model}}$ | T tokens, model dimension |\n",
    "| Query matrix              | $Q$                           | $T \\times d_k$ (per head)     | Same for $K, V$           |\n",
    "| Attention scores          | $QK^\\top$                     | $T \\times T$                  | Pairwise token similarity |\n",
    "| Attention output (1 head) | $T \\times d_k$                | Contextualized token vectors  |                           |\n",
    "| Concatenated heads        | $T \\times (h d_k)$            | Combine perspectives          |                           |\n",
    "| Final projection          | $T \\times d_{\\mathrm{model}}$ | Back to model dimension       |                           |\n",
    "\n",
    "\n",
    "\n",
    "**7. Why Multi-Head Instead of Single Attention?**\n",
    "\n",
    "A single attention head provides **one similarity interpretation**.\n",
    "However, language and token interactions involve multiple simultaneous relationships:\n",
    "\n",
    "* syntactic structure\n",
    "* semantic role\n",
    "* long-term link\n",
    "* local dependency\n",
    "* phrase grouping\n",
    "\n",
    "Splitting representation space:\n",
    "\n",
    "$$d_{\\mathrm{model}} = h \\cdot d_k$$\n",
    "\n",
    "means **each head learns a specialized relational subspace**.\n",
    "\n",
    "This **diversifies representation** and improves model capacity without increasing data dependence.\n",
    "\n",
    "\n",
    "\n",
    "**8. Effectively, What Happens Conceptually**\n",
    "\n",
    "For each token:\n",
    "\n",
    "1. Compute how much it should **focus** on each other token (via $`QK^\\top`$).\n",
    "2. Use this focus to compute a **weighted blend** of other token representations.\n",
    "3. Do this multiple independent times (heads) with different learned projections.\n",
    "4. Combine the resulting representations.\n",
    "\n",
    "The result:\n",
    "\n",
    "* Every token becomes a **context-aware** embedding conditioned on the full sequence.\n",
    "\n",
    "\n",
    "\n",
    "**9. Where Multi-Head Attention is Used**\n",
    "\n",
    "| Transformer Block Stage       | Role of MHA                                 |\n",
    "| ----------------------------- | ------------------------------------------- |\n",
    "| **Encoder Self-Attention**    | Captures relationships among input tokens   |\n",
    "| **Decoder Self-Attention**    | Models relationships among generated tokens |\n",
    "| **Encoder–Decoder Attention** | Allows decoder to attend to encoded source  |\n",
    "\n",
    "\n",
    "\n",
    "**10. Key Insights**\n",
    "\n",
    "1. Attention is **pairwise token comparison**.\n",
    "2. Multi-head splits computation into **parallel representation spaces**.\n",
    "3. Each head extracts **different relational cues**.\n",
    "4. Concatenation + projection produces a unified contextual representation.\n",
    "5. MHA is what enables **parallelism** and **long-context reasoning**, unlike RNNs which propagate sequentially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3c30d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4caba0b",
   "metadata": {},
   "source": [
    "- parameterizing the self-attention mechanism: scaled dot-product attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "d = embedded_sentence.shape[1]\n",
    "one_U_query = torch.rand(d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ce176d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_U_query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006b9c3",
   "metadata": {},
   "source": [
    "- Assume we have eight attention heads similar to the original transformer, that is, `h = 8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59afe426",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 8  # number of heads\n",
    "multihead_U_query = torch.rand(h, d, d)\n",
    "multihead_U_key = torch.rand(h, d, d)\n",
    "multihead_U_value = torch.rand(h, d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9b2d3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 16])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_U_query.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b99ad1",
   "metadata": {},
   "source": [
    "- As seen above, multiple attention heads can be added by simply adding an additional dimension to the projection matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68014d00",
   "metadata": {},
   "source": [
    "- After initializing the projection matrices for all attention heads, we can compute the query, key, and value sequences for all attention heads as follows: \n",
    "\n",
    "$$Q_i = X W_{Q_i}, \\quad K_i = X W_{K_i}, \\quad V_i = X W_{V_i} \\quad \\text{for } i = 1, 2, \\ldots, h$$\n",
    "\n",
    "- we can repeat this computation for all attention heads at once using batch matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6db8ae96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_query_2 = multihead_U_query.matmul(x_2)\n",
    "multihead_query_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e6ea3",
   "metadata": {},
   "source": [
    "- The `multihead_query_2` matrix has eight rows, each corresponding to the query vector for one attention head for the 2nd input element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc19e3",
   "metadata": {},
   "source": [
    "- we can compute the key and value sequences for all attention heads in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a38d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_key_2 = multihead_U_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_U_value.matmul(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d1a35fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9619, -0.7701, -0.7280, -1.6840, -1.0801, -1.6778,  0.6763,  0.6547,\n",
       "         1.4445, -2.7016, -1.1364, -1.1204, -2.4430, -0.5982, -0.8292, -1.4401])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_key_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d12d91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.4229e-01, -1.4590e+00, -8.2361e-01, -1.4182e+00, -1.7702e+00,\n",
       "         -1.7670e+00, -1.3254e+00,  1.5882e+00, -2.9187e+00, -1.7060e+00,\n",
       "         -2.7460e+00,  6.5088e-01, -1.3654e+00, -5.6964e-01, -5.1584e-01,\n",
       "         -1.2448e+00],\n",
       "        [ 2.0341e-03, -1.5079e+00,  1.0892e-02, -9.2818e-01,  4.2887e-01,\n",
       "         -3.9543e+00,  3.1292e-02,  1.1121e-01, -3.7466e-01, -3.7266e-01,\n",
       "         -1.2568e+00, -2.1261e+00, -1.3521e+00, -7.6900e-01, -1.5659e+00,\n",
       "         -3.6651e+00],\n",
       "        [-1.2141e-02, -1.6249e-01, -9.6153e-01, -8.4408e-01, -3.1676e-01,\n",
       "          1.1681e+00, -2.3165e+00, -6.4110e-01, -1.1253e+00, -1.4065e+00,\n",
       "         -4.3523e-01, -1.4211e+00,  2.2433e+00, -2.4570e+00, -2.5382e+00,\n",
       "         -1.0644e+00],\n",
       "        [-4.8746e-01, -2.2939e+00,  1.8204e+00, -1.5698e+00, -5.0320e-01,\n",
       "         -1.5094e+00,  3.9411e-01,  1.6684e+00, -2.5816e+00, -2.1744e+00,\n",
       "         -3.2841e+00,  2.4258e-01,  6.1703e-01, -1.9446e-01,  2.1254e-01,\n",
       "         -2.5788e+00],\n",
       "        [-5.8621e-01, -2.6945e+00, -7.1551e-01, -4.3883e-01, -5.9992e-01,\n",
       "         -8.5535e-01, -3.5932e-01, -7.2940e-01, -1.5533e+00, -3.2353e-01,\n",
       "         -2.3989e-01, -2.2807e+00,  1.1514e+00, -3.5991e-01, -1.4550e+00,\n",
       "         -6.2155e-01],\n",
       "        [-1.5262e+00, -2.9995e-01,  4.2655e-01, -7.8383e-01, -5.2422e-01,\n",
       "         -1.4127e+00, -2.8466e+00, -4.9931e+00, -6.2858e-01, -6.0277e-01,\n",
       "         -2.7914e+00, -4.9083e-01,  1.3707e-01, -8.2201e-01, -1.4863e+00,\n",
       "         -1.3980e+00],\n",
       "        [-4.1307e-02, -1.1790e+00,  6.0783e-01, -2.0356e+00, -1.4726e+00,\n",
       "         -2.1032e+00, -2.6080e+00,  3.5530e-01, -2.7130e+00, -1.3320e+00,\n",
       "         -4.8562e-02, -1.5237e+00,  2.7103e-01, -3.8949e-01, -1.1320e+00,\n",
       "          2.9527e-01],\n",
       "        [-1.4403e+00, -1.1791e+00,  8.6947e-01,  4.3598e-01, -9.8053e-01,\n",
       "         -4.3535e-01, -7.0926e-01,  9.4901e-01, -1.9989e+00, -1.5708e+00,\n",
       "          1.4523e-01, -8.1890e-01, -1.2402e+00,  2.4859e-01, -1.3325e+00,\n",
       "         -7.5451e-01]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_value_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f799d1",
   "metadata": {},
   "source": [
    "- We need to repeat this computation for all attention heads at once using batch matrix multiplication.\n",
    "- We can do this by expanding the input vector to have an additional dimension corresponding to the number of attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46fe1a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(h, 1, 1)\n",
    "stacked_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352c31d",
   "metadata": {},
   "source": [
    "- Then, we can have a batch matrix multiplication between the expanded input and the projection matrices for all attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "890db020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_U_key, stacked_inputs)\n",
    "multihead_keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5808a3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3806, -1.6947, -1.8699, -0.4471, -2.1738, -1.1601, -1.5491,  0.3820],\n",
       "        [ 2.0499, -0.6587,  1.2486, -1.8467, -1.3698, -1.3811, -0.4635,  2.7698],\n",
       "        [ 1.4424, -1.1834,  0.2757, -1.7952, -3.4140, -4.3602,  0.5074,  3.2851],\n",
       "        [ 1.8887, -1.2219, -0.9714, -0.7650, -2.6386, -3.1871,  1.5554,  2.6618],\n",
       "        [ 1.4270, -2.6479, -1.4276, -1.8207, -2.2600, -3.0823, -2.2974,  2.3776],\n",
       "        [ 2.5535, -2.6108, -1.4909, -2.5400, -3.5193, -2.2251, -0.0805,  0.6264],\n",
       "        [ 1.2314, -1.0226, -3.8808, -1.1661, -4.4679, -3.0439, -0.3201,  2.3182],\n",
       "        [ 0.4981, -0.2372, -1.1328, -1.3592, -3.6253, -1.3326,  1.0347,  2.7223],\n",
       "        [ 0.1742,  1.2760, -1.9311, -2.3503, -4.1406, -3.1241,  1.1994,  2.4895],\n",
       "        [-0.7740, -1.0980, -0.4115, -3.7343, -2.7884, -2.6045,  0.1928,  2.5534],\n",
       "        [ 1.5517,  0.9142,  1.2744, -3.3934, -1.1208, -4.1307, -0.0142,  0.8712],\n",
       "        [ 1.7910, -0.8720, -0.2987,  1.0605, -1.5535, -0.7161,  0.5001,  0.4319],\n",
       "        [ 1.8690,  0.6989, -0.1477, -0.7440, -1.9431, -1.3575,  0.4465,  1.4382],\n",
       "        [ 0.3381, -1.9691, -3.2710, -0.6733, -3.6587, -2.3487,  0.2559,  3.7426],\n",
       "        [ 0.1128,  0.3036, -1.9272, -2.0156, -3.3801, -2.8097, -0.0134, -0.6803],\n",
       "        [ 0.7528,  1.0828,  0.4124, -2.3775, -0.6549, -1.5846, -0.8262,  1.8228]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_keys[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf237a6",
   "metadata": {},
   "source": [
    "- We now have a tensor that refers to the eight attention heads in its first dimension. The second and third dimensions refer to the embedding size and the number of input elements, respectively.\n",
    "\n",
    "- Swap the second and third dimensions to facilitate further computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cce6545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 16])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_keys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0536488",
   "metadata": {},
   "source": [
    "- we can access the second key value in the second attention head as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "882637c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9619, -0.7701, -0.7280, -1.6840, -1.0801, -1.6778,  0.6763,  0.6547,\n",
       "         1.4445, -2.7016, -1.1364, -1.1204, -2.4430, -0.5982, -0.8292, -1.4401])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_keys[2, 1] # index: [2nd attention head, 2nd key value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a556f",
   "metadata": {},
   "source": [
    "- we can see that this is the same key value that we computed earlier for the 2nd input element in the 2nd attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abce355",
   "metadata": {},
   "source": [
    "- Let's repeat the same process to compute the multi-head queries and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2fd2de46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 16])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_values = torch.bmm(multihead_U_value, stacked_inputs)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "multihead_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4c0f2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(multihead_values[2, 1], multihead_value_2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2842af",
   "metadata": {},
   "source": [
    "- Calculate the context vectors, we will skip the intermediate steps for brevity and assume that we have already computed the context vectors for second input element as the query and the eight different attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f85db08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_z_2 = torch.rand(h, d)\n",
    "multihead_z_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcec48d",
   "metadata": {},
   "source": [
    "- We concatenate these context vectors and linearly transform them to obtain the final output for the 2nd input element. \n",
    "\n",
    "![Concatenating multi-head context vectors and linearly transforming them to obtain the final output](./figures/16_07.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc43b6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = torch.nn.Linear(h * d, d)\n",
    "context_vector_2 = linear(multihead_z_2.flatten())\n",
    "context_vector_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f836eac",
   "metadata": {},
   "source": [
    "- Multi-head self-attention allows the model to capture diverse relationships within the input sequence by attending to different aspects of the data simultaneously. This enhances the model's ability to understand complex patterns and dependencies, ultimately leading to improved performance on various NLP tasks.\n",
    "  \n",
    "\n",
    "- It's repeating the scaled dot-product attention computation multiple times in parallel and combining the results to form a richer representation of the input data.\n",
    "  \n",
    "\n",
    "- It works very well in practice because the multiple heads help the model to focus on different parts of the input sequence, capturing a wider range of relationships and dependencies.\n",
    "\n",
    "\n",
    "- The multi-head attention mechanism is computationally expensive due to the multiple projections and attention computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15c87a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a7e21",
   "metadata": {},
   "source": [
    "#### **Learning a language model: decoder and masked multi-head attention**\n",
    "\n",
    "- The `decoder` is responsible for generating the output sequence based on the encoded input representations. It consists of several identical layers, each containing three main sub-layers:\n",
    "\n",
    "1. Masked multi-head self-attention mechanism\n",
    "2. Multi-head attention mechanism over the encoder's output\n",
    "3. Position-wise feed-forward neural network (fully connected layer)\n",
    "4. Each sub-layer is followed by a residual connection and layer normalization, similar to the encoder.\n",
    "\n",
    "- Masked attention is used in the decoder to prevent the model from attending to future tokens during training. This is crucial for autoregressive tasks, where the model generates one token at a time and should not have access to future tokens that it has not yet generated.\n",
    "\n",
    "\n",
    "- Masked attention is a variation of the standard attention mechanism where certain positions in the input sequence are masked (i.e., set to zero) to prevent the model from attending to them. This is typically done by adding a large negative value (e.g., negative infinity) to the attention scores for the masked positions before applying the softmax function. This ensures that the attention weights for these positions are effectively zero, preventing the model from using information from future tokens during training.\n",
    "\n",
    "\n",
    "![Layer arrangement in the decoder of the original transformer architecture](./figures/16_08.png)\n",
    "\n",
    "\n",
    "- First, the previous output tokens (output embeddings) are processed through a masked multi-head self-attention layer. This allows the decoder to attend to all previous tokens while preventing access to future tokens.\n",
    "\n",
    "- Then, the second multi-head attention layer allows the decoder to attend to the encoder's output representations. This enables the decoder to incorporate information from the input sequence when generating each output token.\n",
    " \n",
    "- Finally, the output of the attention layers is passed through a position-wise feed-forward neural network to produce the final output for each token in the sequence.\n",
    "\n",
    "\n",
    "Comparing the `decoder` to the `encoder` block, the main differences are:\n",
    "1. The decoder includes a masked multi-head self-attention layer to prevent access to future tokens.\n",
    "2. The decoder has an additional multi-head attention layer that attends to the encoder's output representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d80771",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28d40b",
   "metadata": {},
   "source": [
    "#### **Implementation details: positional encodings and layer normalization**\n",
    "\n",
    "**Positional Encoding** positional encodings are added to the input embeddings to provide the model with information about the position of each token in the sequence. Since transformers do not have a built-in notion of order (unlike RNNs), positional encodings are crucial for capturing the sequential nature of language.\n",
    "\n",
    "- Without positional encodings, the transformer would treat the input tokens as a bag of words, losing the order information that is essential for understanding the meaning of sentences.\n",
    "\n",
    "- Transfomers use positional encodings to inject information about the position of each token in the sequence. This is typically done by adding a positional encoding vector to each input embedding before feeding it into the transformer layers.\n",
    "\n",
    "- Transformers enable the same words at different positions to have slightly different representations, by adding a vector of small values to the input embeddings at the beginning of the encoder and decoder blocks.\n",
    "\n",
    "- Positional encodings can be implemented using `sinusoidal functions` or `learned embeddings`. The sinusoidal approach uses `sine` and `cosine` functions of different frequencies to encode the position of each token, allowing the model to generalize to longer sequences than those seen during training.\n",
    "\n",
    "\n",
    "`Sinusoidal Positional Encoding` Formula:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "- Here, $pos$ is the position of the token in the sequence, $i$ is the dimension index, and $d_{model}$ is the dimensionality of the model embeddings.\n",
    "\n",
    "\n",
    "- The sinusoidal positional encoding allows the model to learn relative positions between tokens, as the `sine` and `cosine` functions provide a continuous representation of position that can be easily interpreted by the model.\n",
    "\n",
    "\n",
    "- These positional encodings are added to the input embeddings before they are fed into the transformer layers, allowing the model to incorporate both the content of the tokens and their positions in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7a39f",
   "metadata": {},
   "source": [
    "#### **Deep Dive into Positional Encoding**\n",
    "\n",
    "Transformers **do not have built-in sequence order**.\n",
    "Unlike RNNs (which read tokens sequentially) and CNNs (which use local receptive fields), the Transformer processes all tokens **in parallel**.\n",
    "Therefore, the model needs **an explicit way to encode token position** so it can reason about order, word proximity, phrase structure, and directionality.\n",
    "\n",
    "Positional Encoding (PE) provides this structure.\n",
    "\n",
    "\n",
    "\n",
    "**1. Why Positional Information is Required**\n",
    "\n",
    "Given an input sequence:\n",
    "\n",
    "$$X = \\begin{bmatrix} x_1 \\ x_2 \\ \\cdots \\ x_T \\end{bmatrix} \\in \\mathbb{R}^{T \\times d_{\\mathrm{model}}}$$\n",
    "\n",
    "Self-Attention computes:\n",
    "\n",
    "$$\\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This operation depends only on **similarity** between tokens — **not their order**.\n",
    "\n",
    "Without positional encoding:\n",
    "\n",
    "* “dog bites man” and “man bites dog” produce identical attention structures.\n",
    "\n",
    "So we introduce:\n",
    "\n",
    "$$X' = X + PE$$\n",
    "\n",
    "Where $PE$ injects **position-dependent structure**.\n",
    "\n",
    "\n",
    "\n",
    "**2. Sinusoidal Positional Encoding (Original Transformer)**\n",
    "\n",
    "The key design principle:\n",
    "Positions should be represented **continuously and relationally**, not discretely.\n",
    "\n",
    "For token at position $`pos`$ and embedding index $`i`$:\n",
    "\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\mathrm{model}}}}}\\right)$$\n",
    "\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\mathrm{model}}}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $pos$ ∈ $[0, T)$ is the position index.\n",
    "* $i$ ∈ $[0, d_{\\mathrm{model}})$ controls frequency scaling.\n",
    "\n",
    "##### **Interpretation**\n",
    "\n",
    "* Lower dimensions vary **slowly**, encoding **global** order.\n",
    "* Higher dimensions vary **rapidly**, encoding **local** detail.\n",
    "* The encoding creates a **smooth geometric space** of positions.\n",
    "\n",
    "\n",
    "#### **Critical Mathematical Property**\n",
    "\n",
    "The encoding is **translation-consistent**:\n",
    "\n",
    "\n",
    "$$PE(pos + k) \\approx \\text{simple transform of } PE(pos)$$\n",
    "\n",
    "Meaning the model can learn to reason about **distance** between tokens.\n",
    "\n",
    "\n",
    "\n",
    "**3. Why Sinusoidal Instead of Trainable Vectors?**\n",
    "\n",
    "| Property                               | Sinusoidal | Learned Position Embeddings                |\n",
    "| -------------------------------------- | ---------- | ------------------------------------------ |\n",
    "| Generalizes to unseen longer sequences | **Yes**    | No (limited to learned max length)         |\n",
    "| Provides explicit distance structure   | **Yes**    | No inherent structure                      |\n",
    "| Easy to compute                        | Yes        | Yes                                        |\n",
    "| Used in early Transformers             | ✅          | Later Transformers (BERT, GPT) use learned |\n",
    "\n",
    "Transformers like GPT and BERT use **learned absolute** PE for flexibility.\n",
    "Modern models (GPT-NeoX, PaLM, LLaMA) use **Rotary Positional Encoding (RoPE)** — best of both worlds.\n",
    "\n",
    "\n",
    "\n",
    "**4. Rotary Positional Encoding (RoPE)** *(Modern LLM Standard)*\n",
    "\n",
    "RoPE rotates token embeddings by a **position-dependent rotation matrix**:\n",
    "\n",
    "\n",
    "$$\\mathrm{RoPE}(x, pos) = R(pos) \\cdot x$$\n",
    "\n",
    "Where $R(pos)$ is a block-diagonal rotation operator on embedding subspaces.\n",
    "\n",
    "##### Benefits:\n",
    "\n",
    "* Encodes **relative** position, not absolute.\n",
    "* Handles **long context** efficiently.\n",
    "* Supports extrapolation beyond training length.\n",
    "\n",
    "RoPE is used in:\n",
    "\n",
    "* LLaMA\n",
    "* GPT-J/NeoX\n",
    "* Mistral\n",
    "* Qwen\n",
    "* Phi-3, GPT-4 series\n",
    "\n",
    "\n",
    "\n",
    "**5. Absolute vs Relative Position Encoding**\n",
    "\n",
    "| Approach                                 | Core Idea                             | Used In         | Strength                             |\n",
    "| ---------------------------------------- | ------------------------------------- | --------------- | ------------------------------------ |\n",
    "| **Absolute (Sinusoidal or Learned)**     | Tokens know their numerical position  | BERT, GPT-2     | Simple, works for moderate sequences |\n",
    "| **Relative Position Bias (Shaw et al.)** | Model learns distances between tokens | T5, DeBERTa     | Better for syntax and structure      |\n",
    "| **RoPE**                                 | Rotates embeddings to encode offsets  | GPT-NeoX, LLaMA | Best long-context performance        |\n",
    "\n",
    "Relative and rotary encodings allow the model to understand:\n",
    "\n",
    "* Token A is **5 positions before** token B\n",
    "* Meaningful for language structure (e.g., grammar, object-verb agreement)\n",
    "\n",
    "\n",
    "\n",
    "**6. Why Positional Encoding Works**\n",
    "\n",
    "Self-attention computes similarity:\n",
    "\n",
    "$$QK^\\top$$\n",
    "\n",
    "With PE added:\n",
    "\n",
    "$$(X + PE)W_Q \\cdot (X + PE)W_K^\\top$$\n",
    "\n",
    "Positions influence the **attention score landscape**, meaning:\n",
    "\n",
    "* The model no longer matches tokens solely by meaning\n",
    "* But also considers **where** they occur in the sequence\n",
    "\n",
    "Thus:\n",
    "\n",
    "* **Order** emerges from geometry\n",
    "* **Structure** emerges from learned weighting\n",
    "\n",
    "\n",
    "\n",
    "**7. Key Takeaways**\n",
    "\n",
    "1. Transformers require positional encoding because they **lack inherent sequential bias**.\n",
    "2. Sinusoidal PE creates a **continuous spatial encoding** of token positions.\n",
    "3. Learned PE offers flexibility but limited generalization.\n",
    "4. Relative and RoPE encodings are the modern default because they encode **distance**, not absolute index.\n",
    "5. Position encodings shape the **attention map**, influencing how tokens contextualize each other.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa0b77",
   "metadata": {},
   "source": [
    "#### **Deep Dive into Layer Normalization**\n",
    "\n",
    "Layer Normalization (LayerNorm) is a normalization technique used heavily in **Transformers** and **LLMs**.\n",
    "Its purpose is to **stabilize training**, preserve representation scale, and improve gradient flow — especially in architectures where **batch dimension varies or parallelism is essential**.\n",
    "\n",
    "\n",
    "![Batch and Layer Normalization comparison](./figures/16_09.png)\n",
    "\n",
    "\n",
    "**1. The Problem LayerNorm Solves**\n",
    "\n",
    "Neural networks, especially deep ones, can suffer from:\n",
    "\n",
    "* **Internal Covariate Shift**: distributions of activations change during training.\n",
    "* **Exploding/Vanishing Gradients**: gradients become unstable across layers.\n",
    "* **Sensitivity to learning rates** and initialization.\n",
    "\n",
    "Batch Normalization (BatchNorm) solves part of this problem but depends on **batch statistics**, making it:\n",
    "\n",
    "* Unstable when batch sizes are small\n",
    "* Incompatible with sequence parallel decoding (e.g., autoregressive inference)\n",
    "* Dependent on ordering in distributed training\n",
    "\n",
    "Transformers require *position-invariant*, *batch-independent* normalization — which leads to **LayerNorm**.\n",
    "\n",
    "\n",
    "\n",
    "**2. The Layer Normalization Operation**\n",
    "\n",
    "Given an input activation vector for a single sample:\n",
    "\n",
    "\n",
    "$$x = (x_1, x_2, \\dots, x_H)$$\n",
    "\n",
    "\n",
    "Where (H) = hidden dimension.\n",
    "\n",
    "LayerNorm normalizes **over features**, not over batch:\n",
    "\n",
    "\n",
    "$$\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i$$\n",
    "\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2$$\n",
    "\n",
    "Normalization:\n",
    "\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "Then apply learned scale and shift:\n",
    "\n",
    "\n",
    "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $( \\gamma ) (scale) and ( \\beta ) (bias)$ are **trainable parameters**, same shape as the feature dimension.\n",
    "* $( \\epsilon )$ prevents division-by-zero.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**3. Key Design Difference vs BatchNorm**\n",
    "\n",
    "| Aspect                               | **LayerNorm**                                  | **BatchNorm**                |\n",
    "| ------------------------------------ | ---------------------------------------------- | ---------------------------- |\n",
    "| Normalizes across                    | Feature dimension                              | Batch dimension              |\n",
    "| Works well on                        | NLP, Transformers, RNNs, autoregressive models | CNNs, computer vision        |\n",
    "| Requires large batch size?           | **No**                                         | Yes (unstable otherwise)     |\n",
    "| Stable for variable sequence length? | **Yes**                                        | No                           |\n",
    "| Works at inference same as training? | **Yes**                                        | No running averages required |\n",
    "\n",
    "**Transformers would not work reliably with BatchNorm** — LayerNorm is essential.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**4. Where LayerNorm Appears in Transformers**\n",
    "\n",
    "There are two canonical placements:\n",
    "\n",
    "##### **(A) Post-Layer Norm (Original Transformer - Vaswani et al.)**\n",
    "\n",
    "\n",
    "$$x = x + \\mathrm{Attention}( \\mathrm{LayerNorm}(x) )$$\n",
    "\n",
    "$$x = x + \\mathrm{FFN}( \\mathrm{LayerNorm}(x) )$$\n",
    "\n",
    "\n",
    "##### **(B) Pre-Layer Norm (Modern Models: GPT-2, LLaMA, Mistral, etc.)**\n",
    "\n",
    "\n",
    "$$x = \\mathrm{LayerNorm}(x) + \\mathrm{Attention}(x)$$\n",
    "\n",
    "\n",
    "$$x = \\mathrm{LayerNorm}(x) + \\mathrm{FFN}(x)$$\n",
    "\n",
    "**Pre-LN** models train more stably, avoid divergence, and support deeper stacks.\n",
    "\n",
    "\n",
    "\n",
    "**5. Why LayerNorm Improves Gradient Flow**\n",
    "\n",
    "Consider the gradient of normalized activations:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\hat{x}_i}{\\partial x_j}$$\n",
    "\n",
    "Normalization ensures:\n",
    "\n",
    "* No single neuron can grow arbitrarily\n",
    "* Gradients are **distributed evenly**\n",
    "* Backprop remains stable across many layers\n",
    "\n",
    "This is especially critical in **attention**, where activations can vary widely depending on context.\n",
    "\n",
    "LayerNorm ensures:\n",
    "\n",
    "* Stable scaling before softmax attention\n",
    "* Stable residual paths\n",
    "* Controlled value magnitudes → smoother optimization\n",
    "\n",
    "\n",
    "\n",
    "**6. Intuition: What LayerNorm Actually Enforces**\n",
    "\n",
    "LayerNorm forces the hidden representation of each token:\n",
    "\n",
    "\n",
    "$$(x_1, x_2, ..., x_H)$$\n",
    "\n",
    "to have:\n",
    "\n",
    "* **Mean = 0** → removes bias shift\n",
    "* **Variance = 1** → fixes scale\n",
    "\n",
    "This encourages the network to encode *the meaningful differences between components* of the feature vector, not absolute magnitude.\n",
    "\n",
    "Put differently:\n",
    "\n",
    "> LayerNorm does not change **what** information is encoded — only the **coordinate system** it lives in.\n",
    "\n",
    "\n",
    "\n",
    "**7. Why LayerNorm is Crucial for Transformers**\n",
    "\n",
    "Self-attention amplifies certain features and suppresses others.\n",
    "Without normalization, these transformations can rapidly destabilize:\n",
    "\n",
    "* Queries and keys explode → softmax saturates\n",
    "* Gradients collapse → no learning signal\n",
    "\n",
    "LayerNorm maintains controlled scale throughout the computation graph — keeping attention *differentiable*, *expressive*, and *trainable*.\n",
    "\n",
    "\n",
    "\n",
    "**8. Key Takeaways**\n",
    "\n",
    "1. **LayerNorm normalizes across hidden features**, making it **independent of batch size**.\n",
    "2. It is essential for **Transformer stability**, especially with:\n",
    "\n",
    "   * Deep residual stacks\n",
    "   * Multi-head attention\n",
    "   * Autoregressive decoding\n",
    "3. **Pre-LN** transformers (modern) outperform **Post-LN** (original) in stability and depth scaling.\n",
    "4. LayerNorm ensures **smooth gradient flow** and prevents activation explosion/collapse.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ca1e6d",
   "metadata": {},
   "source": [
    "#### **Deep Dive into Residual Connections and Their Role in Transformers**\n",
    "\n",
    "Residual connections (also called *skip connections*) are one of the **core stability mechanisms** in Transformers.\n",
    "Without them, transformers **do not train well** — gradients vanish, activations saturate, and deep architectures collapse.\n",
    "\n",
    "We will break this down from **mathematical intuition**, **gradient flow**, and **practical architecture design**.\n",
    "\n",
    "\n",
    "\n",
    "##### **1. What is a Residual Connection?**\n",
    "\n",
    "Given a function (a layer or block):\n",
    "\n",
    "\n",
    "$$F(x)$$\n",
    "\n",
    "A **residual connection** outputs:\n",
    "\n",
    "\n",
    "$$y = x + F(x)$$\n",
    "\n",
    "Instead of:\n",
    "\n",
    "\n",
    "$$y = F(x)$$\n",
    "\n",
    "\n",
    "So the network learns **a residual mapping**:\n",
    "\n",
    "\n",
    "$$F(x) = y - x$$\n",
    "\n",
    "This means the layer *only needs to learn what changes relative to the input*.\n",
    "\n",
    "\n",
    "\n",
    "##### **2. Why Residuals Matter in Deep Networks**\n",
    "\n",
    "As networks get deeper:\n",
    "\n",
    "* Gradients become unstable (vanish or explode)\n",
    "* Layers struggle to learn identity transformations\n",
    "* Optimization becomes harder\n",
    "\n",
    "Residual paths allow:\n",
    "\n",
    "\n",
    "$$\\text{Gradient to flow directly from deeper layers back to earlier layers}$$\n",
    "\n",
    "This means:\n",
    "\n",
    "* Even if a deep layer learns **nothing**, the model still preserves the input via `x + ...`.\n",
    "* Early layers receive strong gradient updates, preventing stagnation.\n",
    "\n",
    "\n",
    "\n",
    "##### **3. Residuals in Transformers: Where They Appear**\n",
    "\n",
    "Each Transformer **block** has two residual pathways:\n",
    "\n",
    "**(1) Self-Attention Residual**\n",
    "\n",
    "\n",
    "$$x_1 = x + \\mathrm{MultiHeadAttention}(x)$$\n",
    "\n",
    "**(2) Feed-Forward Network (FFN) Residual**\n",
    "\n",
    "\n",
    "$$x_2 = x_1 + \\mathrm{FFN}(x_1)$$\n",
    "\n",
    "\n",
    "Every transformer layer can be diagrammed as:\n",
    "\n",
    "```\n",
    "      +-------------------+\n",
    "      |                   |\n",
    "      |    Multi-Head     |\n",
    "x --->|     Attention     |---+--> x1\n",
    "      |                   |   |\n",
    "      +-------------------+   |\n",
    "                              |\n",
    "                              + Residual (add and norm)\n",
    "\n",
    "      +-------------------+\n",
    "      |                   |\n",
    "      |       FFN        |\n",
    "x1 -->|  (2-layer MLP)   |---+--> x2\n",
    "      |                   |   |\n",
    "      +-------------------+   |\n",
    "                              |\n",
    "                              + Residual (add and norm)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "##### **4. Pre-LN vs Post-LN Residual Placement**\n",
    "\n",
    "There are two normalization designs:\n",
    "\n",
    "**Original Transformer (Post-LN)**\n",
    "\n",
    "\n",
    "$$x = x + \\mathrm{Attention}( \\mathrm{LayerNorm}(x) )$$\n",
    "\n",
    "\n",
    "$$x = x + \\mathrm{FFN}( \\mathrm{LayerNorm}(x) )$$\n",
    "\n",
    "\n",
    "**Modern Transformers (Pre-LN; GPT-2, LLaMA, PaLM, Mistral, etc.)**\n",
    "\n",
    "\n",
    "$$x = x + \\mathrm{Attention}( \\mathrm{LayerNorm}(x) )$$\n",
    "\n",
    "\n",
    "$$x = x + \\mathrm{FFN}( \\mathrm{LayerNorm}(x) )$$\n",
    "\n",
    "\n",
    "Although they *look the same*, order during forward/backprop is different.\n",
    "\n",
    "**Pre-LN (modern)** gives *stable gradients* and supports *very deep models*.\n",
    "\n",
    "**Post-LN (original)* is harder to train without tricks like warm-up or initialization tuning.\n",
    "\n",
    "\n",
    "\n",
    "##### **5. Gradient Flow Intuition**\n",
    "\n",
    "With a residual connection:\n",
    "\n",
    "\n",
    "$$y = x + F(x)$$\n",
    "\n",
    "Gradient wrt input:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = 1 + \\frac{\\partial F(x)}{\\partial x}$$\n",
    "\n",
    "The `1` is the skip path.\n",
    "\n",
    "This guarantees:\n",
    "\n",
    "* Gradient never becomes zero\n",
    "* Gradient doesn't depend *only* on deep layers\n",
    "* Learning remains stable even when deeper layers are poorly initialized\n",
    "\n",
    "Without residuals:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = \\frac{\\partial F(x)}{\\partial x}$$\n",
    "\n",
    "Gradients become susceptible to:\n",
    "\n",
    "* Vanishing (gradient → 0)\n",
    "* Exploding (gradient → ∞)\n",
    "* Training collapse\n",
    "\n",
    "This is **why very deep transformers (24–200+ layers) are trainable at all.**\n",
    "\n",
    "\n",
    "\n",
    "##### **6. Why Residuals Enable Strong Representation Learning**\n",
    "\n",
    "Self-attention identifies **relationships** between tokens.\n",
    "\n",
    "The FFN identifies **non-linear transformations** on token meaning.\n",
    "\n",
    "Residuals:\n",
    "\n",
    "* Preserve the **original meaning** of the token\n",
    "* Allow attention/FFN to **modify, refine, or add** meaning, not rewrite it\n",
    "\n",
    "So a transformer layer operates like:\n",
    "\n",
    "\n",
    "$$\\text{Meaning}*{\\text{new}} = \\text{Meaning}*{\\text{old}} + \\text{Refinement / Contextualization}$$\n",
    "\n",
    "This is essential for **stable semantic accumulation across depths**.\n",
    "\n",
    "\n",
    "\n",
    "##### **7. Practical Summary**\n",
    "\n",
    "| Component                | Role                                                 |\n",
    "| ------------------------ | ---------------------------------------------------- |\n",
    "| Self-Attention           | Figures out which other tokens matter                |\n",
    "| FFN                      | Learns transformations within token representation   |\n",
    "| LayerNorm                | Keeps scale stable                                   |\n",
    "| **Residual Connections** | Ensure stable gradient flow and meaning preservation |\n",
    "\n",
    "Residuals are the **structural backbone** that allows transformers to scale.\n",
    "\n",
    "\n",
    "\n",
    "##### **8. Why Transformers Without Residuals Fail**\n",
    "\n",
    "If residuals are removed:\n",
    "\n",
    "* Deep layers overwrite token representations instead of improving them\n",
    "* Gradients vanish across ~6+ layers\n",
    "* Model collapses to near-random output\n",
    "* Training becomes extremely unstable\n",
    "\n",
    "Residuals make deep attention-based reasoning possible.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa9cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
