{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers – Improving Natural Language Processing with Attention Mechanisms (Part 3/3)**\n",
        "\n",
        "\n",
        "## **Fine-tuning a BERT model in PyTorch**\n",
        "\n",
        "- fine-tuning a pre-trained BERT model for a specific NLP task, such as sentiment analysis or text classification.\n",
        "- using the `transformers` library by Hugging Face, which provides pre-trained models and tools for natural language processing tasks.\n",
        "\n",
        "`DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased , runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.`\n"
      ],
      "metadata": {
        "id": "7C9jBooHajq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- install dependencies"
      ],
      "metadata": {
        "id": "3bfv93ZHvH5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWsCeCnZw2qa",
        "outputId": "5432340b-2930-48c6-9045-2515c8e72850"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print(datasets.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ru3tzOMxGWF",
        "outputId": "a64d8587-dc69-426d-80c8-24c44f4523be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "g5KSHWiZw-9K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B-BQd5WCvBlT"
      },
      "outputs": [],
      "source": [
        "# Loading the IMDb movie review dataset\n",
        "\n",
        "import gzip\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "# import torchtext\n",
        "\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- General settings"
      ],
      "metadata": {
        "id": "rlhC9Gw5yHa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "NUM_EPOCHS = 3"
      ],
      "metadata": {
        "id": "V1vmTyC6vHf9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyVc6X4MvHbB",
        "outputId": "234d6158-d798-4f75-e915-a5f00a88c32d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download Dataset"
      ],
      "metadata": {
        "id": "Kk0D4O9syfIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/rasbt/machine-learning-book/raw/main/ch08/movie_data.csv.gz\"\n",
        "filename = url.split(\"/\")[-1]\n",
        "\n",
        "with open(filename, \"wb\") as f:\n",
        "    r = requests.get(url)\n",
        "    f.write(r.content)\n",
        "\n",
        "with gzip.open('movie_data.csv.gz', 'rb') as f_in:\n",
        "    with open('movie_data.csv', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)"
      ],
      "metadata": {
        "id": "idKlXVvivHYk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Check that the dataset looks okay:"
      ],
      "metadata": {
        "id": "F-xNu3AFyvZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('movie_data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2No6TKR-vHWm",
        "outputId": "2f247e01-82e1-4316-9405-fc875aed6ca7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49063a15-e511-4bed-847f-c71a6eae8aee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49063a15-e511-4bed-847f-c71a6eae8aee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49063a15-e511-4bed-847f-c71a6eae8aee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49063a15-e511-4bed-847f-c71a6eae8aee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1295b16e-be14-4f90-bf1f-e99dbc5ff0cc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1295b16e-be14-4f90-bf1f-e99dbc5ff0cc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1295b16e-be14-4f90-bf1f-e99dbc5ff0cc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Every time I think about this film I feel physically ill. To read such a great book and later discover there's a film of it was a great feeling. Years later and imagine my joy at switching on the sci-fi channel and finding it starts in just 5mins!!! Up go the titles and then uggg. If just a couple of things had changed OK. Everything is changed. Numerous characters are removed entirely new rubbish ones are added. The main hero is shrunk and de-aged by about 30 years, and hilariously his girlfriend/wife is now his mother! Even the dog is reduced to sub-lassie capabilities. This is truly appalling cinema at its absolute worst. I would quite happily remove my own toenails with pliers rather than sit through another horrific viewing, and I urge anyone thinking of watching this - please don't. If you own a copy burn it now, right now and think how much better your life would have been had this celluloid insult never occurred.\",\n          \"As with all environmentally aware films from the 1970s SOYLENT GREEN has a rather cheesy view of what ecological meltdown is . Overpopulation means there`s too many people to feed ? I was under the impression that famines were caused by either war or failed economic policies . Stalin`s policy in the Soviet Union in the 1930s left millions dead because of famine and to this day the greatest man made tragedy was Mao`s rural policy in China which led over 30 million starvation deaths in the 1950s . And let`s not forget the great famines in the horn of Africa in the 1980s and 90s which were to do with conflicts not overpopulation . You might like to also consider that two of the most heavily populated areas on Earth , Hong Kong and Macau , have never suffered a famine in modern times . Likewise the expansion of shanty towns around cities as seen here isn`t strictly down to overpopulation - it`s down to economic factors where people flock to cities to find better paid work than in the countryside ( It`s a symptom of industrial progress - not of too many births ) so the image of the streets of New York city being too congested to walk through and of having people sleep in stairwells is somewhat laughable<br /><br />But don`t be fooled into thinking SOYLENT GREEN is a pile of corny tree hugging crap because I consider this to be the best ecological film of the<br /><br />70s . It plays on the contempary audience`s knowledge of the world where Sol and Thorn are beside themselves with joy at finding fruit , brandy and fresh meat . Thorn gasps in amazement at having ice in his whisky , puffs on a cigarette and delivers the classic line \\\" If I could afford it I`d smoke two , maybe three of these a day \\\" . But it`s the visage of the euthanasia chamber that`s memorable as Thorn gazes at the images of wild animals , flowers , running water and snow covered mountains , a world Thorn`s generation has never known . This is a very haunting scene which makes SOYLENT GREEN a very memorable film , combined with the fact it features the final screen appearance of Edward G Robinson as the wise old Jew Sol Roth\",\n          \"Yah. I know. It has the name \\\"Sinatra\\\" in the title, so how bad can it be? Well, it's bad, trust me! I rented this thinking it was some movie I missed in the theaters. It's not. It's some garbage \\\"movie\\\" made by the folks at Showtime (cable station). Geez, these cable stations make a few bucks they think they can make whatever garbage movies they want! It's not good. I am as big a Sinatra fan as any sane man, but this movie was just dumb. Boring. Dull. Unfunny. Uninteresting. The only redeeming quality is that (assuming they did stick to the facts) you do learn about what happened to the captors of Frank Jr. Otherwise it's just a stupid film.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiiCndE0vHUI",
        "outputId": "8185a110-675e-47f4-dac3-ccae98e6f053"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Split Dataset into Train/Validation/Test"
      ],
      "metadata": {
        "id": "nq96UEC-zV7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = df.iloc[:35000]['review'].values\n",
        "train_labels = df.iloc[:35000]['sentiment'].values\n",
        "\n",
        "valid_texts = df.iloc[35000:40000]['review'].values\n",
        "valid_labels = df.iloc[35000:40000]['sentiment'].values\n",
        "\n",
        "test_texts = df.iloc[40000:]['review'].values\n",
        "test_labels = df.iloc[40000:]['sentiment'].values"
      ],
      "metadata": {
        "id": "sWAe13tmvHPv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts.shape, valid_texts.shape, test_texts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTp44t2rzVqh",
        "outputId": "1325317f-d809-4672-e2d3-b9abf0593543"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000,), (5000,), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the dataset"
      ],
      "metadata": {
        "id": "sfop62UTzpbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oh5ke1TzVn7",
        "outputId": "a04d0698-4bf4-4b3f-97b0-c8d17f617f22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
        "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "T4a8nb9tzVlO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ev3z5VtzVii",
        "outputId": "26b0a64f-177d-4124-c268-6a6aef686ed0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Dataset Class and Loaders**\n",
        "\n",
        "    -  let’s pack everything into a class called IMDbDataset and create the corresponding data loaders.\n",
        "\n",
        "    - Such a self-defined dataset class lets us customize all the related features and functions for our custom movie review dataset in DataFrame format:"
      ],
      "metadata": {
        "id": "i2fiRjxN1fPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDbDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "valid_dataset = IMDbDataset(valid_encodings, valid_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "NSnbdsIpzVgJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "khTA9_9JzVeH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7o7rJtIzVcI",
        "outputId": "2063a096-6fdf-4697-fa9f-d83f5224bd6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7cb50da29e80>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading and fine-tuning a pre-trained BERT model**\n",
        "\n",
        "- load pre-trained `DistilBERT` model and fine-tune it using the dataset we just created now:"
      ],
      "metadata": {
        "id": "HoCfbmPd3JgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model.to(DEVICE)\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8WJSgyjzVZ1",
        "outputId": "dbf4d580-c59c-4e95-d5a9-e96f3b3b3ed3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEDhaWE04BLZ",
        "outputId": "e188e8b9-cb39-4af2-9894-c5747217579b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-5\n",
        "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Q_UY8KyXzVYJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2QObxVDzVVs",
        "outputId": "3d6b4629-1e69-4b08-e8c8-c102fa2f942a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    decoupled_weight_decay: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 5e-05\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `DistilBertForSequenceClassification` specifies the downstream task we want to fine-tune the model on, which is sequence classification in this case.\n",
        "\n",
        "- 'distilbert-base-uncased' is a lightweight version of a BERT uncased base model with manageable size and good performance."
      ],
      "metadata": {
        "id": "khYvJYMv5Cvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Train Model -- Manual Training Loop**\n",
        "\n"
      ],
      "metadata": {
        "id": "sUDAQKT05xAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    with torch.no_grad():\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        ### Prepare data\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs['logits']\n",
        "            predicted_labels = torch.argmax(logits, 1)\n",
        "            num_examples += labels.size(0)\n",
        "            correct_pred += (predicted_labels == labels).sum()\n",
        "\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "GhrmGVwBzVUR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "\n",
        "        ### Prepare data\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "        labels = batch['labels'].to(DEVICE)\n",
        "\n",
        "        ### Forward\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss, logits = outputs['loss'], outputs['logits']\n",
        "\n",
        "        ### Backward\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        ### Logging\n",
        "        if not batch_idx % 250:\n",
        "            print (f'Epoch: {epoch+1:04d}/{NUM_EPOCHS:04d} | '\n",
        "                   f'Batch {batch_idx:04d}/{len(train_loader):04d} | '\n",
        "                   f'Loss: {loss:.4f}')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'Training accuracy: '\n",
        "              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nValid accuracy: '\n",
        "              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "\n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "\n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "id": "jnh_HsJS5zSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this code, we iterate over multiple epochs. In each epoch we perform the following steps:\n",
        "\n",
        "    - Load the input into the device we are working on (GPU or CPU)\n",
        "    - Compute the model output and loss\n",
        "    - Adjust the weight parameters by backpropagating the loss\n",
        "    - Evaluate the model performance on both the training and validation set"
      ],
      "metadata": {
        "id": "eGKhiPqhBqkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model # free memory"
      ],
      "metadata": {
        "id": "9yb6fLgYzVQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fine-tuning a transformer more conveniently using the Trainer API**\n",
        "\n",
        "- The Trainer API provided by Hugging Face is optimized for transformer models with a wide range of training options and various built-in features. When using the Trainer API, we can skip the effort of writing training loops on our own, and training or fine-tuning a transformer model is as simple as a\n",
        "function (or method) call. Let’s see how this works in practice.\n",
        "\n",
        "\n",
        "**Reload pretrained model:**"
      ],
      "metadata": {
        "id": "lrpGSuD2CQME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model.to(DEVICE)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "rQmcGctQCQ5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The training loop from the previous section can then be replaced by the following code:"
      ],
      "metadata": {
        "id": "GsJZtt9GCucU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "DZ06_qvwCRdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- defined the training arguments\n",
        "- pass the `TrainingArguments` to the Trainer class to instantiate a new trainer object.\n",
        "- the model to be fine-tuned\n",
        "- and the training and evaluation sets\n",
        "- we can train the model by calling `trainer.train()` method\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- evaluate final model by defining an evaluation function as the `compute_metrics` argument for another `Trainer` instance."
      ],
      "metadata": {
        "id": "rOXcJ0cmbErN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred # logits are a numpy array, not pytorch tensor\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(\n",
        "               predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "DAIP2j7MCRaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The updated `Trainer` instantiation (now including `compute_metrics`) is then as follows:"
      ],
      "metadata": {
        "id": "yD8udMfFcY4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    optimizers=(optim, None) # optimizer and learning rate scheduler\n",
        ")\n",
        "\n",
        "# force model to only use 1 GPU (even if multiple are availabe)\n",
        "# to compare more fairly to previous code\n",
        "\n",
        "trainer.args._n_gpu = 1"
      ],
      "metadata": {
        "id": "Wcg6q6SwCRWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- let's train the model again."
      ],
      "metadata": {
        "id": "hmS0LpTbcndw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "trainer.train()\n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
      ],
      "metadata": {
        "id": "JwwanQKZCRS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After training is completed, we can call `trainer.evaluate()` to obtain the model performance on the test set:"
      ],
      "metadata": {
        "id": "l-gv2N-YcuA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "vN_3j4DLvHKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- can directly evaluate the performance of the fine-tuned model on the test dataset by running the following code:"
      ],
      "metadata": {
        "id": "WfDgnqyNdC-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(DEVICE)\n",
        "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "id": "bdukckdEvHHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we saw how we can fine-tune a BERT model for classification. This is different from\n",
        "using other deep learning architectures like RNNs, which we usually train from scratch. However, un-\n",
        "less we are doing research and are trying to develop new transformer architectures—a very expensive\n",
        "endeavor—pre-training transformer models is not necessary. Since transformer models are trained\n",
        "on general, unlabeled dataset resources, pre-training them ourselves may not be a good use of our\n",
        "time and resources; fine-tuning is the way to go."
      ],
      "metadata": {
        "id": "ga2OBjtEdcEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_sSNrcJ2deoa"
      }
    }
  ]
}